{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aae9303-ba38-4a93-bb77-26ec21d832b1",
   "metadata": {},
   "source": [
    "# This notebook uses the NEO4J graph database to associate acts and regulations \n",
    "### The purpose of using a graph database is to understand how graphs can be connected for better retrieval. \n",
    "#### There are a few advantages of using a graph database over traditional databasese and this notebook tries to explore more advantages\n",
    "- It is easier to grow the data in this database without any complex migration scripts or ORM\n",
    "- Much easier to link different data\n",
    "\n",
    "This notebook is inspired byy content from these sources:\n",
    "- Deep learning Knowledge graph for RAG - https://www.deeplearning.ai/short-courses/knowledge-graphs-rag/\n",
    "- https://python.langchain.com/docs/integrations/providers/neo4j\n",
    "- https://neo4j.com/developer-blog/advanced-rag-strategies-neo4j/\n",
    "- Take a step back: https://arxiv.org/pdf/2310.06117.pdf\n",
    "- Open AI blog - https://cookbook.openai.com/examples/rag_with_graph_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4391ee9-c27f-496b-bcc9-ef0ed903a8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install neo4j\n",
    "!pip install bs4\n",
    "!pip install llama-index\n",
    "#!pip uninstall -y trulens_eval\n",
    "!pip install trulens-eval==0.25.1\n",
    "!pip install llmlingua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d54be3d-7670-463f-a13d-9482d394af2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "import warnings\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from llama_index.core import SimpleDirectoryReader, StorageContext\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from sentence_transformers import CrossEncoder\n",
    "import numpy as np\n",
    "from trulens_eval import Tru\n",
    "from trulens_eval.tru_custom_app import instrument\n",
    "from llmlingua import PromptCompressor\n",
    "#from llama_index.indices.postprocessor import SentenceTransformerRerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0843e9-e6e5-417e-a327-e446555a9d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_wrap(string, n_chars=72):\n",
    "    # Wrap a string at the next space after n_chars\n",
    "    if len(string) < n_chars:\n",
    "        return string\n",
    "    else:\n",
    "        return string[:n_chars].rsplit(' ', 1)[0] + '\\n' + word_wrap(string[len(string[:n_chars].rsplit(' ', 1)[0])+1:], n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911112d4-41bc-452f-91b6-942b37b8e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "definition = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f0131a-4016-4724-b184-35b024773b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URI = 'bolt://' + os.getenv('NEO4J_HOST') + ':7687'\n",
    "NEO4J_USERNAME = os.getenv('NEO4J_USER')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "NEO4J_DATABASE = 'neo4j' #os.getenv('NEO4J_DB')\n",
    "print(NEO4J_URI)\n",
    "print(NEO4J_DATABASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b2ed57-cd25-441b-b193-43e9f2118a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg = Neo4jGraph(\n",
    "    url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD, database=NEO4J_DATABASE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48218ea7-da05-491d-a223-390e934e7972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru\n",
    "from trulens_eval.tru_custom_app import instrument\n",
    "TRULENS_USER = os.getenv('TRULENS_USER')\n",
    "TRULENS_PASSWORD = os.getenv('TRULENS_PASSWORD')\n",
    "TRULENS_DB = os.getenv('TRULENS_DB')\n",
    "TRULENS_PORT = os.getenv('TRULENS_PORT')\n",
    "TRULENS_HOST = os.getenv('TRULENS_HOST')\n",
    "\n",
    "TRULENS_CONNECTION_STRING = f'postgresql+psycopg2://{TRULENS_USER}:{TRULENS_PASSWORD}@{TRULENS_HOST}:{TRULENS_PORT}/{TRULENS_DB}'\n",
    "tru = Tru(database_url=TRULENS_CONNECTION_STRING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b6ccc6-c16d-440d-a439-42cdd2a60184",
   "metadata": {},
   "outputs": [],
   "source": [
    "cypher = \"\"\"\n",
    "  MATCH (n) \n",
    "  RETURN count(n)\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1410c8-7469-4620-aa96-6e0e7ecf4d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = kg.query(cypher)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410c1947-5700-4610-bae2-ccf46861e459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all HTML\n",
    "file_metadata = lambda x: {\"filename\": x}\n",
    "Acts_documents = SimpleDirectoryReader(\"./HTML_Acts\",file_metadata=file_metadata).load_data()\n",
    "Regulations_documents = SimpleDirectoryReader(\"./HTML_Regulations\",file_metadata=file_metadata).load_data()\n",
    "print((len(Acts_documents)))\n",
    "print((len(Regulations_documents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a0dd2c-7a7c-4912-ae94-54d9998b0656",
   "metadata": {},
   "source": [
    "In the next section we try to loop through all the Acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a81b357-f9ea-4f73-b5b4-ad32049dc065",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Regulations_documents[1].metadata['filename'])\n",
    "#for key in Regulations_documents[100]:\n",
    "#   print ((key))\n",
    "print(Regulations_documents[100].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43c8431-42d0-452f-a46a-3ee7ac79a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(soup):\n",
    "    title = soup.find_all(\"h2\")\n",
    "    title = title[0].get_text().strip()\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44f8a21-2e21-4f3e-9534-023cf9edbae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_definitions(sections):\n",
    "    for index, section in enumerate(sections):\n",
    "        heading = section.find(\"h4\")\n",
    "        print(heading.get_text())\n",
    "        if 'Definition' in heading.get_text():\n",
    "            definition = section\n",
    "            return definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdb1bf8-f928-45ba-bf74-e8903da84cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preamble(soup):\n",
    "    preamble = soup.find_all(\"div\", class_='preamble')\n",
    "    if preamble:\n",
    "        print(preamble[0].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30c555f-021a-4240-819c-1c5bcaf2980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_from_file(file, soup):\n",
    "    chunks_with_metadata = [] # use this to accumlate chunk records\n",
    "    #print(f'Processing {file}') \n",
    "    item_text = file #file_as_object[item] # grab the text of the item\n",
    "    item_text_chunks = text_splitter.split_text(item_text) # split the text into chunks\n",
    "    token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=20, tokens_per_chunk=256)\n",
    "    token_split_texts = []\n",
    "    for text in item_text_chunks:\n",
    "        token_split_texts += token_splitter.split_text(text)\n",
    "    #print(word_wrap(token_split_texts[10]))\n",
    "    title  = get_title(soup)\n",
    "    print(f\"\\nTitle:{title} Total chunks:{len(token_split_texts)}\")\n",
    "    #print(title)\n",
    "    chunk_seq_id = 0\n",
    "    for chunk in token_split_texts: # only take the first 20 chunks\n",
    "        #form_id = file[file.rindex('/') + 1:file.rindex('.')] # extract form id from file name\n",
    "        # finally, construct a record with metadata and the chunk text\n",
    "        chunks_with_metadata.append({\n",
    "            'text': chunk, \n",
    "            # metadata from looping...\n",
    "            'chunkSeqId': chunk_seq_id,\n",
    "            'chunkId': f'{title}-chunk{chunk_seq_id:04d}',\n",
    "            'ActId': f'{title}',\n",
    "            # constructed metadata...\n",
    "            # metadata from file...\n",
    "        })\n",
    "        chunk_seq_id += 1\n",
    "        #print(f'\\tSplit into {chunk_seq_id} chunks')\n",
    "    return chunks_with_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016671b9-a4a5-4f33-95c7-3a4cc4dca522",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_chunk_node_query = \"\"\"\n",
    "MERGE(mergedChunk:Chunk {chunkId: $chunkParam.chunkId})\n",
    "    ON CREATE SET \n",
    "        mergedChunk.chunkSeqId = $chunkParam.chunkSeqId, \n",
    "        mergedChunk.text = $chunkParam.text,\n",
    "        mergedChunk.ActId = $chunkParam.ActId\n",
    "RETURN mergedChunk\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481dafaa-e733-48d1-9833-31bc6766e65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bde9de-39d1-4502-b13c-4cae59f5febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, Acts in enumerate(Acts_documents):\n",
    "    soup = BeautifulSoup(Acts.get_text(), 'html.parser')\n",
    "    #sections = soup.find_all(\"div\", class_='section')\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 1000,\n",
    "        chunk_overlap  = 200,\n",
    "        length_function = len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    )\n",
    "    item1_text = soup.get_text()\n",
    "   # item1_text_chunks = text_splitter.split_text(item1_text)\n",
    "    first_file_chunks = split_data_from_file(item1_text, soup)\n",
    "    #print(first_file_chunks[0])\n",
    "    kg.query(merge_chunk_node_query, \n",
    "         params={'chunkParam':first_file_chunks[0]})\n",
    "    kg.query(\"\"\"\n",
    "CREATE CONSTRAINT unique_chunk IF NOT EXISTS \n",
    "    FOR (c:Chunk) REQUIRE c.chunkId IS UNIQUE\n",
    "\"\"\")\n",
    "    node_count = 0\n",
    "    for chunk in first_file_chunks:\n",
    "        #print(f\"Creating `:Chunk` node for chunk ID {chunk['chunkSeqId']}\")\n",
    "        kg.query(merge_chunk_node_query, \n",
    "                params={\n",
    "                    'chunkParam': chunk\n",
    "                })\n",
    "        node_count += 1\n",
    "    #print(f\"Created {node_count} nodes\")\n",
    "    kg.query(\"\"\"\n",
    "         MATCH (n)\n",
    "         RETURN count(n) as nodeCount\n",
    "         \"\"\")\n",
    "    kg.query(\"\"\"\n",
    "         CREATE VECTOR INDEX `Acts_chunks` IF NOT EXISTS\n",
    "          FOR (c:Chunk) ON (c.textEmbedding) \n",
    "          OPTIONS { indexConfig: {\n",
    "            `vector.dimensions`: 384,\n",
    "            `vector.similarity_function`: 'cosine'    \n",
    "         }}\n",
    "\"\"\")\n",
    "    # Create the embeddings\n",
    "    for chunk in first_file_chunks:\n",
    "        query_result = embeddings.embed_query(chunk['text'])\n",
    "        #print(chunk['chunkId'])\n",
    "        match =        kg.query(\"\"\"\n",
    "        MATCH (chunk:Chunk) WHERE\n",
    "        chunk.textEmbedding IS NULL\n",
    "        AND chunk.chunkId = $chunkId\n",
    "        AND chunk.chunkSeqId = $chunkSeqId\n",
    "        RETURN chunk\n",
    "        \"\"\",\n",
    "        params={\"chunkSeqId\": chunk['chunkSeqId'], \"chunkId\": chunk['chunkId'], \"ActId\":chunk['ActId'] })\n",
    "        #print(match)\n",
    "        kg.query(\"\"\"\n",
    "        MATCH (chunk:Chunk) WHERE\n",
    "        chunk.textEmbedding IS NULL\n",
    "        AND chunk.chunkSeqId = $chunkSeqId\n",
    "        AND chunk.chunkId = $chunkId\n",
    "        CALL db.create.setNodeVectorProperty(chunk, \"textEmbedding\", $vector)\n",
    "    \"\"\", \n",
    "    params={\"chunkSeqId\": chunk['chunkSeqId'], \"chunkId\": chunk['chunkId'], \"ActId\":chunk['ActId'], \"vector\": query_result} )\n",
    "    kg.query(\"SHOW INDEXES\")\n",
    "    #break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c57db3-2d14-4c1b-8871-b3023d9da4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg.refresh_schema()\n",
    "print(kg.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c3a360-d571-4d92-8082-89b704a70cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neo4j_vector_search(question):\n",
    "  \"\"\"Search for similar nodes using the Neo4j vector index\"\"\"\n",
    "  query_embedding = embeddings.embed_query(question)  \n",
    "  vector_search_query = \"\"\"\n",
    "    CALL db.index.vector.queryNodes($index_name, $top_k, $question) yield node, score\n",
    "    RETURN score, node.ActId, node.RegId, node.text AS text\n",
    "  \"\"\"\n",
    "  similar = kg.query(vector_search_query, \n",
    "                     params={\n",
    "                      'question': query_embedding, \n",
    "                      'index_name':'Acts_chunks', \n",
    "                      'top_k': 10})\n",
    "  return similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d9ee9c-4514-4b97-8336-c09d832d1a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'When an employee is fired what needs to be done next?'\n",
    "search_results = neo4j_vector_search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce293f97-01d0-43cd-9f32-5f6b5ff25a7a",
   "metadata": {},
   "source": [
    "### Doing a rerank using cross-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707c5ee1-6382-4aa2-8ea4-b8a685368001",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results[0]['node.ActId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd0615-d1ed-4d6e-9fb6-0411d17deac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46da424-ebc1-4b86-937e-3edd697dbed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAAI/bge-reranker-base\n",
    "# link: https://huggingface.co/BAAI/bge-reranker-base\n",
    "#rerank = SentenceTransformerRerank(\n",
    "#    top_n=2, model=\"BAAI/bge-reranker-base\"\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b105060-ef3a-49d3-9a03-9c4bd0e35ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [[query, doc['text']] for doc in search_results]\n",
    "#print(pairs)\n",
    "scores = cross_encoder.predict(pairs)\n",
    "print(\"Scores:\")\n",
    "for score in scores:\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0075e6aa-c5c0-47dd-bd54-9424637065ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"New Ordering:\")\n",
    "print(np.argsort(scores)[::-1][0])\n",
    "for o in np.argsort(scores)[::-1]:\n",
    "    print(o)\n",
    "    print(search_results[o]['text'])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b3e43a-9c70-4fbe-b881-131909a3e4a5",
   "metadata": {},
   "source": [
    "# Get the LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0198bb0d-17d5-4a08-a7f3-60338f8bced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openllm\n",
    "\n",
    "llm = openllm.LLM(\n",
    "    \"google/flan-t5-base\",\n",
    "    temperature=0.1,\n",
    "    repetition_penalty=1.2,\n",
    "    backend='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012a890a-1e31-4a4e-8dbc-28ab1523ac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "async def main(query):\n",
    "  previous_texts = ''  \n",
    "  async for gen in llm.generate_iterator(query, max_new_tokens=1024):\n",
    "      print(gen.outputs[0].text, flush=True, end='')\n",
    "      previous_texts += gen.outputs[0].text\n",
    "  return previous_texts\n",
    "\n",
    "\n",
    "asyncio.run(main(\"What age can we drink in B.C?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e770c5d-90cf-4794-8551-b1a97269eb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieval(query_str):\n",
    "    search_results = neo4j_vector_search(query_str)\n",
    "    return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c92a6d2-c4df-41e4-9604-c587e1e8abcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank(search_results):\n",
    "    pairs = [[query, doc['text']] for doc in search_results]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    print(\"New Ordering:\")\n",
    "    for o in np.argsort(scores)[::-1]:\n",
    "        print(o)   \n",
    "        #print(search_results[o])\n",
    "    return \"( \" + search_results[np.argsort(scores)[::-1][0]]['node.ActId']  + ')\\n' + search_results[np.argsort(scores)[::-1][0]]['text'] + \"\\n\\n( \" + search_results[np.argsort(scores)[::-1][1]]['node.ActId']  + ')\\n ' + search_results[np.argsort(scores)[::-1][1]]['text'] + \"\\n\\n( \" + search_results[np.argsort(scores)[::-1][2]]['node.ActId']  + ' )\\n' + search_results[np.argsort(scores)[::-1][2]]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3085d366-6492-4974-9b6b-7b4b6130a60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Or use LLMLingua-2-small model\n",
    "llm_lingua = PromptCompressor(\n",
    "    model_name=\"microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank\",\n",
    "    use_llmlingua2=True, # Whether to use llmlingua-2,\n",
    "    device_map=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dea4025-b314-4e17-bc62-41f144e3598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAG_from_scratch:\n",
    "    @instrument\n",
    "    def retrieve(self, query: str) -> list:\n",
    "        \"\"\"\n",
    "        Retrieve relevant text from vector store.\n",
    "        \"\"\"\n",
    "        return retrieval(query)\n",
    "\n",
    "    @instrument\n",
    "    def reranked(self, search_results) -> str:\n",
    "        return rerank(search_results)\n",
    "\n",
    "    def genprompt(self, query: str, context_str: list) -> str:\n",
    "        \"\"\"\n",
    "        Generate answer from context.\n",
    "        \"\"\"\n",
    "        messages=f\"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "Laws and Acts can be used interchangeably.\n",
    "If the answer is not in the documents, just say that you don't know. \n",
    "Don't try to make up an answer.\n",
    "\n",
    "Context: \n",
    "\n",
    "{context_str}\n",
    "\n",
    "Question: \n",
    "\n",
    "{query}\n",
    "Only return the helpful answer below and nothing else.\n",
    "                    \"\"\"\n",
    "        return messages\n",
    "        \n",
    "    @instrument \n",
    "    def promptcompression(self, prompt, query) ->str:\n",
    "        compressed_prompt = llm_lingua.compress_prompt(prompt, instruction=\"\", question=\"\", target_token=200)\n",
    "        return compressed_prompt['compressed_prompt']\n",
    "\n",
    "    @instrument\n",
    "    def generate_completion(self, compressed_prompt:str) -> str:\n",
    "        print(compressed_prompt)\n",
    "        completion = asyncio.run(main(compressed_prompt))\n",
    "        return completion\n",
    "\n",
    "    @instrument\n",
    "    def query(self, query: str) -> str:\n",
    "        context_str = self.retrieve(query)\n",
    "        rerank = self.reranked(context_str)\n",
    "        prompt = self.genprompt(query, rerank)\n",
    "        print(prompt)\n",
    "        compressed_prompt = self.promptcompression(prompt, query)\n",
    "        completion = self.generate_completion(compressed_prompt)\n",
    "        return completion\n",
    "\n",
    "rag = RAG_from_scratch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426bea3e-f8a8-4785-bd25-84d8e345b70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.query(\"I’m looking to dispute a will, which laws are applied?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e722b4d-1593-4777-8780-439773d098f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruCustomApp\n",
    "tru_rag = TruCustomApp(rag,\n",
    "    app_id = 'NEO_4J',\n",
    "    #feedbacks = [f_groundedness, f_answer_relevance, f_context_relevance]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15205c7c-92c5-4a2c-8013-7319b473db0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_rag as recording:\n",
    "    rag.query(\"I’m looking to dispute a will, which laws are applied?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4301d38-cb9a-4b3d-abcf-257212c26bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Button, HBox, VBox\n",
    "from IPython.display import display\n",
    "\n",
    "thumbs_up_button = Button(description='👍')\n",
    "thumbs_down_button = Button(description='👎')\n",
    "\n",
    "human_feedback = None\n",
    "\n",
    "def on_thumbs_up_button_clicked(b):\n",
    "    global human_feedback\n",
    "    human_feedback = 1\n",
    "\n",
    "def on_thumbs_down_button_clicked(b):\n",
    "    global human_feedback\n",
    "    human_feedback = 0\n",
    "\n",
    "thumbs_up_button.on_click(on_thumbs_up_button_clicked)\n",
    "thumbs_down_button.on_click(on_thumbs_down_button_clicked)\n",
    "\n",
    "HBox([thumbs_up_button, thumbs_down_button])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa06737-0240-4886-87a0-19b4146c6a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "record = recording.get()\n",
    "print(human_feedback)\n",
    "tru.add_feedback(\n",
    "    name=\"Human Feedack\",\n",
    "    record_id=record.record_id,\n",
    "    app_id=tru_rag.app_id,\n",
    "    result=human_feedback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ef9fde-2c36-4c29-aca9-47d1e309adce",
   "metadata": {},
   "outputs": [],
   "source": [
    "records, feedback = tru.get_records_and_feedback(app_ids=[])\n",
    "records.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bf3202-d28f-4aee-b521-b9c6c0fd8f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tru.run_dashboard(address='0.0.0.0', force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b679666e-edb4-48b8-97a5-40870c26c3ae",
   "metadata": {},
   "source": [
    "### Loop through all laws, associate all the chunks, make a parent ACT node and attaching the children chunks to the corresponding parent chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b050381-7459-46ff-8d45-207a793ef5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def act_info_list_fn(actId):\n",
    "    cypher = \"\"\"\n",
    "      MATCH (anyChunk:Chunk) \n",
    "      WHERE anyChunk.ActId = $ActId\n",
    "      WITH anyChunk LIMIT 1\n",
    "      RETURN anyChunk { .ActId } as ActInfo\n",
    "    \"\"\"\n",
    "    act_info_list = kg.query(cypher, params={'ActId': actId})\n",
    "    return act_info_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22278a7f-3794-40b4-a4e8-2d6108e21401",
   "metadata": {},
   "source": [
    "### Connect chunks to their parent form with a PART_OF relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0abe4c1-b35b-4a74-851a-d1cf56329df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parent_act_node(act_info):\n",
    "    cypher = \"\"\"\n",
    "        MERGE (f:Act {ActId: $formInfoParam.ActId })\n",
    "          ON CREATE \n",
    "            SET f.ActId = $formInfoParam.ActId\n",
    "            \"\"\"\n",
    "    kg.query(cypher, params={'formInfoParam': act_info})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ffddc2-8431-4f4a-a01c-4a33b40755c7",
   "metadata": {},
   "source": [
    "### Add a NEXT relationship between subsequent chunks\n",
    "- Use the `apoc.nodes.link` function from Neo4j to link ordered list of `Chunk` nodes with a `NEXT` relationship\n",
    "- Do this for just the \"Item 1\" section to start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77b37d9-1d0f-4a17-b6f8-e265174896eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunk_relationship(act_info):\n",
    "    cypher = \"\"\"\n",
    "  MATCH (from_same_section:Chunk)\n",
    "  WHERE from_same_section.ActId = $ActParam\n",
    "  WITH from_same_section\n",
    "    ORDER BY from_same_section.chunkSeqId ASC\n",
    "  WITH collect(from_same_section) as section_chunk_list\n",
    "    CALL apoc.nodes.link(\n",
    "        section_chunk_list, \n",
    "        \"NEXT\", \n",
    "        {avoidDuplicates: true}\n",
    "    )  // NEW!!!\n",
    "  RETURN size(section_chunk_list)\n",
    "\"\"\"\n",
    "    kg.query(cypher, params={'ActParam': act_info['ActId']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb61da-c389-4869-b48f-e509cf4206a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_chunk_to_parent():\n",
    "    cypher = \"\"\"\n",
    "      MATCH (c:Chunk), (f:Act)\n",
    "        WHERE c.ActId = f.ActId\n",
    "      MERGE (c)-[newRelationship:PART_OF]->(f)\n",
    "      RETURN count(newRelationship)\n",
    "    \"\"\"\n",
    "    kg.query(cypher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d236fb-232c-427a-aab3-464ae5bfbc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, Acts in enumerate(Acts_documents):\n",
    "    soup = BeautifulSoup(Acts.get_text(), 'html.parser')\n",
    "    title  = get_title(soup)\n",
    "    act_info_lists = act_info_list_fn(title)\n",
    "    for act_info_list in act_info_lists:\n",
    "        act_info = act_info_list['ActInfo']        \n",
    "        create_parent_act_node(act_info)\n",
    "        create_chunk_relationship(act_info)\n",
    "        connect_chunk_to_parent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d574716d-67df-4d67-92f6-6122f8971ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg.query(\"SHOW INDEXES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7309701-7cae-482e-94e6-1d5a0767f21a",
   "metadata": {},
   "source": [
    "## Doing the same steps as above for Regulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a176a1b3-ef8f-4ae1-a88c-d26442ae441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_regchunk_node_query = \"\"\"\n",
    "MERGE(mergedChunk:Chunk {chunkId: $chunkParam.chunkId})\n",
    "    ON CREATE SET \n",
    "        mergedChunk.chunkSeqId = $chunkParam.chunkSeqId, \n",
    "        mergedChunk.text = $chunkParam.text,\n",
    "        mergedChunk.ActId = $chunkParam.ActId,\n",
    "        mergedChunk.RegId = $chunkParam.RegId\n",
    "RETURN mergedChunk\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793b90d4-febe-4356-a6d5-5333ffa34ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_regdata_from_file(file, soup):\n",
    "    chunks_with_metadata = [] # use this to accumlate chunk records\n",
    "    #print(f'Processing {file}') \n",
    "    item_text = file #file_as_object[item] # grab the text of the item\n",
    "    item_text_chunks = text_splitter.split_text(item_text) # split the text into chunks\n",
    "    token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=20, tokens_per_chunk=256)\n",
    "    token_split_texts = []\n",
    "    for text in item_text_chunks:\n",
    "        token_split_texts += token_splitter.split_text(text)\n",
    "    #print(word_wrap(token_split_texts[10]))\n",
    "    title = soup.find_all(\"h2\")\n",
    "    act_name = reg_name = title[0].get_text().strip()\n",
    "    reg_name = title[1].get_text().strip()\n",
    "    print(f\"\\nAct:{act_name} Reg:{reg_name} Total chunks:{len(token_split_texts)}\")\n",
    "    #print(title)\n",
    "    chunk_seq_id = 0\n",
    "    for chunk in token_split_texts: # only take the first 20 chunks\n",
    "        #form_id = file[file.rindex('/') + 1:file.rindex('.')] # extract form id from file name\n",
    "        # finally, construct a record with metadata and the chunk text\n",
    "        chunks_with_metadata.append({\n",
    "            'text': chunk, \n",
    "            # metadata from looping...\n",
    "            'chunkSeqId': chunk_seq_id,\n",
    "            'chunkId': f'{act_name}-{reg_name}-chunk{chunk_seq_id:04d}',\n",
    "            'ActId': f'{act_name}',\n",
    "            'RegId':f'{reg_name}',\n",
    "            # constructed metadata...\n",
    "            # metadata from file...\n",
    "        })\n",
    "        chunk_seq_id += 1\n",
    "        #print(f'\\tSplit into {chunk_seq_id} chunks')\n",
    "    return chunks_with_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c55ec2d-61a0-4939-adea-4bc8d63a7807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_Act_exists(act_id):\n",
    "    cypher = \"\"\"\n",
    "    MATCH (n:Act) WHERE\n",
    "    n.ActId CONTAINS $ActId\n",
    "    RETURN n\n",
    "    \"\"\"\n",
    "    return kg.query(cypher, params={'ActId': act_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b15910b-f700-4a62-a162-139a900eadcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_exists = check_Act_exists(\"Transport\")\n",
    "print(len(act_exists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5175101-e17b-4ac0-9710-f5767f2fe7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_regs = []\n",
    "for index, Regulation in enumerate(Regulations_documents):\n",
    "    # Check if ACt exists, if not we skip\n",
    "    soup = BeautifulSoup(Regulation.get_text(), 'html.parser')\n",
    "    # Check if ACt exists, if not we skip\n",
    "    act = get_title(soup)\n",
    "    check_act = check_Act_exists(act)\n",
    "    if not len(check_act):\n",
    "        print(f\"\\n{act} does not exist. Skipping ...\")\n",
    "        continue;\n",
    "    else:\n",
    "        print(f\"\\n{act} exists\")\n",
    "        \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 1000,\n",
    "        chunk_overlap  = 200,\n",
    "        length_function = len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    )\n",
    "    item1_text = soup.get_text()\n",
    "    first_file_chunks = split_regdata_from_file(item1_text, soup)\n",
    "\n",
    "    kg.query(merge_regchunk_node_query, \n",
    "     params={'chunkParam':first_file_chunks[0]})\n",
    "    kg.query(\"\"\"\n",
    "CREATE CONSTRAINT unique_chunk IF NOT EXISTS \n",
    "    FOR (c:Chunk) REQUIRE c.chunkId IS UNIQUE\n",
    "\"\"\")\n",
    "    node_count = 0\n",
    "    for chunk in first_file_chunks:\n",
    "        #print(f\"Creating `:Chunk` node for chunk ID {chunk['chunkSeqId']}\")\n",
    "        kg.query(merge_regchunk_node_query, \n",
    "                params={\n",
    "                    'chunkParam': chunk\n",
    "                })\n",
    "        node_count += 1\n",
    "    #print(f\"Created {node_count} nodes\")\n",
    "    kg.query(\"\"\"\n",
    "         MATCH (n)\n",
    "         RETURN count(n) as nodeCount\n",
    "         \"\"\")\n",
    "    kg.query(\"\"\"\n",
    "         CREATE VECTOR INDEX `Acts_chunks` IF NOT EXISTS\n",
    "          FOR (c:Chunk) ON (c.textEmbedding) \n",
    "          OPTIONS { indexConfig: {\n",
    "            `vector.dimensions`: 384,\n",
    "            `vector.similarity_function`: 'cosine'    \n",
    "         }}\n",
    "\"\"\")\n",
    "    # Create the embeddings\n",
    "    for chunk in first_file_chunks:\n",
    "        query_result = embeddings.embed_query(chunk['text'])\n",
    "        #print(chunk['chunkId'])\n",
    "        match =        kg.query(\"\"\"\n",
    "        MATCH (chunk:Chunk) WHERE\n",
    "        chunk.textEmbedding IS NULL\n",
    "        AND chunk.chunkId = $chunkId\n",
    "        AND chunk.chunkSeqId = $chunkSeqId\n",
    "        AND chunk.ActId = $ActId \n",
    "        AND chunk.RegId = $RegId\n",
    "        RETURN chunk\n",
    "        \"\"\",\n",
    "        params={\"chunkSeqId\": chunk['chunkSeqId'], \"chunkId\": chunk['chunkId'], \"ActId\":chunk['ActId'], \"RegId\":chunk['RegId'] })\n",
    "        #print(match)\n",
    "        kg.query(\"\"\"\n",
    "        MATCH (chunk:Chunk) WHERE\n",
    "        chunk.textEmbedding IS NULL\n",
    "        AND chunk.chunkSeqId = $chunkSeqId\n",
    "        AND chunk.chunkId = $chunkId\n",
    "        AND chunk.ActId = $ActId \n",
    "        AND chunk.RegId = $RegId\n",
    "        CALL db.create.setNodeVectorProperty(chunk, \"textEmbedding\", $vector)\n",
    "    \"\"\", \n",
    "    params={\"chunkSeqId\": chunk['chunkSeqId'], \"chunkId\": chunk['chunkId'], \"ActId\":chunk['ActId'], \"vector\": query_result, \"RegId\":chunk['RegId']} )\n",
    "    kg.query(\"SHOW INDEXES\")\n",
    "    #break;\n",
    "    #act = title  = get_title(soup)\n",
    "    #title = soup.find_all(\"h2\")\n",
    "    #reg_name = title[1].get_text()\n",
    "    #total_acts.append(act)\n",
    "    #print(act)\n",
    "    #print(reg_name.strip() + '\\n\\n')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d714be-78dd-4adf-bb9f-8b6eadf1b994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parent_reg_node(reg_info):\n",
    "    cypher = \"\"\"\n",
    "        MERGE (f:Reg {RegId: $formInfoParam.RegId })\n",
    "          ON CREATE SET \n",
    "          f.RegId = $formInfoParam.RegId,\n",
    "          f.ActId = $formInfoParam.ActId\n",
    "            \"\"\"\n",
    "    kg.query(cypher, params={'formInfoParam': reg_info})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da7c99-1988-499c-8f44-bebfa801c76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_info_list_fn(regId):\n",
    "    cypher = \"\"\"\n",
    "      MATCH (anyChunk:Chunk) \n",
    "      WHERE anyChunk.RegId = $RegId\n",
    "      WITH anyChunk LIMIT 1\n",
    "      RETURN anyChunk { .RegId, .ActId } as RegInfo\n",
    "    \"\"\"\n",
    "    reg_info_list = kg.query(cypher, params={'RegId': regId})\n",
    "    return reg_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e254399-e880-43a6-9abb-81804b45ae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regchunk_relationship(reg_info):\n",
    "    cypher = \"\"\"\n",
    "  MATCH (from_same_section:Chunk)\n",
    "  WHERE from_same_section.RegId = $RegParam\n",
    "  WITH from_same_section\n",
    "    ORDER BY from_same_section.chunkSeqId ASC\n",
    "  WITH collect(from_same_section) as section_chunk_list\n",
    "    CALL apoc.nodes.link(\n",
    "        section_chunk_list, \n",
    "        \"NEXT\", \n",
    "        {avoidDuplicates: true}\n",
    "    )  // NEW!!!\n",
    "  RETURN size(section_chunk_list)\n",
    "\"\"\"\n",
    "    kg.query(cypher, params={'RegParam': reg_info['RegId']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39991c8a-e397-4353-b7ca-c9a40274d426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_regchunk_to_parent():\n",
    "    cypher = \"\"\"\n",
    "      MATCH (c:Chunk), (f:Reg)\n",
    "        WHERE c.RegId = f.RegId\n",
    "      MERGE (c)-[newRelationship:PART_OF]->(f)\n",
    "      RETURN count(newRelationship)\n",
    "    \"\"\"\n",
    "    kg.query(cypher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3babbfcd-7cdf-4fe2-a6ea-1e18798d0b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, Reg in enumerate(Regulations_documents):\n",
    "    soup = BeautifulSoup(Reg.get_text(), 'html.parser')\n",
    "    title = soup.find_all(\"h2\")\n",
    "    reg_name = title[1].get_text().strip()\n",
    "    reg_info_lists = reg_info_list_fn(reg_name)\n",
    "    print(reg_info_lists)\n",
    "    for reg_info_list in reg_info_lists:\n",
    "        reg_info = reg_info_list['RegInfo']        \n",
    "        create_parent_reg_node(reg_info)\n",
    "        create_regchunk_relationship(reg_info)\n",
    "        connect_regchunk_to_parent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bd863a-517c-4735-923b-d34bd366091a",
   "metadata": {},
   "source": [
    "# Create the law node and connect the Regulations and Act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0535a45-611e-4507-bc8a-1f1937e473fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parent_law_node(law_info):\n",
    "    cypher = \"\"\"\n",
    "        MERGE (f:Law {LawId: $formInfoParam.ActId })\n",
    "          ON CREATE SET \n",
    "          f.LawId = $formInfoParam.ActId\n",
    "            \"\"\"\n",
    "    kg.query(cypher, params={'formInfoParam': law_info})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bed743c-626c-442c-8270-971cca8999f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def law_info_list_fn(LawId):\n",
    "    cypher = \"\"\"\n",
    "      MATCH (anyAct:Act) \n",
    "      WHERE anyAct.ActId = $LawId\n",
    "      WITH anyAct LIMIT 1\n",
    "      RETURN anyAct { .ActId } as LawInfo\n",
    "    \"\"\"\n",
    "    law_info_list = kg.query(cypher, params={'LawId': LawId})\n",
    "    return law_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b163c-d13f-4648-a8a6-9f2bffe872ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_act_parentlaw():\n",
    "    cypher = \"\"\"\n",
    "      MATCH (c:Act), (f:Law)\n",
    "        WHERE c.ActId = f.LawId\n",
    "      MERGE (c)-[newRelationship:ACT]->(f)\n",
    "      RETURN count(newRelationship)\n",
    "    \"\"\"\n",
    "    kg.query(cypher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36bb91b-849f-440f-a979-bd48c9896fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_reg_parentlaw():\n",
    "    cypher = \"\"\"\n",
    "      MATCH (c:Reg), (f:Law)\n",
    "        WHERE c.ActId = f.LawId\n",
    "      MERGE (c)-[newRelationship:REGULATIONS]->(f)\n",
    "      RETURN count(newRelationship)\n",
    "    \"\"\"\n",
    "    kg.query(cypher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12764a43-98ea-4eae-be24-1aee0543c360",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, Acts in enumerate(Acts_documents):\n",
    "    soup = BeautifulSoup(Acts.get_text(), 'html.parser')\n",
    "    title  = get_title(soup)\n",
    "    law_info_lists = law_info_list_fn(title)\n",
    "    for law_info_list in law_info_lists:\n",
    "        law_info = law_info_list['LawInfo']        \n",
    "        create_parent_law_node(law_info)\n",
    "connect_act_parentlaw()\n",
    "connect_reg_parentlaw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb66453-9789-4640-95e8-d1d24ade6fbe",
   "metadata": {},
   "source": [
    "# THE END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
