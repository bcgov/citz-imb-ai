# ╔═══════════════════════════════════════════════════════════════════════╗
# ║  llama.cpp LLM Server - Intel oneAPI MKL Optimized                  ║
# ║  Multi-model server with OpenAI-compatible API (/v1/chat/completions)║
# ║  Target: 5th Gen Intel Xeon (AVX-512 + AMX)                         ║
# ╚═══════════════════════════════════════════════════════════════════════╝
#
# Build:
#   docker build -f .docker/Dockerfile.llamacpp -t llamacpp-server .
#
# Run:
#   docker run -p 8090:8090 llamacpp-server
#
# Override models at runtime:
#   MODELS=1,6 docker run -p 8090:8090 llamacpp-server
#
# Build for a different CPU arch (default targets 5th Gen Xeon / Sapphire Rapids+):
#   docker build --build-arg TARGET_ARCH=native -t llamacpp-server .
#   docker build --build-arg TARGET_ARCH=skylake-avx512 -t llamacpp-server .


# ─── Stage 1: Build llama.cpp with Intel oneAPI ─────────────────────────────
FROM intel/oneapi-basekit:latest AS builder

ARG DEBIAN_FRONTEND=noninteractive
ARG LLAMA_CPP_VERSION=master

# Target CPU microarchitecture for the Intel compiler (-march flag).
# "sapphirerapids" covers 4th/5th Gen Xeon with AVX-512 + AMX.
# Set to "native" if building directly on the target machine.
# Common values: sapphirerapids, emeraldrapids, graniterapids, skylake-avx512, native
ARG TARGET_ARCH=sapphirerapids

# Install build tools
RUN apt-get update && apt-get install -y --no-install-recommends \
    git cmake pkg-config \
    && rm -rf /var/lib/apt/lists/*

# Clone llama.cpp
RUN git clone --depth 1 --branch ${LLAMA_CPP_VERSION} \
    https://github.com/ggml-org/llama.cpp.git /build/llama.cpp

WORKDIR /build/llama.cpp

# Build with:
#   - Intel icx/icpx compilers targeting ${TARGET_ARCH}
#   - Intel MKL BLAS for optimized matrix math
#   - All AVX-512 extensions (F, VBMI, VNNI, BF16)
#   - All AMX extensions (TILE, INT8, BF16) for 4th/5th Gen Xeon
#   - GGML_NATIVE=OFF because we cross-compile for a different CPU than the build host
SHELL ["/bin/bash", "-c"]
RUN source /opt/intel/oneapi/setvars.sh && \
    cmake -B build \
      -DCMAKE_C_COMPILER=icx \
      -DCMAKE_CXX_COMPILER=icpx \
      -DCMAKE_C_FLAGS="-march=${TARGET_ARCH}" \
      -DCMAKE_CXX_FLAGS="-march=${TARGET_ARCH}" \
      -DCMAKE_BUILD_TYPE=Release \
      \
      -DGGML_NATIVE=OFF \
      \
      -DGGML_BLAS=ON \
      -DGGML_BLAS_VENDOR=Intel10_64lp \
      \
      -DGGML_AVX=ON \
      -DGGML_AVX2=ON \
      -DGGML_AVX512=ON \
      -DGGML_AVX512_VBMI=ON \
      -DGGML_AVX512_VNNI=ON \
      -DGGML_AVX512_BF16=ON \
      \
      -DGGML_AMX_TILE=ON \
      -DGGML_AMX_INT8=ON \
      -DGGML_AMX_BF16=ON \
      \
      -DGGML_CPU_REPACK=ON \
    && cmake --build build --config Release -j $(nproc) --target llama-server


# ─── Stage 2: Runtime ───────────────────────────────────────────────────────────
FROM intel/oneapi-basekit:latest

ARG DEBIAN_FRONTEND=noninteractive

# Install huggingface-cli for model downloads
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl python3 python3-pip procps \
    && pip3 install --no-cache-dir "huggingface_hub[cli]" \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

# Set MKL library path (pre-configured in oneapi-basekit runtime)
ENV LD_LIBRARY_PATH="/opt/intel/oneapi/mkl/latest/lib/intel64:${LD_LIBRARY_PATH}"

# Copy llama-server binary from builder
COPY --from=builder /build/llama.cpp/build/bin/llama-server /usr/local/bin/llama-server
RUN chmod +x /usr/local/bin/llama-server

# Copy entrypoint and model config
COPY llm/entrypoint.sh /entrypoint.sh
COPY llm/models.conf /etc/llm/models.conf
RUN chmod +x /entrypoint.sh

# Model storage (persisted via Docker volume)
VOLUME /models

# Expose ports for all 7 models
EXPOSE 8090 8091 8092 8093 8094 8095 8096

# ─── Configuration via environment variables ─────────────────────────────────────
# MODELS         Comma-separated model IDs to run (default: "1,6")
#                  1=Qwen2.5-3B  2=Qwen3VL-4B  3=GLM4.6V  4=KimiVL
#                  5=Devstral24B  6=Qwen2-0.5B  7=Qwen3-8B
# CONTEXT_SIZE   Context window size (default: 4096)
# THREADS        Threads per model, empty = auto (default: "")
# HF_TOKEN       HuggingFace token for gated models (optional)
ENV MODELS="1,6" \
    CONTEXT_SIZE=4096 \
    THREADS="" \
    HF_TOKEN=""

ENTRYPOINT ["/entrypoint.sh"]
