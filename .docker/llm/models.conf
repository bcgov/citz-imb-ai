# Model configuration for llama.cpp LLM server
#
# Format: MODEL_ID|PORT|MODEL_SOURCE|FILENAME|EXTRA_ARGS
#
# EXTRA_ARGS can include --mmproj for vision models.
# Model files must exist in MODEL_SOURCE_DIR (default: /models) before startup.
# MODEL_SOURCE can be either:
#   - Hugging Face repo id (e.g., Qwen/Qwen2.5-3B-Instruct-GGUF)
#   - Direct file URL for the main model (https://.../model.gguf)
# For direct mmproj URL, add:
#   --mmproj <filename> --mmproj-url https://.../mmproj.gguf
#
# To add a new model:
#   1. Add a line below with a unique ID and port
#   2. Find the GGUF repo on https://huggingface.co (search for "ModelName GGUF")
#   3. Use the Q4_K_M quantization for best quality/speed tradeoff on CPU
#
# ──────────────────────────────────────────────────────────────────────────────
# ID  PORT  MODEL_SOURCE                                         FILENAME                                         EXTRA_ARGS
# ──────────────────────────────────────────────────────────────────────────────

# Model 1: Qwen 2.5 3B - Fast text generation
1|8090|Qwen/Qwen2.5-3B-Instruct-GGUF|qwen2.5-3b-instruct-q4_k_m.gguf|

# Model 2: Qwen3 VL 4B - Vision + OCR
2|8091|unsloth/Qwen3-VL-4B-Instruct-GGUF|Qwen3-VL-4B-Instruct-Q4_K_M.gguf|--mmproj mmproj-Qwen3-VL-4B-Instruct-F16.gguf

# Model 3: GLM 4.6V Flash - Text, long context
3|8092|bartowski/GLM-4.1V-9B-Thinking-GGUF|GLM-4.1V-9B-Thinking-Q4_K_M.gguf|

# Model 4: Kimi VL A3B - Vision + Thinking + OCR
4|8093|bartowski/Kimi-VL-A3B-Thinking-2506-GGUF|Kimi-VL-A3B-Thinking-2506-Q4_K_M.gguf|--mmproj mmproj-Kimi-VL-A3B-Thinking-2506-bf16.gguf

# Model 5: Devstral 24B - Code specialist
5|8094|bartowski/Devstral-Small-2505-GGUF|Devstral-Small-2505-Q4_K_M.gguf|

# Model 6: Qwen2 0.5B - Ultra-fast tiny model
6|8095|Qwen/Qwen2-0.5B-Instruct-GGUF|qwen2-0_5b-instruct-q4_k_m.gguf|

# Model 7: Qwen3 8B - Balanced general purpose
7|8096|bartowski/Qwen3-8B-GGUF|Qwen3-8B-Q4_K_M.gguf|

# Model 8: Qwen3 VL 8B - Vision + OCR (direct URLs, no token needed)
8|8097|https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct-GGUF/resolve/main/Qwen3VL-8B-Instruct-Q4_K_M.gguf|Qwen3VL-8B-Instruct-Q4_K_M.gguf|--mmproj mmproj-Qwen3VL-8B-Instruct-Q8_0.gguf --mmproj-url https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct-GGUF/resolve/main/mmproj-Qwen3VL-8B-Instruct-Q8_0.gguf
