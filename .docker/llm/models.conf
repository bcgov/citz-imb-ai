# Model configuration for llama.cpp LLM server
#
# Format: MODEL_ID|PORT|HF_REPO|FILENAME|EXTRA_ARGS
#
# EXTRA_ARGS can include --mmproj for vision models.
# For mmproj files, the entrypoint will download them from the same HF_REPO.
#
# To add a new model:
#   1. Add a line below with a unique ID and port
#   2. Find the GGUF repo on https://huggingface.co (search for "ModelName GGUF")
#   3. Use the Q4_K_M quantization for best quality/speed tradeoff on CPU
#
# ──────────────────────────────────────────────────────────────────────────────
# ID  PORT  HF_REPO                                              FILENAME                                         EXTRA_ARGS
# ──────────────────────────────────────────────────────────────────────────────

# Model 1: Qwen 2.5 3B - Fast text generation
1|8090|Qwen/Qwen2.5-3B-Instruct-GGUF|qwen2.5-3b-instruct-q4_k_m.gguf|

# Model 2: Qwen3 VL 4B - Vision + OCR
2|8091|unsloth/Qwen3-VL-4B-Instruct-GGUF|Qwen3-VL-4B-Instruct-Q4_K_M.gguf|--mmproj mmproj-Qwen3-VL-4B-Instruct-F16.gguf

# Model 3: GLM 4.6V Flash - Text, long context
3|8092|bartowski/GLM-4.1V-9B-Thinking-GGUF|GLM-4.1V-9B-Thinking-Q4_K_M.gguf|

# Model 4: Kimi VL A3B - Vision + Thinking + OCR
4|8093|bartowski/Kimi-VL-A3B-Thinking-2506-GGUF|Kimi-VL-A3B-Thinking-2506-Q4_K_M.gguf|--mmproj mmproj-Kimi-VL-A3B-Thinking-2506-bf16.gguf

# Model 5: Devstral 24B - Code specialist
5|8094|bartowski/Devstral-Small-2505-GGUF|Devstral-Small-2505-Q4_K_M.gguf|

# Model 6: Qwen2 0.5B - Ultra-fast tiny model
6|8095|Qwen/Qwen2-0.5B-Instruct-GGUF|qwen2-0_5b-instruct-q4_k_m.gguf|

# Model 7: Qwen3 8B - Balanced general purpose
7|8096|bartowski/Qwen3-8B-GGUF|Qwen3-8B-Q4_K_M.gguf|
