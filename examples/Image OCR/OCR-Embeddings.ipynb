{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-huggingface\n",
    "%pip install langchain-neo4j\n",
    "\n",
    "%pip install langchain\n",
    "%pip install langchain-community\n",
    "%pip install langchain-text-splitters\n",
    "%pip install neo4j\n",
    "%pip install sentence-transformers\n",
    "%pip install python-dotenv\n",
    "%pip install pydantic\n",
    "%pip install pydantic_core\n",
    "%pip install numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Any\n",
    "import time\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Configure Environment and Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Neo4j connection settings\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\", \"bolt://localhost:7687\")\n",
    "NEO4J_USERNAME = os.getenv(\"NEO4J_USERNAME\", \"\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\", \"\")\n",
    "NEO4J_DATABASE = os.getenv(\"NEO4J_DATABASE\", \"neo4j\")\n",
    "\n",
    "# Initialize Neo4j connection\n",
    "graph = Neo4jGraph(\n",
    "    url=NEO4J_URI,\n",
    "    username=NEO4J_USERNAME,\n",
    "    password=NEO4J_PASSWORD,\n",
    "    database=NEO4J_DATABASE\n",
    ")\n",
    "\n",
    "# Initialize embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initialize text splitter\n",
    "text_splitter = SentenceTransformersTokenTextSplitter(\n",
    "    chunk_overlap=20, \n",
    "    tokens_per_chunk=256\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Create Index in Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_neo4j_indexes():\n",
    "    \"\"\"Create vector index in Neo4j database for similarity search.\"\"\"\n",
    "    # Create constraint for unique chunk IDs\n",
    "    graph.query(\"\"\"\n",
    "    CREATE CONSTRAINT IF NOT EXISTS FOR (c:Chunk) REQUIRE c.id IS UNIQUE\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create vector index for embeddings\n",
    "    graph.query(\"\"\"\n",
    "    CREATE VECTOR INDEX image_chunk_embeddings IF NOT EXISTS\n",
    "    FOR (c:Chunk) \n",
    "    ON c.embedding\n",
    "    OPTIONS {\n",
    "        indexConfig: {\n",
    "            `vector.dimensions`: 384,\n",
    "            `vector.similarity_function`: 'cosine'\n",
    "        }\n",
    "    }\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"Neo4j indexes created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_path_components(path: str) -> Dict[str, str]:\n",
    "    \"\"\"Extract folder and file components from a path structure.\"\"\"\n",
    "    components = path.split('/')\n",
    "    if len(components) >= 2:\n",
    "        folder = components[0]\n",
    "        subfolder = components[1] if len(components) > 1 else None\n",
    "        filename = components[-1]\n",
    "    else:\n",
    "        folder = None\n",
    "        subfolder = None\n",
    "        filename = components[0]\n",
    "    \n",
    "    return {\n",
    "        \"folder\": folder, \n",
    "        \"subfolder\": subfolder,\n",
    "        \"filename\": filename\n",
    "    }\n",
    "\n",
    "def extract_sections(text: str) -> Dict[str, str]:\n",
    "    \"\"\"Extract sections from OCR text based on numbered headers.\"\"\"\n",
    "    sections = {}\n",
    "    lines = text.split('\\n')\n",
    "    \n",
    "    current_section = None\n",
    "    current_content = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        # Check for section headers like \"1. Image Type and Category:\"\n",
    "        if any(line.startswith(f\"{i}. \") for i in range(1, 7)):\n",
    "            # Save previous section if exists\n",
    "            if current_section:\n",
    "                sections[current_section] = '\\n'.join(current_content).strip()\n",
    "            \n",
    "            # Start new section\n",
    "            current_section = line\n",
    "            current_content = []\n",
    "        else:\n",
    "            # Add line to current section content\n",
    "            if current_section:\n",
    "                current_content.append(line)\n",
    "    \n",
    "    # Add the last section\n",
    "    if current_section and current_content:\n",
    "        sections[current_section] = '\\n'.join(current_content).strip()\n",
    "    \n",
    "    # If there's a \"Detailed Description\" section\n",
    "    detailed_idx = next((i for i, line in enumerate(lines) if \"Detailed Description:\" in line), -1)\n",
    "    if detailed_idx >= 0:\n",
    "        detailed_text = '\\n'.join(lines[detailed_idx+1:]).strip()\n",
    "        sections[\"Detailed Description:\"] = detailed_text\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Document Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json_file(file_path: str) -> Dict:\n",
    "    \"\"\"Load and parse JSON file containing OCR data.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def create_metadata(\n",
    "    folder: str, \n",
    "    subfolder: str, \n",
    "    filename: str, \n",
    "    section_name: str\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Create metadata for a chunk.\"\"\"\n",
    "    return {\n",
    "        \"folder\": folder,\n",
    "        \"subfolder\": subfolder,\n",
    "        \"file_name\": filename,\n",
    "        \"file_type\": \"image\",\n",
    "        \"section\": section_name,\n",
    "    }\n",
    "\n",
    "def create_chunk_embedding(text: str, metadata: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Create embedding for a chunk with given text and metadata.\"\"\"\n",
    "    embedding = embedding_model.embed_query(text)\n",
    "    \n",
    "    # Create a unique ID based on metadata\n",
    "    chunk_id = f\"{metadata['folder']}_{metadata['subfolder']}_{metadata['file_name']}_{metadata['section']}\"\n",
    "    chunk_id = chunk_id.replace(\" \", \"_\").replace(\":\", \"\")\n",
    "    \n",
    "    return {\n",
    "        \"id\": chunk_id,\n",
    "        \"text\": text,\n",
    "        \"metadata\": metadata,\n",
    "        \"embedding\": embedding\n",
    "    }\n",
    "\n",
    "def save_chunk_to_neo4j(chunk_data: Dict[str, Any]):\n",
    "    \"\"\"Save chunk data to Neo4j.\"\"\"\n",
    "    query = \"\"\"\n",
    "    MERGE (c:Chunk {id: $id})\n",
    "    SET c.text = $text,\n",
    "        c.embedding = $embedding,\n",
    "        c.folder = $metadata.folder,\n",
    "        c.subfolder = $metadata.subfolder,\n",
    "        c.file_name = $metadata.file_name,\n",
    "        c.file_type = $metadata.file_type,\n",
    "        c.section = $metadata.section\n",
    "    RETURN c\n",
    "    \"\"\"\n",
    "    \n",
    "    result = graph.query(\n",
    "        query=query,\n",
    "        params={\n",
    "            \"id\": chunk_data[\"id\"],\n",
    "            \"text\": chunk_data[\"text\"],\n",
    "            \"embedding\": chunk_data[\"embedding\"],\n",
    "            \"metadata\": chunk_data[\"metadata\"]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Main Processing Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ocr_data(data: Dict):\n",
    "    \"\"\"Process all OCR data from the loaded JSON.\"\"\"\n",
    "    total_chunks = 0\n",
    "    \n",
    "    # Process the nested structure\n",
    "    for folder, subfolders in data.items():\n",
    "        print(f\"Processing folder: {folder}\")\n",
    "        \n",
    "        for subfolder, files in subfolders.items():\n",
    "            print(f\"  Processing subfolder: {subfolder}\")\n",
    "            \n",
    "            for filename, ocr_text in files.items():\n",
    "                print(f\"    Processing file: {filename}\")\n",
    "                \n",
    "                # Extract sections from the OCR text\n",
    "                sections = extract_sections(ocr_text)\n",
    "                \n",
    "                # Process each section\n",
    "                for section_name, section_content in sections.items():\n",
    "                    # Create chunks from the section text\n",
    "                    chunks = text_splitter.split_text(section_content)\n",
    "                    \n",
    "                    for chunk in chunks:\n",
    "                        # Create metadata\n",
    "                        metadata = create_metadata(\n",
    "                            folder=folder,\n",
    "                            subfolder=subfolder,\n",
    "                            filename=filename,\n",
    "                            section_name=section_name\n",
    "                        )\n",
    "                        \n",
    "                        # Create embedding\n",
    "                        chunk_data = create_chunk_embedding(\n",
    "                            text=chunk,\n",
    "                            metadata=metadata\n",
    "                        )\n",
    "                        \n",
    "                        # Save to Neo4j\n",
    "                        save_chunk_to_neo4j(chunk_data)\n",
    "                        total_chunks += 1\n",
    "    \n",
    "    print(f\"Completed processing. Total chunks created: {total_chunks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Query Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_chunks(query_text: str, top_k: int = 5):\n",
    "    \"\"\"Search for similar chunks based on the query text.\"\"\"\n",
    "    # Generate embedding for the query\n",
    "    query_embedding = embedding_model.embed_query(query_text)\n",
    "    \n",
    "    # Search in Neo4j using vector similarity\n",
    "    search_query = \"\"\"\n",
    "    CALL db.index.vector.queryNodes('image_chunk_embeddings', $top_k, $embedding)\n",
    "    YIELD node, score\n",
    "    RETURN \n",
    "        node.id as id,\n",
    "        node.text as text,\n",
    "        node.folder as folder,\n",
    "        node.subfolder as subfolder,\n",
    "        node.file_name as file_name,\n",
    "        node.section as section,\n",
    "        score\n",
    "    ORDER BY score DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    results = graph.query(\n",
    "        query=search_query,\n",
    "        params={\"embedding\": query_embedding, \"top_k\": top_k}\n",
    "    )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    # Setup Neo4j indexes\n",
    "    setup_neo4j_indexes()\n",
    "    \n",
    "    # Specify path to your JSON file\n",
    "    json_file_path = \"./final_image_sonnet.json\"\n",
    "    \n",
    "    # Process the OCR data\n",
    "    print(f\"Loading data from {json_file_path}...\")\n",
    "    data = process_json_file(json_file_path)\n",
    "    \n",
    "    # Process the data\n",
    "    start_time = time.time()\n",
    "    process_ocr_data(data)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Processing completed in {end_time - start_time:.2f} seconds.\")\n",
    "    \n",
    "    # Example search\n",
    "    print(\"\\nExample search:\")\n",
    "    search_results = search_similar_chunks(\"What does the official seal look like?\", top_k=3)\n",
    "    for result in search_results:\n",
    "        print(f\"Score: {result['score']:.4f}\")\n",
    "        print(f\"Document: {result['folder']}/{result['subfolder']}/{result['file_name']}\")\n",
    "        print(f\"Section: {result['section']}\")\n",
    "        print(f\"Text: {result['text'][:100]}...\\n\")\n",
    "\n",
    "# Run the main function when executing the notebook\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
