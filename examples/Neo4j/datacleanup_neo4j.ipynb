{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55cb036-12e8-4542-b540-1851bc21bfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install neo4j\n",
    "!pip install bs4\n",
    "!pip install llama-index\n",
    "!pip uninstall -y trulens_eval\n",
    "#!pip install trulens-eval==0.25.1\n",
    "!pip install llmlingua\n",
    "!pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6800097d-8bcd-44ef-b328-d2f936f5cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y trulens_eval\n",
    "!pip install trulens-eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df74b51-25ab-4acb-b9e1-022f8fb7872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_community.vectorstores import Neo4jVector\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "import warnings\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from llama_index.core import SimpleDirectoryReader, StorageContext\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from sentence_transformers import CrossEncoder\n",
    "import numpy as np\n",
    "from trulens_eval import Tru\n",
    "from trulens_eval.tru_custom_app import instrument\n",
    "from llmlingua import PromptCompressor\n",
    "import re\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84993598-ecc2-4eeb-a1ad-d1ee9813f169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_wrap(string, n_chars=72):\n",
    "    # Wrap a string at the next space after n_chars\n",
    "    if len(string) < n_chars:\n",
    "        return string\n",
    "    else:\n",
    "        return string[:n_chars].rsplit(' ', 1)[0] + '\\n' + word_wrap(string[len(string[:n_chars].rsplit(' ', 1)[0])+1:], n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4b8599-83bd-4b87-90aa-8aee7751e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URI = 'bolt://' + os.getenv('NEO4J_HOST') + ':7687'\n",
    "NEO4J_USERNAME = os.getenv('NEO4J_USER')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "NEO4J_DATABASE = 'neo4j' #os.getenv('NEO4J_DB')\n",
    "print(NEO4J_URI)\n",
    "print(NEO4J_DATABASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18877c47-5303-45d6-992b-d6760dde8556",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg = Neo4jGraph(\n",
    "    url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD, database=NEO4J_DATABASE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4cd2f1-2d4d-4c7d-80d2-667352cb686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cypher = \"\"\"\n",
    "  MATCH (n) \n",
    "  RETURN count(n)\n",
    "  \"\"\"\n",
    "result = kg.query(cypher)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cc37b8-ba42-408c-b504-ad047bbdfcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5795d6f2-bb1c-40bd-ab99-00436cbf9e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_metadata = lambda x: {\"filename\": x}\n",
    "Acts_documents = SimpleDirectoryReader(\"./XML\",file_metadata=file_metadata).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849dc678-c299-416e-be23-9c642c982354",
   "metadata": {},
   "source": [
    "## Working with the 1st section\n",
    "\n",
    "#### bcl - B.C Laws #####\n",
    "bcl:num gives the number of sections, subsections and something more\n",
    "\n",
    "bcl:section gives all the sections and subsections\n",
    "\n",
    "bcl:marginalnote gices the section heading\n",
    "\n",
    "if the section is a definition then each definition has a definition tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b5b91b-3302-40a0-bb6b-c940582cbea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_1 = num_sections[2]\n",
    "section_heading = section_1.find_all('bcl:marginalnote')\n",
    "section_definitions = section_1.find_all('bcl:definition')\n",
    "section_subsection = section_1.find_all('bcl:subsection')\n",
    "print(section_1)\n",
    "print(len(section_subsection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ebf4b2-eb39-4cc7-9aac-8d58313105a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_chunk_node_query = \"\"\"\n",
    "MERGE(mergedChunk:UpdatedChunk {chunkId: $chunkParam.chunkId})\n",
    "    ON CREATE SET \n",
    "        mergedChunk.chunkSeqId = $chunkParam.chunkSeqId, \n",
    "        mergedChunk.text = $chunkParam.text,\n",
    "        mergedChunk.ActId = $chunkParam.ActId,\n",
    "        mergedChunk.sectionId = $chunkParam.sectionId,\n",
    "        mergedChunk.sectionName = $chunkParam.sectionName\n",
    "RETURN mergedChunk\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf0ffcb-ac6b-412a-9825-b62b74f5abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_chunk_node_query =  \"\"\"\n",
    "        MATCH (chunk:UpdatedChunk) WHERE\n",
    "        chunk.chunkId = $chunkParam.chunkId\n",
    "        AND chunk.chunkSeqId = $chunkParam.chunkSeqId\n",
    "        AND chunk.ActId = $chunkParam.ActId\n",
    "        AND chunk.sectionId = $chunkParam.sectionId\n",
    "        AND chunk.sectionName = $chunkParam.sectionName\n",
    "        AND chunk.text = $chunkParam.text\n",
    "        RETURN chunk\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dddbc7-33d9-4749-93d5-a879d3ceed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a7b3a-309a-4ecb-95a2-fdaf7fb20e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_embeddings = \"\"\"\n",
    "        MATCH (chunk:UpdatedChunk) WHERE\n",
    "        chunk.chunkId = $chunkParam.chunkId\n",
    "        AND chunk.chunkSeqId = $chunkParam.chunkSeqId\n",
    "        AND chunk.ActId = $chunkParam.ActId\n",
    "        AND chunk.sectionId = $chunkParam.sectionId\n",
    "        AND chunk.sectionName = $chunkParam.sectionName\n",
    "        AND chunk.text = $chunkParam.text\n",
    "        AND chunk.textEmbedding is NULL\n",
    "        CALL db.create.setNodeVectorProperty(chunk, \"textEmbedding\", $vector)\n",
    "        RETURN chunk\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715225f5-d64b-45c4-9c63-d2c784a559a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "connect_chunks = \"\"\"\n",
    "      MATCH (chunk:UpdatedChunk), (f:UpdatedChunk)\n",
    "      WHERE\n",
    "        chunk.chunkId = $chunkParam.chunkId\n",
    "        AND chunk.chunkSeqId = $chunkParam.chunkSeqId\n",
    "        AND chunk.ActId = $chunkParam.ActId\n",
    "        AND chunk.sectionId = $chunkParam.sectionId\n",
    "        AND chunk.sectionName = $chunkParam.sectionName\n",
    "        AND f.ActId = $chunkParam.ActId\n",
    "        AND f.sectionId = $chunkParam.connectnedsectionId\n",
    "        AND f.chunkSeqId = 0\n",
    "      MERGE (chunk)-[newRelationship:REFERENCE]->(f)\n",
    "      RETURN count(newRelationship)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bba0bde-5091-493a-be0a-4de55a5080d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunk_relationship(act_info):\n",
    "    cypher = \"\"\"\n",
    "  MATCH (from_same_section:UpdatedChunk)\n",
    "  WHERE from_same_section.ActId = $ActParam['ActId']\n",
    "  AND from_same_section.sectionName = $ActParam['sectionName']\n",
    "  AND from_same_section.sectionId = $ActParam['sectionId']\n",
    "  WITH from_same_section\n",
    "    ORDER BY from_same_section.chunkSeqId ASC\n",
    "  WITH collect(from_same_section) as section_chunk_list\n",
    "    CALL apoc.nodes.link(\n",
    "        section_chunk_list, \n",
    "        \"NEXT\", \n",
    "        {avoidDuplicates: true}\n",
    "    )  // NEW!!!\n",
    "  RETURN size(section_chunk_list)\n",
    "\"\"\"\n",
    "    kg.query(cypher, params={'ActParam': act_info})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b3c22-9afb-460a-a511-0167d590df3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function will search for any reference that has the word section or subsection followed by a number\n",
    "def extract_references(str, index):\n",
    "    references = re.findall(r\"section (\\d+)\", str)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9161709f-5d67-4fd7-8b1a-2c5d2fe21edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_links(subsection):\n",
    "    if not subsection:\n",
    "        return\n",
    "    if (subsection.find_all(\"bcl:link\")):\n",
    "        xml_link = subsection.find_all(\"bcl:link\")[0]['xlink:href']\n",
    "        display(HTML(f'<a href=\"{xml_link}\">{subsection.find_all(\"bcl:link\")[0].get_text()}</a>'))\n",
    "    extract_references(subsection.get_text().replace(\"\\n\\n\", \"\").replace(\"\\r\", \"\"), index)\n",
    "    return subsection.get_text().replace(\"\\n\", \" \").replace(\"\\r\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30df312-23ba-4976-b25b-cb118fe3b3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get subsection\n",
    "def subsection(section_definitions, index):\n",
    "    string = \"\"\n",
    "    for subsection_index, subsection in enumerate(section_definitions):\n",
    "        string += \"\\n\" + find_links(subsection)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23416766-7217-4a2b-9ca0-d8c16b082521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metadata(token_split_texts,title, section_heading, section_id):\n",
    "    chunks_with_metadata = [] # use this to accumlate chunk records\n",
    "    chunk_seq_id = 0\n",
    "    for chunk in token_split_texts: # only take the first 20 chunks\n",
    "        #form_id = file[file.rindex('/') + 1:file.rindex('.')] # extract form id from file name\n",
    "        # finally, construct a record with metadata and the chunk text\n",
    "        chunks_with_metadata.append({\n",
    "            'text': chunk, \n",
    "            # metadata from looping...\n",
    "            'chunkSeqId': chunk_seq_id,\n",
    "            'chunkId': f'{title}-chunk-{section_heading}-{chunk_seq_id:04d}',\n",
    "            'ActId': f'{title}',\n",
    "            'sectionId': f'{section_id}',\n",
    "            'sectionName':f'{section_heading}',\n",
    "            # constructed metadata...\n",
    "            # metadata from file...\n",
    "        })\n",
    "        chunk_seq_id += 1\n",
    "    return chunks_with_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa4c984-ca82-4a28-b699-04c1bb96013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(item_text, title, section_heading, section_id):\n",
    "    item_text_chunks = text_splitter.split_text(item_text) # split the text into chunks\n",
    "    token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=20, tokens_per_chunk=256)\n",
    "    token_split_texts = []\n",
    "    for text in item_text_chunks:\n",
    "        token_split_texts += token_splitter.split_text(text)\n",
    "    meta_data = create_metadata(token_split_texts,title, section_heading, section_id)    \n",
    "    return meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615d3e50-fd5c-48d9-9eb3-ee8ab6adeca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_create_reference(match):\n",
    "    #print(match[0]['chunk'])\n",
    "    text = match[0]['chunk']['text']\n",
    "    references = re.findall(r\"(?i)(?:section)\\s+(\\d+|\\(\\d+\\))(?:\\s*\\([a-z]\\))?\", text)\n",
    "    if (references):\n",
    "        print(match[0]['chunk'])\n",
    "        print(references)\n",
    "        # create the edges\n",
    "        print(\"Match found - creating references\")\n",
    "        chunk_seq_id = match[0]['chunk']['chunkSeqId']\n",
    "        section_heading = match[0]['chunk']['sectionName']\n",
    "        section_id = match[0]['chunk']['sectionId']\n",
    "        chunk = {\n",
    "            'text': text, \n",
    "            # metadata from looping...\n",
    "            'chunkSeqId': match[0]['chunk']['chunkSeqId'],\n",
    "            'chunkId': match[0]['chunk']['chunkId'],\n",
    "            'ActId': match[0]['chunk']['ActId'],\n",
    "            'sectionId': match[0]['chunk']['sectionId'],\n",
    "            'sectionName': match[0]['chunk']['sectionName'],\n",
    "            'connectnedsectionId': references[0]\n",
    "        }\n",
    "        result = kg.query(connect_chunks,\n",
    "                params={\n",
    "                    'chunkParam':chunk\n",
    "                }\n",
    "                )\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9f485e-5728-45d3-b464-96b583edd546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunk_embeddings(tokens):\n",
    "    for chunk in tokens:\n",
    "        print(f\"Creating `:Chunk` node for chunk ID {chunk['chunkSeqId']}\")\n",
    "        kg.query(merge_chunk_node_query, \n",
    "            params={\n",
    "                'chunkParam': chunk\n",
    "            })\n",
    "        vector = embeddings.embed_query(chunk['text'])\n",
    "        result = kg.query(create_embeddings, params={'chunkParam':chunk, 'vector':vector})\n",
    "        if result:\n",
    "            print(\"Embedding created\")\n",
    "        else:\n",
    "            print(result)\n",
    "            print(\"Embedding not created\")\n",
    "    create_chunk_relationship(tokens[0])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fb3e87-7986-4efc-99ed-622680aa82ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunk_neo4j(tokens, search=False):\n",
    "    for chunk in tokens:\n",
    "        if search:\n",
    "            vector = embeddings.embed_query(chunk['text'])\n",
    "            result = kg.query(create_embeddings, params={'chunkParam':chunk, 'vector':vector})\n",
    "            if result:\n",
    "                print(\"Embedding creating\")\n",
    "            else:\n",
    "                print(result)\n",
    "                return\n",
    "        else:\n",
    "            print(f\"Creating `:Chunk` node for chunk ID {chunk['chunkSeqId']}\")\n",
    "            kg.query(merge_chunk_node_query, \n",
    "                    params={\n",
    "                        'chunkParam': chunk\n",
    "                    })\n",
    "    if (not search):\n",
    "        create_chunk_relationship(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dd475e-989e-49a6-9944-4b70a3c7d316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(file, index,  search=False):\n",
    "    #get the ACT's title\n",
    "    title = file.find('act:title')\n",
    "    if title:\n",
    "        title = title.get_text()\n",
    "        print(title)\n",
    "    else:\n",
    "        return\n",
    "    #get all the sections\n",
    "    preamble = file.find_all('bcl:preamble')\n",
    "    if (preamble):\n",
    "        item_text = subsection(preamble, 0)\n",
    "        token = create_chunks(item_text, title, 'preamble', 0)\n",
    "    sections = file.find_all('bcl:section')\n",
    "    #find the definition subsection\n",
    "    for index, section in enumerate(sections):\n",
    "        token = []\n",
    "        section_heading = section.find('bcl:marginalnote')#[0].get_text()\n",
    "        if (section_heading):\n",
    "            section_heading = section_heading.get_text()\n",
    "        else:\n",
    "            section_heading = \"\"\n",
    "        section_definitions = section.find_all('bcl:definition')\n",
    "        if (len(section_definitions) < 1):\n",
    "            #find the remaining subsection\n",
    "            section_subsection = section.find_all('bcl:subsection')\n",
    "            #find the section number\n",
    "            section_number = section.find('bcl:num').get_text()\n",
    "            if len(section_number):\n",
    "                print(\"section number is:\" + section_number)\n",
    "            #print(section_subsection)\n",
    "            if len(section_subsection):\n",
    "                item_text = subsection(section_subsection, index+1)\n",
    "                print(item_text)\n",
    "                token = create_chunks(item_text, title, section_heading, section_number)\n",
    "            else:\n",
    "                item_text = find_links(section)\n",
    "                token = create_chunks(item_text, title, section_heading, section_number)\n",
    "        else:\n",
    "            item_text = subsection(section_definitions, index+1)\n",
    "            #print(item_text)\n",
    "            token = create_chunks(item_text, title, section_heading, index+1)\n",
    "        found = create_chunk_embeddings(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9159ad29-4457-416c-8c39-8ce46bf96ca9",
   "metadata": {},
   "source": [
    "# This cell will create chunks based on sections of the Acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde21a12-dbcf-40da-9aba-01ed87761358",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, Acts in enumerate(Acts_documents):\n",
    "    soup = BeautifulSoup(Acts.get_text(), 'xml')\n",
    "    extract_data(soup, index,  False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
