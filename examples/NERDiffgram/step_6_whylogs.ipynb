{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2889f1db-6923-4ab1-a2bf-29a3feea6b3c",
   "metadata": {},
   "source": [
    "## Step 6: WhyLogs Validation\n",
    "\n",
    "This notebook performs validation of the **metadata annotations** using WhyLogs. It helps ensure that fields like `Act ID`, `Section ID`, and other metadata labels are applied consistently and correctly.\n",
    "\n",
    "In addition, it attempts to validate whether the word **\"section\"** is annotated correctly. However, this check is inherently brittle and may produce false positives, since the word \"section\" can appear in both referential and non-referential contexts. Despite this limitation, tracking occurrences of \"section\" still provides useful insights into how it's being labeled across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4bfbe6-4af8-47ac-ab6d-5e4a352897ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3\n",
    "!pip install torch transformers diffgram neo4j anthropic pandas tqdm\n",
    "!pip install llama_index\n",
    "!pip install matplotlib\n",
    "!pip install tabulate\n",
    "!pip install 'whylogs[viz]'\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "679e660c-ade3-400a-985d-c661f1708692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from diffgram import Project\n",
    "from typing import List, Dict, Optional\n",
    "import anthropic\n",
    "import json\n",
    "from neo4j import GraphDatabase\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import requests\n",
    "import pprint\n",
    "import json\n",
    "from diffgram import Project\n",
    "from llama_index.core import SimpleDirectoryReader, StorageContext\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib as mpl\n",
    "from tabulate import tabulate\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bec641c4-a6ef-4857-87ce-739bae184fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use os.getcwd() since __file__ is not available in interactive environments\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# If your structure is such that the package is in the parent directory, compute the parent directory:\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "\n",
    "# Add the parent directory to sys.path if it's not already there\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5530d7d1-0e72-4568-a750-93bbafd823aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import diffgram_utils as du"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a516f37e-4111-40e7-baf5-ae757d5319af",
   "metadata": {},
   "source": [
    "## Connect  to Diffgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77156ce4-488c-44dd-a5d9-0120c8946734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DIFFGRAM_CONFIG = {\n",
    "    \"host\": \"http://dispatcher:8085\",\n",
    "    \"project_string_id\": \"translucenttracker\",\n",
    "    \"client_id\": \"LIVE__u3v8q0m7tx1p851dp0ap\",\n",
    "    \"client_secret\": \"1qgd8as7xfcbuem6mw9j1z0xvjfmmvlagbugqr8z1g1ntypugr2ul24cce5k\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cca7cc09-d9b0-4f9c-bd5f-0be41fc2f316",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = Project(host=DIFFGRAM_CONFIG[\"host\"],\n",
    "        project_string_id = \"translucenttracker\",\n",
    "        client_id = \"LIVE__u3v8q0m7tx1p851dp0ap\",\n",
    "        client_secret = \"1qgd8as7xfcbuem6mw9j1z0xvjfmmvlagbugqr8z1g1ntypugr2ul24cce5k\"\n",
    "      )\n",
    "project_local = project\n",
    "auth = project.session.auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da3ba714-2552-455f-af92-02c8378f5258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 256\n",
    "NUM_TRAIN_SAMPLES = 5440  # Number of samples to use for training\n",
    "NUM_TRAINING_DATA = 5440\n",
    "train_dataset_suffix = \"NER_train_batch_\"\n",
    "test_dataset_suffix = \"NER_test_batch_\"\n",
    "JOB_NAME = \"Law_NER_task1\"\n",
    "JOB_TRAIN_SUFFIX = \"NER_train_JOB_\"\n",
    "JOB_TEST_SUFFIX = \"NER_test_JOB_\"\n",
    "MAX_NUM_OF_TASK = 250\n",
    "NER_schema_name = 'ENTITY_TRAINING_SCHEMA'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9ada4a-aed7-49e5-b1fb-4e4640261cda",
   "metadata": {},
   "source": [
    "## Get  schema ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72269f58-8186-4bbd-96c6-e8e863ef2fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing Schemas in Diffgram:\n",
      "[\n",
      "  {\n",
      "    \"archived\": false,\n",
      "    \"id\": 8,\n",
      "    \"is_default\": true,\n",
      "    \"member_created_id\": 1,\n",
      "    \"member_updated_id\": null,\n",
      "    \"name\": \"Default Schema\",\n",
      "    \"project_id\": 4,\n",
      "    \"time_created\": \"2025-02-04 22:16:17\",\n",
      "    \"time_updated\": null\n",
      "  },\n",
      "  {\n",
      "    \"archived\": false,\n",
      "    \"id\": 9,\n",
      "    \"is_default\": false,\n",
      "    \"member_created_id\": 10,\n",
      "    \"member_updated_id\": null,\n",
      "    \"name\": \"NER_TRAINING_SCHEMA\",\n",
      "    \"project_id\": 4,\n",
      "    \"time_created\": \"2025-02-05 17:08:24\",\n",
      "    \"time_updated\": null\n",
      "  },\n",
      "  {\n",
      "    \"archived\": false,\n",
      "    \"id\": 11,\n",
      "    \"is_default\": false,\n",
      "    \"member_created_id\": 10,\n",
      "    \"member_updated_id\": null,\n",
      "    \"name\": \"ENTITY_TRAINING_SCHEMA\",\n",
      "    \"project_id\": 4,\n",
      "    \"time_created\": \"2025-02-05 17:20:02\",\n",
      "    \"time_updated\": null\n",
      "  }\n",
      "]\n",
      "Schema 'ENTITY_TRAINING_SCHEMA' already exists with id: 11\n"
     ]
    }
   ],
   "source": [
    "schema_id = du.find_schema(NER_schema_name, project)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e9f581-b43b-4147-914d-caad8a026c80",
   "metadata": {},
   "source": [
    "## Get diffgram tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4dc43e45-869e-493f-933b-58aa1acf950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = project.job\n",
    "get_job = project.job.list(limit=10000, page_number=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a43233a4-723f-4cba-bd6c-54ee882ea434",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_with_data_index = du.get_all_tasks(project, get_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5808faa7-497a-4327-8b24-967166eab7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#jobs_with_data_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e449c8f-f848-4c85-b542-6ac83efb8172",
   "metadata": {},
   "source": [
    "## NER Schema labesl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a597175-fbdc-46fc-bffc-3020f25a9bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_list = du.get_schema_list(schema_id, project, DIFFGRAM_CONFIG)\n",
    "ner_schema_list = []\n",
    "for labels in schema_list['labels_out']:\n",
    "    #print(labels['label']['name'])\n",
    "    ner_schema_list.append(labels['label']['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75fbc8e-6ede-4861-87b5-fc715bc70c0e",
   "metadata": {},
   "source": [
    "## Populate df of all the data for whylogs to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c6a37105-cfd7-40af-aeda-22e4c3f1c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_arrange(completed_annotations, pd_idx, pd_word, pd_schema, pd_job_id, dataset_id, pd_job_name, pd_dataset_url, job_index):\n",
    "    for completed_annotation in completed_annotations:\n",
    "        #print(f\"{completed_annotation} ----\")\n",
    "        if completed_annotation in [\"attribute_groups_reference\", \"export_info\", \"label_map\", \"readme\", \"label_colour_map\"]:\n",
    "            continue  # Skip metadata entries\n",
    "        sentence_local = []\n",
    "        labels_local = []\n",
    "        task_url = None\n",
    "\n",
    "        job_id = get_job[job_index[\"index\"]][\"id\"]\n",
    "        if job_id and auth:\n",
    "            task_url = du.get_diffgram_task_url(job_id, completed_annotation, auth, DIFFGRAM_CONFIG)\n",
    "            task_url = task_url.replace(\"dispatcher\", \"localhost\") if task_url else \"URL Not Found\"\n",
    "\n",
    "            \n",
    "        #print(completed_annotations[completed_annotation]['text']['tokens']['words'])\n",
    "        for idx, words in enumerate(completed_annotations[completed_annotation]['text']['tokens']['words']):\n",
    "            #print(idx)\n",
    "            #print(words)\n",
    "            #print(f\"Idx: {idx} Word: {words['value']}\")\n",
    "            ## find the token value\n",
    "            for start in completed_annotations[completed_annotation]['instance_list']:\n",
    "                if 'start_token' in start:\n",
    "                    if idx == start['start_token']:\n",
    "                        ## Find the schmea value\n",
    "                        schema_id = completed_annotations['label_map'][str(start['label_file_id'])]\n",
    "                        #print(f\"Idx: {idx} {words['value']} {start['start_token']} schmea: {schema_id}\")\n",
    "                        pd_idx.append(idx)\n",
    "                        pd_word.append(words['value'])\n",
    "                        pd_schema.append(schema_id)\n",
    "                        pd_job_id.append(job_index['index'])  \n",
    "                        pd_job_name.append(job_index['nickname'])\n",
    "                        pd_dataset_id.append(completed_annotation)\n",
    "                        pd_dataset_url.append(f'<a href=\"{task_url}\" target=\"_blank\">View</a>')\n",
    "                                # Get task URL if parameters are provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ffb81c8-7df1-4c77-ae56-46855075550e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing annotations from annotation.parquet\n",
      "Loaded DataFrame with 1195135 rows and 8000 unique datasets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the Parquet file path\n",
    "parquet_file = \"annotation.parquet\"\n",
    "\n",
    "# Check if the Parquet file already exists\n",
    "if os.path.exists(parquet_file):\n",
    "    print(f\"Loading existing annotations from {parquet_file}\")\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "else:\n",
    "    print(\"Parquet file not found. Processing annotations from Diffgram...\")\n",
    "    # Initialize your lists\n",
    "    files_index_in_job_total = []\n",
    "    pd_idx = []\n",
    "    pd_word = []\n",
    "    pd_schema = []\n",
    "    pd_job_id = []\n",
    "    pd_job_name = []\n",
    "    pd_dataset_url = []\n",
    "    pd_dataset_id = []\n",
    "    \n",
    "    # Process the data\n",
    "    start_index = 0\n",
    "    last_index = 255  # len(jobs_with_data_index)\n",
    "    for job_index in jobs_with_data_index[start_index:last_index]:\n",
    "        print(f\"The job nickname is {job_index['nickname']} and the index is {job_index['index']}\")\n",
    "        results.refresh_from_dict(get_job[job_index['index']])\n",
    "        completed_annotations = results.generate_export()\n",
    "        extract_and_arrange(completed_annotations, pd_idx, pd_word, pd_schema, pd_job_id, \n",
    "                           pd_dataset_id, pd_job_name, pd_dataset_url, job_index)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'dataset_id': pd_dataset_id,\n",
    "        'token index': pd_idx,\n",
    "        'word': pd_word,\n",
    "        'schema': pd_schema,\n",
    "        'job_id': pd_job_id,\n",
    "        'job_name': pd_job_name,\n",
    "        'task_url': pd_dataset_url\n",
    "    })\n",
    "    \n",
    "    # Save to Parquet\n",
    "    print(f\"Saving annotations to {parquet_file}\")\n",
    "    df.to_parquet(parquet_file, engine=\"pyarrow\", index=False)\n",
    "\n",
    "# Now df is available for further processing in either case\n",
    "print(f\"Loaded DataFrame with {len(df)} rows and {df['dataset_id'].nunique()} unique datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "67b58ebd-585d-4362-9a32-5199f24e3506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>token index</th>\n",
       "      <th>word</th>\n",
       "      <th>schema</th>\n",
       "      <th>job_id</th>\n",
       "      <th>job_name</th>\n",
       "      <th>task_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20723</td>\n",
       "      <td>0</td>\n",
       "      <td>Chunk</td>\n",
       "      <td>B-METADATA_FIELD</td>\n",
       "      <td>0</td>\n",
       "      <td>NER_train_batch_249</td>\n",
       "      <td>&lt;a href=\"http://localhost:8085/task/36909?file...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20723</td>\n",
       "      <td>1</td>\n",
       "      <td>ID</td>\n",
       "      <td>I-METADATA_FIELD</td>\n",
       "      <td>0</td>\n",
       "      <td>NER_train_batch_249</td>\n",
       "      <td>&lt;a href=\"http://localhost:8085/task/36909?file...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20723</td>\n",
       "      <td>2</td>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>0</td>\n",
       "      <td>NER_train_batch_249</td>\n",
       "      <td>&lt;a href=\"http://localhost:8085/task/36909?file...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20723</td>\n",
       "      <td>3</td>\n",
       "      <td>Homeowner</td>\n",
       "      <td>B-CHUNK_ID</td>\n",
       "      <td>0</td>\n",
       "      <td>NER_train_batch_249</td>\n",
       "      <td>&lt;a href=\"http://localhost:8085/task/36909?file...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20723</td>\n",
       "      <td>4</td>\n",
       "      <td>Protection</td>\n",
       "      <td>I-CHUNK_ID</td>\n",
       "      <td>0</td>\n",
       "      <td>NER_train_batch_249</td>\n",
       "      <td>&lt;a href=\"http://localhost:8085/task/36909?file...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  dataset_id  token index        word            schema  job_id  \\\n",
       "0      20723            0       Chunk  B-METADATA_FIELD       0   \n",
       "1      20723            1          ID  I-METADATA_FIELD       0   \n",
       "2      20723            2           :                 O       0   \n",
       "3      20723            3   Homeowner        B-CHUNK_ID       0   \n",
       "4      20723            4  Protection        I-CHUNK_ID       0   \n",
       "\n",
       "              job_name                                           task_url  \n",
       "0  NER_train_batch_249  <a href=\"http://localhost:8085/task/36909?file...  \n",
       "1  NER_train_batch_249  <a href=\"http://localhost:8085/task/36909?file...  \n",
       "2  NER_train_batch_249  <a href=\"http://localhost:8085/task/36909?file...  \n",
       "3  NER_train_batch_249  <a href=\"http://localhost:8085/task/36909?file...  \n",
       "4  NER_train_batch_249  <a href=\"http://localhost:8085/task/36909?file...  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d25bec6-62f3-4c72-902e-21f6c11766ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(HTML(df.to_html(escape=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0126fa51-3e2c-45e8-9301-0eea865f81a2",
   "metadata": {},
   "source": [
    "## Whylogs Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "299bfbb0-444f-4c8f-8563-7f0a2a0108b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_b_metadata_field_count = lambda x: {\n",
    "    item.value: item.est for item in x.to_summary_dict()['frequent_strings']\n",
    "}.get('B-METADATA_FIELD', 0) == 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5488f49f-c98f-4166-8d05-a8ea48d00dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_b_metadata_value_count = lambda x: {\n",
    "    item.value: item.est for item in x.to_summary_dict()['frequent_strings']\n",
    "}.get('B-METADATA_VALUE', 0) == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea7bccd9-8e51-4af6-aba2-e95f5ccd11fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_metadata_annotation_overall(metric):\n",
    "    \"\"\"Check if B-METADATA_VALUE count is 6 and the rest of the medata data is 0\"\"\"\n",
    "    item_counts = {\n",
    "        item.value: item.est for item in metric.to_summary_dict()['frequent_strings']\n",
    "    }\n",
    "    meta_field = item_counts.get('B-METADATA_FIELD', 0)\n",
    "    meta_value = item_counts.get('B-METADATA_VALUE', 0)\n",
    "    if ((meta_value ==  6) and (meta_field ==7)):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c2100b40-8a7c-43b4-823b-2e71438cf4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_section_annotation(profile_view, unique_df, dataset_id):\n",
    "    #print(f\"print the main {dataset_id}\")\n",
    "    try:\n",
    "        dataset_slice = df[df['dataset_id'] == dataset_id]\n",
    "        constraint = False\n",
    "        for word in dataset_slice:\n",
    "            if word['word'] == 'section':\n",
    "                if schema['schema'] == 'B-SECTION-REF' or schema['schema'] == 'I-SECTION-REF':\n",
    "                    constraint = True\n",
    "                    continue\n",
    "                else:\n",
    "                    constraint = False\n",
    "        return cosntraint\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "40a52a20-c51e-4ba8-9719-589f6483f8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_section(metric):\n",
    "    # your logic here\n",
    "    item_counts = {\n",
    "        item.value: item.est for item in metric.to_summary_dict()['frequent_strings']\n",
    "    }\n",
    "    constraint = True\n",
    "    for dataset_id in item_counts:\n",
    "        dataset_slice = df[df['dataset_id'] == dataset_id]\n",
    "        for idx, word in enumerate(dataset_slice['word']):\n",
    "            if word in ['section', 'sections']:\n",
    "                label = dataset_slice['schema'].iloc[idx]\n",
    "                #print(f\" {word} and label is {label}\")\n",
    "                if label not in ['B-SECTION_REF', 'I-SECTION_REF']:\n",
    "                    constraint = False\n",
    "                    break\n",
    "    #condition_result = True  # or False, based on your check\n",
    "    #metric.__dict__\n",
    "    # optionally add details to the second item\n",
    "    return constraint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1e4d95-3d65-4bc1-aace-3cca36f60a9f",
   "metadata": {},
   "source": [
    "## Profile dataset using whylogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "909972c3-8f02-45d9-b7dc-9d5de02cb8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing profile summaries from whylogs_profile_summaries.parquet\n",
      "Unique dataset IDs file already exists at annotation_unique.parquet\n",
      "Loading existing schema counts from schema_counts.parquet\n",
      "All WhyLogs processing and file saving completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import whylogs as why\n",
    "from whylogs.core import DatasetProfileView\n",
    "\n",
    "# Define all the file paths\n",
    "unique_file = \"annotation_unique.parquet\"\n",
    "profile_summaries_file = \"whylogs_profile_summaries.parquet\"\n",
    "schema_counts_file = \"schema_counts.parquet\"\n",
    "\n",
    "# Create profiles only if the profile summaries file doesn't exist\n",
    "if not os.path.exists(profile_summaries_file):\n",
    "    print(f\"WhyLogs profile summaries not found. Generating profiles...\")\n",
    "    \n",
    "    # Profile the entire dataset\n",
    "    profile = why.log(df)\n",
    "    \n",
    "    # Profile by dataset_id\n",
    "    profiles_by_dataset = {}\n",
    "    unique_df = df['dataset_id'].unique()\n",
    "    for dataset_id in unique_df:\n",
    "        dataset_slice = df[df['dataset_id'] == dataset_id]\n",
    "        profiles_by_dataset[dataset_id] = why.log(dataset_slice)\n",
    "    \n",
    "    # Create profile summaries\n",
    "    profile_summaries = []\n",
    "    for dataset_id, profile_obj in profiles_by_dataset.items():\n",
    "        # Get the profile view\n",
    "        view = profile_obj.view()\n",
    "        \n",
    "        # Extract schema frequency data\n",
    "        schema_data = view.get_column(\"schema\").get_metric(\"frequent_items\").to_summary_dict()\n",
    "        schema_counts = {item.value: item.est for item in schema_data.get('frequent_strings', [])}\n",
    "        \n",
    "        # Extract word frequency data\n",
    "        try:\n",
    "            word_data = view.get_column(\"word\").get_metric(\"frequent_items\").to_summary_dict()\n",
    "            word_counts = {item.value: item.est for item in word_data.get('frequent_strings', [])}\n",
    "        except:\n",
    "            word_counts = {}\n",
    "        \n",
    "        # Create summary dictionary\n",
    "        summary = {\n",
    "            \"dataset_id\": dataset_id,\n",
    "            \"timestamp\": pd.Timestamp.now(),\n",
    "            \"schema_counts\": str(schema_counts),\n",
    "            \"word_counts\": str(word_counts),\n",
    "            \"num_rows\": view.get_column(\"schema\").get_metric(\"counts\").to_summary_dict().get('n', 0)\n",
    "        }\n",
    "        \n",
    "        profile_summaries.append(summary)\n",
    "    \n",
    "    # Save profile summaries\n",
    "    profile_summary_df = pd.DataFrame(profile_summaries)\n",
    "    print(f\"Saving profile summaries to {profile_summaries_file}\")\n",
    "    profile_summary_df.to_parquet(profile_summaries_file, engine=\"pyarrow\", index=False)\n",
    "else:\n",
    "    print(f\"Loading existing profile summaries from {profile_summaries_file}\")\n",
    "    profile_summary_df = pd.read_parquet(profile_summaries_file)\n",
    "\n",
    "# Save unique dataset IDs if file doesn't exist\n",
    "if not os.path.exists(unique_file):\n",
    "    print(f\"Saving unique dataset IDs to {unique_file}\")\n",
    "    unique_df = pd.DataFrame({'dataset_id': df['dataset_id'].unique()})\n",
    "    unique_df.to_parquet(unique_file, engine=\"pyarrow\", index=False)\n",
    "else:\n",
    "    print(f\"Unique dataset IDs file already exists at {unique_file}\")\n",
    "    unique_df = pd.read_parquet(unique_file)\n",
    "\n",
    "# Calculate and save schema counts if file doesn't exist\n",
    "if not os.path.exists(schema_counts_file):\n",
    "    print(f\"Calculating schema counts and saving to {schema_counts_file}\")\n",
    "    schema_counts = df.groupby(['dataset_id', 'schema']).size().reset_index(name='count')\n",
    "    schema_counts.to_parquet(schema_counts_file, engine=\"pyarrow\", index=False)\n",
    "else:\n",
    "    print(f\"Loading existing schema counts from {schema_counts_file}\")\n",
    "    schema_counts = pd.read_parquet(schema_counts_file)\n",
    "\n",
    "print(\"All WhyLogs processing and file saving completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09ee21b-e681-4bec-af1a-0ad276ae28dd",
   "metadata": {},
   "source": [
    "## Process all the whylog profiles and check all the constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "862dcbd1-6006-4382-b8c2-e7b6db6a7226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whylogs as why\n",
    "from whylogs.core import DatasetProfileView\n",
    "\n",
    "# Profile the entire dataset\n",
    "profile = why.log(df)\n",
    "\n",
    "# You can also profile by dataset_id and job_id\n",
    "profiles_by_dataset = {}\n",
    "for dataset_id in unique_df['dataset_id']:\n",
    "    dataset_slice = df[df['dataset_id'] == dataset_id]\n",
    "    profiles_by_dataset[dataset_id] = why.log(dataset_slice)\n",
    "\n",
    "# Get schema counts\n",
    "schema_counts = df.groupby(['dataset_id', 'schema']).size().reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "591ccc84-1419-4deb-9e57-d4bfdc7f0b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_df['dataset_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6ff444f2-9c1f-402d-af15-065b1573cc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import whylogs as why\n",
    "from whylogs.core.constraints import ConstraintsBuilder\n",
    "from whylogs.core.constraints import Constraints, ConstraintsBuilder, MetricsSelector, MetricConstraint, DatasetConstraint\n",
    "from whylogs.viz import NotebookProfileVisualizer\n",
    "#from whylogs.core.metrics.metrics_selector import MetricsSelector\n",
    "#from IPython.display import display, HTML\n",
    "\n",
    "schema_issues = []\n",
    "#schema_types = ner_schema_list\n",
    "# Define all schema types to track\n",
    "schema_types = [\n",
    "    'B-METADATA_FIELD', \n",
    "    'B-METADATA_VALUE',\n",
    "    'B-CHUNK_ID',\n",
    "    'B-ACT_ID',\n",
    "    'B-SECTION_NAME',\n",
    "    'B-REGULATION_ID',\n",
    "    'B-SECTION_ID',\n",
    "    'B-SEQUENCE_ID'\n",
    "]\n",
    "\n",
    "for profile in profiles_by_dataset:\n",
    "    dataset_id = profile\n",
    "    #print(profile)\n",
    "    #print(profiles_by_dataset[profile])\n",
    "    whylog_data_profile = profiles_by_dataset[profile]\n",
    "    profile_view = whylog_data_profile.view()\n",
    "    builder = ConstraintsBuilder(dataset_profile_view=profile_view)\n",
    "\n",
    "    ## Extract URL\n",
    "    task_url_summary = profile_view.get_column('task_url').to_summary_dict()\n",
    "    frequent_strings = task_url_summary.get('frequent_items/frequent_strings', [])\n",
    "    if frequent_strings and len(frequent_strings) > 0:\n",
    "        url_item = frequent_strings[0]\n",
    "        url_value = url_item.value\n",
    "\n",
    "    ## Extract Job Name\n",
    "    job_name = profile_view.get_column('job_name').to_summary_dict()\n",
    "    frequent_strings = job_name.get('frequent_items/frequent_strings', [])\n",
    "    if frequent_strings and len(frequent_strings) > 0:\n",
    "        job_item = frequent_strings[0]\n",
    "        task_name = job_item.value\n",
    "\n",
    "    ## Extract dataset id\n",
    "    #dataset_id = profile_view.get_column('dataset_id').to_summary_dict()\n",
    "    #frequent_strings = dataset_id.get('frequent_items/frequent_strings', [])\n",
    "    #if frequent_strings and len(frequent_strings) > 0:\n",
    "    #    dataset_item = frequent_strings[0]\n",
    "    #    file_id = dataset_item.value\n",
    "    \n",
    "    # ✅ Add constraint for metadata fields\n",
    "    builder.add_constraint(\n",
    "        MetricConstraint(\n",
    "            name=\"B-METADATA_FIELD Count\",\n",
    "            condition=check_b_metadata_field_count,\n",
    "            metric_selector=MetricsSelector(column_name=\"schema\", metric_name=\"frequent_items\"),\n",
    "        )\n",
    "    )\n",
    "    meta_field_constraint = builder.build().validate(profile_view)\n",
    "\n",
    "    builder.add_constraint(\n",
    "        MetricConstraint(\n",
    "            name=\"B-METADATA_VALUE Count\",\n",
    "            condition=check_b_metadata_value_count,\n",
    "            metric_selector=MetricsSelector(column_name=\"schema\", metric_name=\"frequent_items\"),\n",
    "        )\n",
    "    )\n",
    "    meta_value_constraint = builder.build().validate(profile_view)\n",
    "\n",
    "    builder.add_constraint(\n",
    "        MetricConstraint(\n",
    "            name=\"METADATA OVERALL Count\",\n",
    "            condition=check_metadata_annotation_overall,\n",
    "            metric_selector=MetricsSelector(column_name=\"schema\", metric_name=\"frequent_items\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    meta_overall_constraint = builder.build().validate(profile_view)\n",
    "\n",
    "    \n",
    "    freq_values_values = profile_view.get_column(\"word\").get_metric(\"frequent_items\").to_summary_dict()\n",
    "    word_counts = {item.value: item.est for item in freq_values_values['frequent_strings']}\n",
    "\n",
    "    section_variants = {\"section\", \"Section\", \"sections\", \"Sections\"}\n",
    "    section_found = {word: count for word, count in word_counts.items() if word in section_variants}\n",
    "\n",
    "    section_constraint = None\n",
    "    if section_found['Section'] > 2 or len(section_found) > 1:\n",
    "        #print(\"Section-like words found:\", section_found)\n",
    "            #Check if sections are annotated right\n",
    "        condition = check_section_annotation(word_counts, unique_df, dataset_id)\n",
    "        #print(condition)\n",
    "        builder.add_constraint(MetricConstraint(\n",
    "            name=\"section_annotation_check\",\n",
    "            condition=check_section,\n",
    "            metric_selector=MetricsSelector(column_name=\"dataset_id\", metric_name=\"frequent_items\"),\n",
    "        ))\n",
    "        section_constraint = builder.build().validate(profile_view)\n",
    "    #else:\n",
    "    #    print(\"No 'section' variants found.\")\n",
    "\n",
    "    constraints = builder.build()\n",
    "\n",
    "    # Create a dictionary for the current file's results\n",
    "    # Store summary row, profile_view, and visualization function\n",
    "    file_result = {\n",
    "        \"Task #\": task_name,\n",
    "        \"File #\": dataset_id, #file_id,\n",
    "        \"URL\": f'{url_value}',\n",
    "        \"Metadata Pass\": f\"✔ ({meta_overall_constraint})\" if meta_overall_constraint else f\"❌ ({meta_overall_constraint})\",\n",
    "        #\"profile_view\": profile_view,\n",
    "        #\"constraints\": constraints,\n",
    "    }\n",
    "    if section_constraint is not None:\n",
    "        file_result['SECTION-CONSTRAINT'] = f\"✔ ({section_constraint})\" if section_constraint else f\"❌ ({section_constraint})\"\n",
    "    else:\n",
    "        file_result['SECTION-CONSTRAINT'] = 'N/A'\n",
    "\n",
    "    schema_freq_values = profile_view.get_column(\"schema\").get_metric(\"frequent_items\").to_summary_dict()\n",
    "    schema_counts = {item.value: item.est for item in schema_freq_values['frequent_strings']}\n",
    "    \n",
    "    # Count occurrences of each schema type\n",
    "    for schema_type in schema_types:\n",
    "        if schema_type in schema_counts: \n",
    "            count = schema_counts[schema_type]\n",
    "        else:\n",
    "            count = 0\n",
    "        # For now, set a minimum threshold of 1 for all types except the original two\n",
    "        min_threshold = 5 if schema_type in ['B-METADATA_FIELD', 'B-METADATA_VALUE'] else 1\n",
    "        status = f\"✔ ({count})\" if count >= min_threshold else f\"❌ ({count})\"\n",
    "        file_result[f\"Schema {schema_type}\"] = status\n",
    "\n",
    "    file_result[\"render_visualization\"] =  lambda pv=profile_view, cs=constraints: display_visualization(pv, cs)\n",
    "\n",
    "    # ✅ Store results\n",
    "    schema_issues.append(file_result)\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fb37efb2-8a12-46d8-98fb-60b31fc320bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(schema_issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "226f56f4-9a0a-46c9-bff8-1befbea7794b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task #</th>\n",
       "      <th>File #</th>\n",
       "      <th>URL</th>\n",
       "      <th>Metadata Pass</th>\n",
       "      <th>SECTION-CONSTRAINT</th>\n",
       "      <th>Schema B-METADATA_FIELD</th>\n",
       "      <th>Schema B-METADATA_VALUE</th>\n",
       "      <th>Schema B-CHUNK_ID</th>\n",
       "      <th>Schema B-ACT_ID</th>\n",
       "      <th>Schema B-SECTION_NAME</th>\n",
       "      <th>Schema B-REGULATION_ID</th>\n",
       "      <th>Schema B-SECTION_ID</th>\n",
       "      <th>Schema B-SEQUENCE_ID</th>\n",
       "      <th>render_visualization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NER_train_batch_249</td>\n",
       "      <td>20723</td>\n",
       "      <td><a href=\"http://localhost:8085/task/36909?file=20723&\" target=\"_blank\">View</a></td>\n",
       "      <td>❌ (False)</td>\n",
       "      <td>N/A</td>\n",
       "      <td>✔ (7)</td>\n",
       "      <td>❌ (0)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td><function <lambda> at 0x7f555893a980></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NER_train_batch_249</td>\n",
       "      <td>20724</td>\n",
       "      <td><a href=\"http://localhost:8085/task/36910?file=20724&\" target=\"_blank\">View</a></td>\n",
       "      <td>✔ (True)</td>\n",
       "      <td>N/A</td>\n",
       "      <td>✔ (7)</td>\n",
       "      <td>✔ (6)</td>\n",
       "      <td>❌ (0)</td>\n",
       "      <td>❌ (0)</td>\n",
       "      <td>❌ (0)</td>\n",
       "      <td>❌ (0)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>❌ (0)</td>\n",
       "      <td><function <lambda> at 0x7f555899bf60></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NER_train_batch_249</td>\n",
       "      <td>20725</td>\n",
       "      <td><a href=\"http://localhost:8085/task/36911?file=20725&\" target=\"_blank\">View</a></td>\n",
       "      <td>❌ (False)</td>\n",
       "      <td>N/A</td>\n",
       "      <td>✔ (7)</td>\n",
       "      <td>❌ (0)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td><function <lambda> at 0x7f555899afc0></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NER_train_batch_249</td>\n",
       "      <td>20726</td>\n",
       "      <td><a href=\"http://localhost:8085/task/36912?file=20726&\" target=\"_blank\">View</a></td>\n",
       "      <td>❌ (False)</td>\n",
       "      <td>N/A</td>\n",
       "      <td>✔ (7)</td>\n",
       "      <td>❌ (0)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td><function <lambda> at 0x7f555894d9e0></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NER_train_batch_249</td>\n",
       "      <td>20727</td>\n",
       "      <td><a href=\"http://localhost:8085/task/36913?file=20727&\" target=\"_blank\">View</a></td>\n",
       "      <td>❌ (False)</td>\n",
       "      <td>N/A</td>\n",
       "      <td>✔ (7)</td>\n",
       "      <td>❌ (0)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td><function <lambda> at 0x7f555882d1c0></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NER_train_batch_249</td>\n",
       "      <td>20728</td>\n",
       "      <td><a href=\"http://localhost:8085/task/36914?file=20728&\" target=\"_blank\">View</a></td>\n",
       "      <td>❌ (False)</td>\n",
       "      <td>❌ (False)</td>\n",
       "      <td>✔ (7)</td>\n",
       "      <td>❌ (0)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td><function <lambda> at 0x7f55589d3100></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NER_train_batch_249</td>\n",
       "      <td>20729</td>\n",
       "      <td><a href=\"http://localhost:8085/task/36915?file=20729&\" target=\"_blank\">View</a></td>\n",
       "      <td>❌ (False)</td>\n",
       "      <td>N/A</td>\n",
       "      <td>✔ (7)</td>\n",
       "      <td>❌ (2)</td>\n",
       "      <td>❌ (0)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>❌ (0)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td><function <lambda> at 0x7f555887c4a0></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NER_train_batch_249</td>\n",
       "      <td>20730</td>\n",
       "      <td><a href=\"http://localhost:8085/task/36916?file=20730&\" target=\"_blank\">View</a></td>\n",
       "      <td>✔ (True)</td>\n",
       "      <td>N/A</td>\n",
       "      <td>✔ (7)</td>\n",
       "      <td>✔ (6)</td>\n",
       "      <td>❌ (0)</td>\n",
       "      <td>❌ (0)</td>\n",
       "      <td>❌ (0)</td>\n",
       "      <td>❌ (0)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>❌ (0)</td>\n",
       "      <td><function <lambda> at 0x7f555885fb00></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NER_train_batch_249</td>\n",
       "      <td>20731</td>\n",
       "      <td><a href=\"http://localhost:8085/task/36917?file=20731&\" target=\"_blank\">View</a></td>\n",
       "      <td>❌ (False)</td>\n",
       "      <td>❌ (False)</td>\n",
       "      <td>✔ (7)</td>\n",
       "      <td>❌ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>❌ (0)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td><function <lambda> at 0x7f55588c2ca0></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NER_train_batch_249</td>\n",
       "      <td>20732</td>\n",
       "      <td><a href=\"http://localhost:8085/task/36918?file=20732&\" target=\"_blank\">View</a></td>\n",
       "      <td>❌ (False)</td>\n",
       "      <td>N/A</td>\n",
       "      <td>✔ (7)</td>\n",
       "      <td>❌ (2)</td>\n",
       "      <td>❌ (0)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>❌ (0)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td>✔ (1)</td>\n",
       "      <td><function <lambda> at 0x7f55589d1b20></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ✅ Convert to DataFrame for Display\n",
    "schema_issues_df = pd.DataFrame(schema_issues)\n",
    "\n",
    "# ✅ Display the validation summary table\n",
    "display(HTML(schema_issues_df[0:10].to_html(escape=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "da987459-3ff1-45c4-b042-0b4bde69d515",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization.difference_distribution_chart(feature_name=[\"schema\"]) ## Because there is no difference between the ref and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "64a67c56-fb82-4a1a-ab02-389e8bbb0f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd6fe8ef14bb4b1bb330c0dca0432a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Select File:', layout=Layout(width='60%'), options=(('NER_train_batch_249 - 20723', 0), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb3ac52484641b4afd52185a1d88aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=[(f\"{entry['Task #']} - {entry['File #']}\", i) for i, entry in enumerate(schema_issues)],\n",
    "    description='Select File:',\n",
    "    layout={'width': '60%'}\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_change(change):\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        selected_index = change[\"new\"]\n",
    "        if selected_index is not None:\n",
    "            schema_issues[selected_index][\"render_visualization\"]()\n",
    "\n",
    "dropdown.observe(on_change, names=\"value\")\n",
    "\n",
    "display(dropdown, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10eae81-f8bf-4f65-80cb-33051766de93",
   "metadata": {},
   "source": [
    "## Export Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0d00a794-4f20-4c36-a625-411a605efa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "visualization = NotebookProfileVisualizer()\n",
    "visualization.set_profiles(target_profile_view=profile_view, reference_profile_view=profile_view)\n",
    "visualization.write(\n",
    "    visualization.difference_distribution_chart(feature_name=\"schema\"),\n",
    "    html_file_name=os.getcwd() + \"/example\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d786d437-ef51-4304-9d55-1a6e5ac459c6",
   "metadata": {},
   "source": [
    "## Collect failed valdiation for pre-annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0eebf4-6227-45f0-a629-7719e800e641",
   "metadata": {},
   "source": [
    "Send passed validation for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c18afd1-bf72-4faf-af53-4d87e3f2eee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
