{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7e24a3-19f9-4e28-8c85-5496f25539bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf246ab-bbac-4984-a7e2-c23a5936962b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install diffgram\n",
    "!pip install llama_index\n",
    "!pip install matplotlib\n",
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49f92142-5c34-4a96-8928-cd85fb8dc209",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8527/2932230743.py:5: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython.display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from diffgram import Project\n",
    "from llama_index.core import SimpleDirectoryReader, StorageContext\n",
    "import pandas as pd\n",
    "from IPython.core.display import display, HTML\n",
    "import matplotlib as mpl\n",
    "from tabulate import tabulate\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5083615-6b22-4da5-a8ab-ecd31b5b9ebf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af40867-2457-4684-9c18-0dcc7618a0ee",
   "metadata": {},
   "source": [
    "## Connect to diffgram project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "258828ae-43c5-41d3-a43d-0690808d0b71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project_local = Project(host = \"https://7ea5-2604-3d08-4f7f-e8c0-64d7-690b-93d-9368.ngrok-free.app\",\n",
    "        project_string_id = \"valiantbiter\",\n",
    "        client_id = \"LIVE__ywmyayoir4c6zx1kz5j4\",\n",
    "        client_secret = \"wwwh2eiwc7f4q35qywxgghuyfuh2oemwshec0tiu4gctapsxqe3orcyfe5u3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896f857a-4abe-40ec-b492-afd73c6fb0fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utility functions to create/check a directory or file in diffgram with dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39f1ae19-26aa-406b-817b-c85338096f39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_if_directory_exist(dir_name):\n",
    "    project = project_local.directory.get_directory_list(limit=50000)\n",
    "    for project_dir in project:\n",
    "        if (project_dir.__dict__['nickname'] == dir_name):\n",
    "            return project_dir\n",
    "    return None\n",
    "    #if (project_dir.__dict__['nickname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "019b7410-1281-478d-a669-f5362f422471",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## You may need to run this twice to see if the directory is created\n",
    "def create_diffgram_directory(dataset_name):\n",
    "    #directory = project_local.directory.get(name = dataset_name)\n",
    "    directory = check_if_directory_exist(dataset_name)\n",
    "    if (directory is None):\n",
    "        project_local.directory.new(name=dataset_name)\n",
    "        directory = check_if_directory_exist(dataset_name)\n",
    "        print(directory.__dict__)    \n",
    "    return directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24ed78f0-1230-4baa-addb-5e4551d9a21d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## check if file exist in the dir\n",
    "def check_if_file_exist_in_dir(filename):\n",
    "    file = project_local.file.file_list_exists(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "867af4b4-5709-4b45-bdf5-f68d978c6e17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataset_job(data_suffix, job_suffix, index, member_list_ids):\n",
    "    dataset_batch_name = data_suffix + str(index)\n",
    "    directory = create_diffgram_directory(dataset_batch_name)\n",
    "\n",
    "    if directory is None:\n",
    "        print(f\"{dataset_batch_name} Directory does not exist\")\n",
    "        return\n",
    "\n",
    "    job_name = job_suffix + str(index)\n",
    "\n",
    "    job = project_local.job.new(\n",
    "        name = job_name,\n",
    "        instance_type = \"box\",\n",
    "        share = \"Project\",\n",
    "        sync_directories = [directory],\n",
    "        label_schema_id = schema_id,\n",
    "        tag_list = [\"Laws\", \"Acts\", \"Regulations\"],\n",
    "        members_list_ids = member_list_ids,\n",
    "        auto_launch = True\n",
    "    )\n",
    "    print(f\"The {job_name} task is created\")\n",
    "    return directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05a9911d-4147-4f93-a728-3e412dae077d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upload_files_to_dataset(index,batch_size, offset, directory):\n",
    "    for document in range((index+offset) * batch_size, ((index + offset) * batch_size) + batch_size):\n",
    "        filename = diffgram_documents[document].metadata['filename']\n",
    "        # check if the file exist in the diffgram directory\n",
    "        try:\n",
    "            file = project_local.file.from_local(filename,directory_id=directory.__dict__['id'])\n",
    "        except:\n",
    "            print(f\"File with {filename} exist in this directory. Continuing ....\")\n",
    "            continue;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40c738b-03d1-47a1-b4ad-55d7cad54032",
   "metadata": {},
   "source": [
    "## Import all the files \n",
    "### make sure you have the diffgram_processing_v2 folder which has all the data arranged for NER task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f83a32b6-2898-4f9e-9000-220dbecea460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aef452fa-6ef0-4c60-9363-512f7372b120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_metadata = lambda x: {\"filename\": x}\n",
    "diffgram_documents = SimpleDirectoryReader(\"diffgram_processing_v2\",file_metadata=file_metadata).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66845844-502e-4ff2-9b8f-3afcb7005621",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81612\n"
     ]
    }
   ],
   "source": [
    "print(len(diffgram_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0fbc6de-aa67-4206-b7c5-b16ef724c00b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk ID: Criminal Records Review Act-chunk-Reconsideration-0003\n",
      "Act ID: Criminal Records Review Act\n",
      "Regulation ID: None\n",
      "Section Name: Reconsideration\n",
      "Section ID: 5\n",
      "Sequence ID: 3\n",
      "Text:\n",
      "7 the registrar must promptly provide notification of a decision under subsection ( 6 ) ( a ) or ( b ) to the individual who is the subject of the decision and to the persons or entities that were provided with a notification under section 4 ( 4. 1 ) or ( 4. 2 ) or 4. 1 ( 4 ), as applicable.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(diffgram_documents[0].get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a6f78b-e639-458c-9825-3a6d5100597e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fetch and add the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "119ca54b-6d63-4678-a675-b974e0864aa7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'schema': [{'name': 'O', 'description': 'Outside of named entity (default)', 'example': 'The'}, {'name': 'B-INTERNAL_REF_SECTION', 'description': 'Beginning of internal section reference', 'example': \"Section in 'Section 101'\"}, {'name': 'I-INTERNAL_REF_SECTION', 'description': 'Inside of internal section reference', 'example': \"101 in 'Section 101'\"}, {'name': 'B-EXTERNAL_REF_ACT_SECTION', 'description': 'Beginning of external act section reference', 'example': \"Section in 'Section 5 of the Clean Air Act'\"}, {'name': 'I-EXTERNAL_REF_ACT_SECTION', 'description': 'Inside of external act section reference', 'example': \"5 in 'Section 5 of the Clean Air Act'\"}, {'name': 'B-EXTERNAL_REF_REGULATION_SECTION', 'description': 'Beginning of external regulation section reference', 'example': \"Section in 'Section 3.2 of the Federal Acquisition Regulation'\"}, {'name': 'I-EXTERNAL_REF_REGULATION_SECTION', 'description': 'Inside of external regulation section reference', 'example': \"3.2 in 'Section 3.2 of the Federal Acquisition Regulation'\"}, {'name': 'B-REF_TYPE', 'description': 'Beginning of reference type', 'example': \"subsection in 'subsection (a)'\"}, {'name': 'I-REF_TYPE', 'description': 'Inside of reference type', 'example': \"paragraph in 'subparagraph (2)'\"}, {'name': 'B-EXCLUDED_SECTION', 'description': 'Beginning of excluded section reference', 'example': \"except in 'except Section 101'\"}, {'name': 'I-EXCLUDED_SECTION', 'description': 'Inside of excluded section reference', 'example': \"101 in 'except Section 101'\"}, {'name': 'B-INCLUDED_SECTION', 'description': 'Beginning of included section reference', 'example': \"including in 'including Section 202'\"}, {'name': 'I-INCLUDED_SECTION', 'description': 'Inside of included section reference', 'example': \"202 in 'including Section 202'\"}, {'name': 'B-DECISION', 'description': 'Beginning of a decision reference', 'example': \"decision in 'decision under subsection (6)(a)'\"}, {'name': 'I-DECISION', 'description': 'Inside of a decision reference', 'example': \"under subsection (6)(a) in 'decision under subsection (6)(a)'\"}, {'name': 'B-ACT_NAME', 'description': 'Beginning of an act name', 'example': \"Clean in 'Clean Air Act'\"}, {'name': 'I-ACT_NAME', 'description': 'Inside of an act name', 'example': \"Air Act in 'Clean Air Act'\"}, {'name': 'B-REGULATION_NAME', 'description': 'Beginning of a regulation name', 'example': \"Federal in 'Federal Acquisition Regulation'\"}, {'name': 'I-REGULATION_NAME', 'description': 'Inside of a regulation name', 'example': \"Acquisition Regulation in 'Federal Acquisition Regulation'\"}, {'name': 'B-SUBSECTION', 'description': 'Beginning of a subsection reference', 'example': \"( in '(a)'\"}, {'name': 'I-SUBSECTION', 'description': 'Inside of a subsection reference', 'example': \"a) in '(a)'\"}, {'name': 'B-PARAGRAPH', 'description': 'Beginning of a paragraph reference', 'example': \"( in '(1)'\"}, {'name': 'I-PARAGRAPH', 'description': 'Inside of a paragraph reference', 'example': \"1) in '(1)'\"}]}\n"
     ]
    }
   ],
   "source": [
    "f = open('legal_ner_schema.json')\n",
    "NER_schema = json.load(f)\n",
    "print(NER_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04bea780-dd36-4a97-9026-681c49fb3c9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]\n",
      "['O', 'B-INTERNAL_REF_SECTION', 'I-INTERNAL_REF_SECTION', 'B-EXTERNAL_REF_ACT_SECTION', 'I-EXTERNAL_REF_ACT_SECTION', 'B-EXTERNAL_REF_REGULATION_SECTION', 'I-EXTERNAL_REF_REGULATION_SECTION', 'B-REF_TYPE', 'I-REF_TYPE', 'B-EXCLUDED_SECTION', 'I-EXCLUDED_SECTION', 'B-INCLUDED_SECTION', 'I-INCLUDED_SECTION', 'B-DECISION', 'I-DECISION', 'B-ACT_NAME', 'I-ACT_NAME', 'B-REGULATION_NAME', 'I-REGULATION_NAME', 'B-SUBSECTION', 'I-SUBSECTION', 'B-PARAGRAPH', 'I-PARAGRAPH']\n"
     ]
    }
   ],
   "source": [
    "## Id the Labels\n",
    "# Create label to ID mapping\n",
    "label2id = []\n",
    "id2label = []\n",
    "for id, label in enumerate(NER_schema['schema']):\n",
    "    label2id.append(id)\n",
    "    id2label.append(NER_schema['schema'][id]['name'])\n",
    "print(label2id)\n",
    "print(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3227ff0f-dd22-4453-bf21-0804827a3766",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NER_schema_name = 'NER_TRAINING_SCHEMA'\n",
    "schema_id = None\n",
    "schemas = project_local.schema.list()\n",
    "print(json.dumps(schemas, indent=2))\n",
    "for schema in schemas:\n",
    "    if schema['name'] == NER_schema_name:\n",
    "        schema_id = schema['id']\n",
    "if schema_id == None:\n",
    "    json_response = project_local.new_schema(name=NER_schema_name)\n",
    "    schema_id = json_response[\"id\"]\n",
    "    for NER in NER_schema['schema']:\n",
    "        print(NER['name'])\n",
    "        project_local.label_new(NER, schema_id=schema_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e19157-775c-4d00-ba90-9f116eaab96f",
   "metadata": {},
   "source": [
    "## Fetch the files and add it to diffgram\n",
    "if you want to insert one file at a time\n",
    "    file = project_local.file.from_local(\"diffgram_processing_v2/800.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bedfc97-b252-48e6-8a78-184ce39fd290",
   "metadata": {},
   "source": [
    "### Separating the data into train and test\n",
    "1. Batch each train or test data to a size of 64 or any multiple of 2^ (easier for memory alignment/ faster processing)\n",
    "2. To make the netowkr good at NER the more data it is trained on the better. IF we want to train it with 5000 examples, we need to create 5000/64 batches of test data\n",
    "3. We can then have 5% * 5000 as the test data\n",
    "4. we can name the diffgram dataset as NER_train_batch_x where x is the batch number\n",
    "5. similar we can name the diffgram dataset for test as NER_test_batch_x \n",
    "6. After creating the dataset we create the task for both test and train e.g. NER_train_JOB_. \n",
    "7. The task links the dataset and the schema.\n",
    "8. After which we add files to the dataset based on the batch size\n",
    "9. Repeat steps 4 - 8 till all the data is populated and arranged for annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "81236e39-9ae6-4395-a41d-e65d55a5982b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_datasets(diffgram_documents, num_training_data, batch_size, train_suffix, test_suffix, job_train_suffix, job_test_suffix):\n",
    "    #check if the lenght of all the data is equal to or more than num_training_data\n",
    "    if (len(diffgram_documents) < num_training_data):\n",
    "        print(f\"Not sufficient data for training {len(diffgram_documents)}\")\n",
    "        return\n",
    "    \n",
    "    train_batch_size = math.floor(num_training_data/batch_size)\n",
    "    test_batch_size = math.floor((num_training_data * (5 /100))/ batch_size)\n",
    "    #train_dataset_name = \"NER_train_batch_\"\n",
    "    \n",
    "    print(f\"The batch size of the training data is : {train_batch_size}\")\n",
    "    print(f\"The batch size of the test data is: {test_batch_size}\")\n",
    "    \n",
    "    member_list = project_local.get_member_list()\n",
    "    member_list_ids = [x['member_id'] for x in member_list]\n",
    "    \n",
    "    #schemas = project_local.schema.list()\n",
    "    \n",
    "    for index in range(0, train_batch_size):\n",
    "        directory = create_dataset_job(train_suffix, job_train_suffix, index, member_list_ids)\n",
    "        \n",
    "        print(f\"Creating / Uploading data to directory {directory.__dict__['nickname']}\")\n",
    "        upload_files_to_dataset(index,batch_size, 0, directory)\n",
    "        \n",
    "    for index in range(0, test_batch_size):\n",
    "        directory = create_dataset_job(test_suffix, job_test_suffix, index, member_list_ids)\n",
    "        \n",
    "        print(f\"Creating / Uploading data to directory {directory.__dict__['nickname']}\")\n",
    "        upload_files_to_dataset(index,batch_size, train_batch_size+1, directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a9098da-b0ff-4e06-be2a-92a34dc0a851",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_TRAINING_DATA = 5000\n",
    "BATCH_SIZE = 64\n",
    "train_dataset_suffix = \"NER_train_batch_\"\n",
    "test_dataset_suffix = \"NER_test_batch_\"\n",
    "JOB_NAME = \"Law_NER_task1\"\n",
    "JOB_TRAIN_SUFFIX = \"NER_train_JOB_\"\n",
    "JOB_TEST_SUFFIX = \"NER_test_JOB_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaf9af9-03fb-43ad-a669-34e0c053fee6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_datasets(diffgram_documents, NUM_TRAINING_DATA, BATCH_SIZE, train_dataset_suffix, test_dataset_suffix,  JOB_TRAIN_SUFFIX, JOB_TEST_SUFFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bff895c-c09a-4996-a45b-58c109d83e8e",
   "metadata": {},
   "source": [
    "## Export annotated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7f46199-cc31-4b7c-9893-c205c25df521",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_job_names(diffgram_documents, num_training_data, batch_size, data_suffix):\n",
    "    if (len(diffgram_documents) < num_training_data):\n",
    "        print(f\"Not sufficient data for training {len(diffgram_documents)}\")\n",
    "        return\n",
    "    \n",
    "    num_batches = math.floor(num_training_data/batch_size)\n",
    "    jobs = []\n",
    "    for index in range(0, num_batches):\n",
    "        jobs_name = JOB_TRAIN_SUFFIX + str(index)\n",
    "        jobs.append(jobs_name)\n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a2f8aee-a0ce-43eb-b583-dd2689fb2cef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jobs = get_job_names(diffgram_documents, NUM_TRAINING_DATA, BATCH_SIZE, train_dataset_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45c7f9c0-3695-4a96-a513-fe7bfbf05921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_job = project_local.job.list(limit=10000, page_number=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "394c12f3-9970-43ce-932a-5a4e92511cde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "print(len(get_job))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb0edbd7-29bd-416d-b4ef-54186c10d4a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Get the job based on the id and name\n",
    "def get_completed_job(jobs):\n",
    "    get_job = project_local.job.list()\n",
    "    #print(f\"Number of jobs {len(get_job)}\")\n",
    "    completed_jobs = []\n",
    "    for jobs_completed in get_job:\n",
    "        #print(f\"{jobs_completed['status']} {jobs_completed['name']}\")\n",
    "        for job in jobs:\n",
    "            #print(f\"Job name is {job}\")\n",
    "            if (jobs_completed['status'] == 'complete') and (jobs_completed['name'] == job):\n",
    "                print(f\"{jobs_completed['name']} is completed\")   \n",
    "                completed_jobs.append(jobs_completed)\n",
    "                break\n",
    "            else:\n",
    "                #print(f\"{job} is not annotated completely. Please comeplete annotation to train the A.I\") \n",
    "                continue\n",
    "    return completed_jobs            \n",
    "        #print(f\"{jobs_completed['name']} is not completed\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "750adc45-9f3f-4da3-a0c2-e9300b5b8c93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER_train_JOB_0 is completed\n",
      "The status of the job NER_train_JOB_0 (id = 23)  is complete.\n"
     ]
    }
   ],
   "source": [
    "completed_jobs = get_completed_job(jobs)\n",
    "#print(completed_jobs)\n",
    "if completed_jobs is not None:\n",
    "    for completed_job in completed_jobs:\n",
    "        print(f\"The status of the job {completed_job['name']} (id = {completed_job['id']})  is {completed_job['status']}.\")\n",
    "else: \n",
    "    print(\"The annotation task is not completed. Complete all annotation before proceeding.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "709e4847-5ff0-4a67-95eb-1599b38a80ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = project_local.job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db713005-6182-466f-9da6-5ca565d5b4cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#def download_annotated_data(completed_jobs):\n",
    "#    for completed_job in completed_jobs:\n",
    "#        results.refresh_from_dict(completed_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e7d1ae0-689e-422f-83e8-118cd6673343",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results.refresh_from_dict(completed_jobs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6cc384e-9d7a-4a80-8cd9-dcc62cc8bcd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "completed_annotations = results.generate_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "441f521f-9628-4317-8976-f2ade0fe6f25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DIFFGRAM_EXPORT_ADDITIONAL_PARAMETERS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6541d21c-fc58-4e89-8287-e18ea7a118fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of annotated data 64\n"
     ]
    }
   ],
   "source": [
    "print(f\" Number of annotated data {len(completed_annotations) - DIFFGRAM_EXPORT_ADDITIONAL_PARAMETERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2c2144-88d7-41ec-ab24-74735a143d6e",
   "metadata": {},
   "source": [
    "## Post Processing\n",
    "### processing the annotated data to train the A.I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6dc84d22-f2bf-40b8-a49e-c6af3f34b7fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "sentences = []\n",
    "labels = []\n",
    "data_index = 0\n",
    "for completed_annotation in completed_annotations:\n",
    "    #print(f\"{completed_annotation} ----\")\n",
    "    if (completed_annotation != 'attribute_groups_reference')  \\\n",
    "        and (completed_annotation != 'export_info') \\\n",
    "        and (completed_annotation != 'label_map') \\\n",
    "        and (completed_annotation != 'readme') \\\n",
    "        and (completed_annotation != 'label_colour_map'):\n",
    "        sentence_local = []\n",
    "        labels_local = []\n",
    "        # First get the point where the annotation is started\n",
    "        for start in completed_annotations[completed_annotation]['instance_list']:\n",
    "            if 'start_token' in start:\n",
    "                start_token =  start['start_token']\n",
    "                break\n",
    "            \n",
    "        #start_token = completed_annotations[completed_annotation]['instance_list'][0]['start_token']\n",
    "        for annotated_index in range(start_token, len(completed_annotations[completed_annotation]['text']['tokens']['words'])):\n",
    "            # check if this text is annotated\n",
    "            for data in completed_annotations[completed_annotation]['instance_list']:\n",
    "                if 'start_token' in data:\n",
    "                    if annotated_index == data['start_token']:\n",
    "                        sentence_local.append(completed_annotations[completed_annotation]['text']['tokens']['words'][annotated_index]['value'])\n",
    "                        labels_local.append(completed_annotations['label_map'][str(data['label_file_id'])])\n",
    "                        #print(f\"{completed_annotations[completed_annotation]['text']['tokens']['words'][annotated_index]['value']} - {completed_annotations['label_map'][str(data['label_file_id'])]}\")\n",
    "                        break;\n",
    "        sentences.append(sentence_local)       \n",
    "        labels.append(labels_local)\n",
    "        data_index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "077f1475-2bd0-496f-bf82-1b4e68e722f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c54e9a-a0aa-4abc-919b-0f5391fb33b5",
   "metadata": {},
   "source": [
    "## View of the annotated data before feeding into the neural network for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "092e6c63-4e64-47e9-aaf2-c4e123ed67fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒════╤══════════════╤════════════════════════╕\n",
      "│    │ Token        │ Label                  │\n",
      "╞════╪══════════════╪════════════════════════╡\n",
      "│  0 │ 7            │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│  1 │ the          │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│  2 │ registrar    │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│  3 │ must         │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│  4 │ promptly     │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│  5 │ provide      │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│  6 │ notification │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│  7 │ of           │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│  8 │ a            │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│  9 │ subsection   │ B-REF_TYPE             │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 10 │ (            │ B-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 11 │ 6            │ B-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 12 │ )            │ B-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 13 │ (            │ B-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 14 │ a            │ B-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 15 │ )            │ B-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 16 │ or           │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 17 │ (            │ B-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 18 │ b            │ B-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 19 │ )            │ B-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 20 │ to           │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 21 │ the          │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 22 │ individual   │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 23 │ who          │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 24 │ is           │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 25 │ the          │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 26 │ subject      │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 27 │ of           │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 28 │ the          │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 29 │ decision     │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 30 │ and          │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 31 │ to           │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 32 │ the          │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 33 │ persons      │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 34 │ or           │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 35 │ entities     │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 36 │ that         │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 37 │ were         │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 38 │ provided     │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 39 │ with         │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 40 │ a            │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 41 │ notification │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 42 │ under        │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 43 │ section      │ B-REF_TYPE             │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 44 │ 4            │ B-INTERNAL_REF_SECTION │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 45 │ (            │ B-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 46 │ 4            │ I-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 47 │ .            │ I-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 48 │ 1            │ I-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 49 │ )            │ I-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 50 │ or           │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 51 │ (            │ B-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 52 │ 4            │ I-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 53 │ .            │ I-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 54 │ 2            │ I-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 55 │ )            │ I-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 56 │ or           │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 57 │ 4            │ B-INTERNAL_REF_SECTION │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 58 │ .            │ I-INTERNAL_REF_SECTION │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 59 │ 1            │ I-INTERNAL_REF_SECTION │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 60 │ (            │ B-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 61 │ 4            │ I-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 62 │ )            │ I-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 63 │ ,            │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 64 │ as           │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 65 │ applicable   │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 66 │ .            │ O                      │\n",
      "╘════╧══════════════╧════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "# Creating a pandas DataFrame\n",
    "for iter in range (0, 1):\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    ner_df = pd.DataFrame({\n",
    "        \"Token\": sentences[iter],\n",
    "        \"Label\": labels[iter]\n",
    "    })  \n",
    "    # Display the DataFrame with some custom styles\n",
    "    ner_df.style.set_properties(**{'background-color': 'lightyellow', \n",
    "                               'color': 'black',\n",
    "                               'border-color': 'black'})\n",
    "    # Use tabulate to display a nice table in the terminal\n",
    "    print(tabulate(ner_df, headers='keys', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cad661e-5c57-4986-9ae6-df76ea8b5240",
   "metadata": {},
   "source": [
    "## Preparing the data for input to pytorch\n",
    "### We need to make all the data the same length by padding if they are not 255 tokens in length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "162077bf-133d-4aa1-b6be-c2cfa625bbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dee7e125-5c85-4c51-b9be-6877382cd1df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 255\n",
    "PAD_TOKEN = \"[PAD]\"\n",
    "PAD_LABEL = \"O\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c44d2a6b-10b9-48df-986c-5afe3abb9d0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pad_sequence(sequence, max_length, pad_value):\n",
    "    return sequence + [pad_value] * (max_length - len(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ed8b6fd5-fe1b-485a-adbe-990ddfcedd2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "data = []\n",
    "NER_label = []\n",
    "for sent_id, (sentence, sent_labels) in enumerate(zip(sentences, labels)):\n",
    "    tmp_data = []\n",
    "    tokens = sentence  # Simple tokenization\n",
    "    #print(tokens)\n",
    "    sentence_string = \"\"\n",
    "    for string_value in tokens:\n",
    "        sentence_string += string_value + ' '\n",
    "    #print(sentence_string)\n",
    "    padded_tokens = pad_sequence(tokens, MAX_LENGTH, PAD_TOKEN)\n",
    "    padded_labels = pad_sequence(sent_labels, MAX_LENGTH, PAD_LABEL)\n",
    "    encoded = tokenizer(sentence_string,\n",
    "                        padding='max_length',\n",
    "                        truncation=True,\n",
    "                        max_length=MAX_LENGTH,  # Adjust based on your needs\n",
    "                        return_tensors='pt')\n",
    "    #print(encoded['input_ids'][0][0])\n",
    "    data.append(encoded)\n",
    "    NER_label.append(padded_labels)\n",
    "    for token_id in range(MAX_LENGTH):\n",
    "        tmp_data.append({\n",
    "            'Sentence_ID': sent_id,\n",
    "            'Token_ID': token_id,\n",
    "            'Token': padded_tokens[token_id],\n",
    "            'NER_Label': padded_labels[token_id],\n",
    "            'Is_Pad': padded_tokens[token_id] == PAD_TOKEN\n",
    "        })\n",
    "    #data.append(tmp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e8dae967-9041-4869-b884-a43f60eb5dd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_label_to_id(ner_label,id2label):\n",
    "    label_id = []\n",
    "    for label in ner_label:\n",
    "        label_id.append(id2label.index(label))\n",
    "    return label_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a436139d-91fd-4289-905f-3228f46f5c9d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "595d8422-aaed-4c63-8e26-dc68d37a0de9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', \n",
    "                                                   num_labels=len(id2label),\n",
    "                                                   id2label=id2label,\n",
    "                                                   label2id=label2id)\n",
    "\n",
    "# Training loop (simplified)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6cef2b5a-d8bc-4729-8c44-a4d149888269",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d1d11c-ef69-42f2-b674-4012de25eaa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epoch in range(5):  # Number of epochs\n",
    "    for i, batch in enumerate(data):\n",
    "        labelled_id = convert_label_to_id(NER_label[i], id2label)\n",
    "        #print(labelled_id)\n",
    "        input_ids = torch.tensor(batch['input_ids']).to(device)\n",
    "        attention_mask = torch.tensor(batch['attention_mask']).to(device)\n",
    "        labels = torch.tensor(labelled_id, dtype=torch.long).to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24a0586-53e7-494c-a8c7-07549a88e2da",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ee5dccd9-9464-4859-abf0-8bd3815e18c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=23, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2731d4-5955-42a4-a6b7-af3ab9c8a1b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = data\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_data):\n",
    "        # Convert inputs to tensors\n",
    "        input_ids = torch.tensor(batch['input_ids']).to(device)\n",
    "        attention_mask = torch.tensor(batch['attention_mask']).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Get predictions\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        print(predictions)\n",
    "        \n",
    "        # Compare predictions with true labels if available\n",
    "        #true_labels = batch['labels']  # Assuming test data has true labels\n",
    "        # Calculate accuracy or other metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f73f3b-eec6-4059-b4df-1c748d2635da",
   "metadata": {},
   "source": [
    "## Lets compare the accuracy with the trained dataset. See if it is overfitting or underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2d565b00-4773-43ab-afa4-20c5970a733f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compare_output():\n",
    "    print(\"Comparing the models predictions vs the labelled predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d4935d0b-8c01-4871-ba10-045a7ec0b277",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing the models predictions vs the labelled predictions\n"
     ]
    }
   ],
   "source": [
    "compare_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f510add-5e08-4b16-8a87-3eaa3cb3f479",
   "metadata": {},
   "source": [
    "## Post-Processing NER Output\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. **Find `B-REF-TYPE`**\n",
    "   - Look for the token labeled as `B-REF_TYPE`, which indicates the beginning of a reference type like \"subsection\" or \"paragraph.\"\n",
    "\n",
    "2. **Check if it is Followed by `B-INTERNAL_REF_SECTION` or `B-EXTERNAL_REF_ACT_SECTION/REGULATION_SECTION`**\n",
    "   - After finding `B-REF_TYPE`, check the subsequent tokens to see if they are labeled as `B-INTERNAL_REF_SECTION`, `B-EXTERNAL_REF_ACT_SECTION`, or `B-EXTERNAL_REF_REGULATION_SECTION`.\n",
    "   - This step ensures that the reference type is correctly associated with either an internal or external reference.\n",
    "\n",
    "3. **Handle Internal References**\n",
    "   - If `B-INTERNAL_REF_SECTION` is found, capture all tokens labeled with `I-INTERNAL_REF_SECTION` until the first `O` (Outside of named entity) is encountered.\n",
    "   - Convert the captured tokens into a section number or identifier.\n",
    "   - Create a Cypher query to establish a link to the internal reference in your graph database.\n",
    "\n",
    "4. **Handle External References**\n",
    "   - If `B-EXTERNAL_REF_ACT_SECTION` or `B-EXTERNAL_REF_REGULATION_SECTION` is found, capture all tokens labeled with `I-EXTERNAL_REF_ACT_SECTION` or `I-EXTERNAL_REF_REGULATION_SECTION` until the first `O` is encountered.\n",
    "   - Continue to search for tokens labeled `B-ACT_NAME` or `B-REGULATION_NAME`, which indicate the act or regulation name.\n",
    "   - Capture these tokens and combine them with the section number to form a full reference.\n",
    "   - Create a Cypher query to establish a link to the external reference, specifying the act or regulation name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6962ae5a-ce3e-4d6c-a1b8-8a0befc4f7d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'internal', 'ref_type': 'B-REF_TYPE', 'content': '4'}]\n",
      "CREATE (n:InternalRef {type: 'B-REF_TYPE', section: '4'})\n",
      "╒════╤══════════════╤════════════════════════╕\n",
      "│    │ Token        │ Label                  │\n",
      "╞════╪══════════════╪════════════════════════╡\n",
      "│  0 │ 7            │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│  1 │ the          │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│  2 │ registrar    │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│  3 │ must         │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│  4 │ promptly     │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│  5 │ provide      │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│  6 │ notification │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│  7 │ of           │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│  8 │ a            │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│  9 │ subsection   │ B-REF_TYPE             │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 10 │ (            │ B-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 11 │ 6            │ B-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 12 │ )            │ B-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 13 │ (            │ B-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 14 │ a            │ B-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 15 │ )            │ B-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 16 │ or           │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 17 │ (            │ B-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 18 │ b            │ B-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 19 │ )            │ B-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 20 │ to           │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 21 │ the          │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 22 │ individual   │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 23 │ who          │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 24 │ is           │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 25 │ the          │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 26 │ subject      │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 27 │ of           │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 28 │ the          │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 29 │ decision     │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 30 │ and          │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 31 │ to           │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 32 │ the          │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 33 │ persons      │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 34 │ or           │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 35 │ entities     │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 36 │ that         │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 37 │ were         │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 38 │ provided     │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 39 │ with         │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 40 │ a            │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 41 │ notification │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 42 │ under        │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 43 │ section      │ B-REF_TYPE             │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 44 │ 4            │ B-INTERNAL_REF_SECTION │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 45 │ (            │ B-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 46 │ 4            │ I-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 47 │ .            │ I-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 48 │ 1            │ I-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 49 │ )            │ I-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 50 │ or           │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 51 │ (            │ B-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 52 │ 4            │ I-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 53 │ .            │ I-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 54 │ 2            │ I-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 55 │ )            │ I-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 56 │ or           │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 57 │ 4            │ B-INTERNAL_REF_SECTION │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 58 │ .            │ I-INTERNAL_REF_SECTION │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 59 │ 1            │ I-INTERNAL_REF_SECTION │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 60 │ (            │ B-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 61 │ 4            │ I-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 62 │ )            │ I-SUBSECTION           │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 63 │ ,            │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 64 │ as           │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 65 │ applicable   │ O                      │\n",
      "├────┼──────────────┼────────────────────────┤\n",
      "│ 66 │ .            │ O                      │\n",
      "╘════╧══════════════╧════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "# Creating a pandas DataFrame\n",
    "references = []\n",
    "for iter in range (0, 1):\n",
    "    length_of_sentence = len(sentences[iter])\n",
    "    for index, sentence in enumerate(sentences[iter]):\n",
    "        #print(f\"{sentence} {labels[iter][index]}\")\n",
    "        \n",
    "        #Step 1: Check B-REF-TYPE\n",
    "        if (labels[iter][index] == 'B-REF_TYPE'):\n",
    "            ref_type = labels[iter][index]\n",
    "            ref_content = \"\"\n",
    "            ref_target = \"\"\n",
    "            \n",
    "            #Step 2: Check for the next label\n",
    "            if ((index+1) > (length_of_sentence - 1)):\n",
    "                continue\n",
    "            j = index +1    \n",
    "            next_label = labels[iter][j]\n",
    "            \n",
    "            # Step 3: Handle Internal References\n",
    "            if next_label.startswith('B-INTERNAL_REF_SECTION'):\n",
    "                ref_content += sentences[iter][j] + \" \"\n",
    "                j+=1\n",
    "                while (j < length_of_sentence) and labels[iter][j].startswith('I-INTERNAL_REF_SECTION'):\n",
    "                    print(sentences[iter][j])\n",
    "                    ref_content += sentences[iter][j] + \" \"\n",
    "                    j += 1\n",
    "                ref_content = ref_content.strip()\n",
    "                references.append({\"type\": \"internal\", \"ref_type\": ref_type, \"content\": ref_content})\n",
    "                print(references)\n",
    "                # Generate Cypher query\n",
    "                cypher_query = f\"CREATE (n:InternalRef {{type: '{ref_type}', section: '{ref_content}'}})\"\n",
    "                print(cypher_query)\n",
    "                \n",
    "            # Step 4: Handle External References\n",
    "            elif next_label.startswith('B-EXTERNAL_REF'):\n",
    "                ref_content += sentences[iter][j] + \" \"\n",
    "                j+=1\n",
    "                while (j < length_of_sentence) and labels[iter][j].startswith('I-INTERNAL_REF_SECTION'):\n",
    "                    print(sentences[iter][j])\n",
    "                    ref_content += sentences[iter][j] + \" \"\n",
    "                    j += 1\n",
    "                #ref_content = ref_content.strip()\n",
    "                references.append({\"type\": \"internal\", \"ref_type\": ref_type, \"content\": ref_content})\n",
    "                \n",
    "                # Search for ACT_NAME or REGULATION_NAME\n",
    "                while j < len(ner_output):\n",
    "                    if ner_output[j][1] == 'B-ACT_NAME' or ner_output[j][1] == 'B-REGULATION_NAME':\n",
    "                        k = j\n",
    "                        while k < len(ner_output) and ner_output[k][1].startswith(('I-ACT_NAME', 'I-REGULATION_NAME')):\n",
    "                            ref_target += ner_output[k][0] + \" \"\n",
    "                            k += 1\n",
    "                        ref_target = ref_target.strip()\n",
    "                        break\n",
    "                    j += 1\n",
    "                references.append({\"type\": \"external\", \"ref_type\": ref_type, \"content\": ref_content, \"target\": ref_target})\n",
    "                print(references)\n",
    "\n",
    "                # Generate Cypher query\n",
    "                cypher_query = f\"CREATE (n:ExternalRef {{type: '{ref_type}', section: '{ref_content}', target: '{ref_target}'}})\"\n",
    "                print(cypher_query)\n",
    "\n",
    "    # Print the data in a pretty table\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    ner_df = pd.DataFrame({\n",
    "        \"Token\": sentences[iter],\n",
    "        \"Label\": labels[iter]\n",
    "    })  \n",
    "    # Display the DataFrame with some custom styles\n",
    "    ner_df.style.set_properties(**{'background-color': 'lightyellow', \n",
    "                               'color': 'black',\n",
    "                               'border-color': 'black'})\n",
    "    # Use tabulate to display a nice table in the terminal\n",
    "    print(tabulate(ner_df, headers='keys', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a635f44-ea38-4ec3-ae51-2a818952d502",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ------------------------------------------------------------------------------------------------\n",
    "# **************************************  End of Script  *****************************************\n",
    "# ------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64170a0-933c-42fe-a665-16873c6e738d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
