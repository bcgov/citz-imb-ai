{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a95ede9c-fce8-4b77-902b-9946f1925a5c",
   "metadata": {},
   "source": [
    "# Automated NER Annotation Pipeline for Legislative Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f323b0b-aebd-425a-a113-ec28b315e6f0",
   "metadata": {},
   "source": [
    "### Description\n",
    "This Jupyter notebook implements an automated Named Entity Recognition (NER) annotation pipeline for legislative text using Claude 3.5 Sonnet and Diffgram. The pipeline automates the process of identifying and labeling entities in legal documents, focusing on elements such as section references, subsection references, and metadata fields. The system integrates with AWS Bedrock for AI processing and Diffgram for annotation management.\n",
    "Key Components\n",
    "\n",
    "### Configuration and Setup\n",
    "- AWS Bedrock integration for Claude 3.5 Sonnet\n",
    "- Diffgram project setup and configuration\n",
    "- Schema management for NER labels\n",
    "\n",
    "### Core Functionality\n",
    "- Automated text processing and entity recognition\n",
    "- Integration with Diffgram's annotation system\n",
    "- Batch processing capabilities for multiple documents\n",
    "- Custom NER prompt engineering for legislative text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8374a19e-e3c6-4014-9ccc-7141abb03508",
   "metadata": {},
   "source": [
    "#### Install required dependencies for the annotation pipeline\n",
    "Core ML and data processing libraries\n",
    "AWS and API integration libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974d8e59-a574-4ac6-a5b3-d1a7189add2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers diffgram neo4j anthropic pandas tqdm\n",
    "!pip install llama_index\n",
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbc16c5-6005-4cea-bfb6-b6b1c744b90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install arize-phoenix-otel\n",
    "!pip install openinference-instrumentation-bedrock opentelemetry-exporter-otlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77d27448-300d-4bf2-98d2-4a4a47b92fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from diffgram import Project\n",
    "from typing import List, Dict, Optional\n",
    "import anthropic\n",
    "import json\n",
    "from neo4j import GraphDatabase\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import requests\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f561de-4bec-4d64-8272-898f38b0cb33",
   "metadata": {},
   "source": [
    "## Connect to Arize\n",
    "### Please install the arise container before using this script. Arise can help with observability when using A.I to annotate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52c42bfa-2ecb-4364-924f-933181bdc377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the new tracking code\n",
    "#from bedrock_output_fix import setup_claude_tracking, get_response_with_tracking\n",
    "from opentelemetry.trace import get_tracer_provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19d7ba06-a6d9-47c6-88e9-83d7abdb2fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”­ OpenTelemetry Tracing Details ðŸ”­\n",
      "|  Phoenix Project: pre-annotation-with-AI\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: http://phoenix:6006/v1/traces\n",
      "|  Transport: HTTP + protobuf\n",
      "|  Transport Headers: {}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from phoenix.otel import register\n",
    "\n",
    "tracer_provider = register(\n",
    "  project_name=\"pre-annotation-with-AI\", # Default is 'default'\n",
    "  endpoint=\"http://phoenix:6006/v1/traces\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "797c37b4-4553-4e0c-b30b-0fabe7687dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openinference.instrumentation.bedrock import BedrockInstrumentor\n",
    "BedrockInstrumentor().instrument(tracer_provider=tracer_provider,\n",
    "                                capture_response_body=True  # Enable response capture\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7cac79-2c18-4365-9f71-d17a337d8ff8",
   "metadata": {},
   "source": [
    "## Connect to bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b674b85-bd75-47b9-ab34-486a7af9bfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use os.getcwd() since __file__ is not available in interactive environments\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# If your structure is such that the package is in the parent directory, compute the parent directory:\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "\n",
    "# Add the parent directory to sys.path if it's not already there\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "696de560-7bc2-4dba-b621-40fb30283060",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AgenticWorkflow.bedrock_session import get_boto_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0432e5c4-9e62-4004-9a24-89af5d70b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  get_claude_kwargs\n",
    "from get_claude_kwargs import get_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cced89f1-4551-4207-9570-c827fe7816ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Does this work?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cd291e3-44f8-4514-a2bb-ad052fc045b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_with_tracking(prompt, job_data = None):\n",
    "    with get_tracer_provider().get_tracer(__name__).start_as_current_span(\"claude_request\") as span:\n",
    "        # Convert nested structure to flat attributes with dot notation\n",
    "        #span.set_attribute(\"llm.model_name\", \"anthropic.claude-3-5-sonnet-20240620-v1:0\")\n",
    "        #span.set_attribute(\"llm.token_count.prompt\", len(prompt.split()))\n",
    "        #span.set_attribute(\"llm.invocation_parameters\", get_response(prompt))\n",
    "        \n",
    "        try:\n",
    "            span.set_attribute(\"input.value\", prompt)\n",
    "            # Get response using original function\n",
    "            output = get_response(prompt)\n",
    "                     \n",
    "            # Set output as string\n",
    "            span.set_attribute(\"output.value\", output if output else \"None\")\n",
    "        \n",
    "            span.set_attribute(\"task.name\", job_data['nickname'] if job_data else \"None\")\n",
    "            span.set_attribute(\"task.index\", job_data['index'] if job_data else \"None\")\n",
    "            span.set_attribute(\"task.fileID\", job_data['file'] if job_data else \"None\")\n",
    "            \n",
    "            # Set span kind as string\n",
    "            span.set_attribute(\"openinference.span.kind\", \"LLM\")\n",
    "            \n",
    "            return output\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Log error as flat strings\n",
    "            span.set_attribute(\"error.message\", str(e))\n",
    "            span.set_attribute(\"error.type\", e.__class__.__name__)\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d18b1ce-5af1-459f-81ff-e7d1a872ff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_response_with_tracking(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e585a9a8-d522-42c6-a8cc-edd4b2b16217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I apologize, but I don\\'t have enough context to determine what \"this\" refers to or what you\\'re asking about. Could you please provide more information or context about what you\\'re trying to do or what you\\'re asking about? That way, I\\'ll be able to give you a more accurate and helpful response.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c08c9b-47b3-41dc-9f71-e09478d2239d",
   "metadata": {},
   "source": [
    "## Connect to Diffgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0cf52536-a6ec-4fba-bbab-8967972495f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diffgram project configuration\n",
    "DIFFGRAM_CONFIG = {\n",
    "    \"host\": \"http://dispatcher:8085\",\n",
    "    \"project_string_id\": \"translucenttracker\",\n",
    "    \"client_id\": \"LIVE__u3v8q0m7tx1p851dp0ap\",\n",
    "    \"client_secret\": \"1qgd8as7xfcbuem6mw9j1z0xvjfmmvlagbugqr8z1g1ntypugr2ul24cce5k\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0c35e68-0503-43d4-9b8a-a2d50938ef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize connection to Diffgram project\n",
    "project = Project(host=DIFFGRAM_CONFIG[\"host\"],\n",
    "        project_string_id = \"translucenttracker\",\n",
    "        client_id = \"LIVE__u3v8q0m7tx1p851dp0ap\",\n",
    "        client_secret = \"1qgd8as7xfcbuem6mw9j1z0xvjfmmvlagbugqr8z1g1ntypugr2ul24cce5k\"\n",
    "      )\n",
    "project_local = project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93db4cf9-f59d-4043-9082-fb2435ceb3c3",
   "metadata": {},
   "source": [
    "## Fetch Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afa798b4-2cd3-4818-9d88-0c2263cba6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and manage NER schema in Diffgram\n",
    "# Retrieve and process existing schema labels\n",
    "NER_schema_name = 'ENTITY_TRAINING_SCHEMA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf787fde-c18f-4f3b-b490-de58a27f8195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema_list(id):\n",
    "    auth = project.session.auth\n",
    "    url = f\"{DIFFGRAM_CONFIG['host']}/api/project/{DIFFGRAM_CONFIG['project_string_id']}/labels?schema_id={id}\"\n",
    "    # Step 4: Make the POST request using the SDK's session auth\n",
    "    response = requests.get(url, auth=auth)\n",
    "    # Step 5: Handle the response\n",
    "    if response.status_code == 200:\n",
    "        #print(\"Annotation update successful!\")\n",
    "        #pprint.pprint(response.json())  # View the updated data\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)  # Print error details for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85cbd3f7-ddd2-43f2-b235-beaa40525a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing Schemas in Diffgram:\n",
      "[\n",
      "  {\n",
      "    \"archived\": false,\n",
      "    \"id\": 8,\n",
      "    \"is_default\": true,\n",
      "    \"member_created_id\": 1,\n",
      "    \"member_updated_id\": null,\n",
      "    \"name\": \"Default Schema\",\n",
      "    \"project_id\": 4,\n",
      "    \"time_created\": \"2025-02-04 22:16:17\",\n",
      "    \"time_updated\": null\n",
      "  },\n",
      "  {\n",
      "    \"archived\": false,\n",
      "    \"id\": 9,\n",
      "    \"is_default\": false,\n",
      "    \"member_created_id\": 10,\n",
      "    \"member_updated_id\": null,\n",
      "    \"name\": \"NER_TRAINING_SCHEMA\",\n",
      "    \"project_id\": 4,\n",
      "    \"time_created\": \"2025-02-05 17:08:24\",\n",
      "    \"time_updated\": null\n",
      "  },\n",
      "  {\n",
      "    \"archived\": false,\n",
      "    \"id\": 11,\n",
      "    \"is_default\": false,\n",
      "    \"member_created_id\": 10,\n",
      "    \"member_updated_id\": null,\n",
      "    \"name\": \"ENTITY_TRAINING_SCHEMA\",\n",
      "    \"project_id\": 4,\n",
      "    \"time_created\": \"2025-02-05 17:20:02\",\n",
      "    \"time_updated\": null\n",
      "  }\n",
      "]\n",
      "Schema 'ENTITY_TRAINING_SCHEMA' already exists with id: 11\n",
      "{'I-SEQUENCE_ID', 'I-REGULATION_ID', 'B-ACT_NAME', 'I-DEFINITION', 'I-CHUNK_ID', 'I-ACT_NAME', 'O', 'B-REGULATION_ID', 'B-SECTION_ID', 'I-REQUIREMENT', 'I-ACT_ID', 'I-METADATA_VALUE', 'I-AUTHORITY', 'B-SECTION_NAME', 'B-METADATA_VALUE', 'B-REQUIREMENT', 'B-SECTION_REF', 'I-SECTION_ID', 'B-METADATA_FIELD', 'I-SECTION_REF', 'B-DEFINITION', 'I-SECTION_NAME', 'B-CHUNK_ID', 'I-METADATA_FIELD', 'I-SUBSECTION_REF', 'B-ACT_ID', 'B-SUBSECTION_REF', 'B-SEQUENCE_ID', 'B-AUTHORITY'}\n"
     ]
    }
   ],
   "source": [
    "schema_id = None\n",
    "\n",
    "# List the existing schemas in your Diffgram project.\n",
    "schemas = project.schema.list()\n",
    "print(\"Existing Schemas in Diffgram:\")\n",
    "print(json.dumps(schemas, indent=2))\n",
    "\n",
    "# Check if a schema with the name NER_schema_name already exists.\n",
    "for schema in schemas:\n",
    "    if schema.get('name') == NER_schema_name:\n",
    "        schema_id = schema.get('id')\n",
    "        break\n",
    "\n",
    "# If the schema does not exist, create a new one.\n",
    "if schema_id is None:\n",
    "    print(f\"Schema '{NER_schema_name}' not found. Creating a new one...\")\n",
    "    json_response = project.new_schema(name=NER_schema_name)\n",
    "    schema_id = json_response.get(\"id\")\n",
    "    print(f\"Created new schema with id: {schema_id}\")\n",
    "else:\n",
    "    print(f\"Schema '{NER_schema_name}' already exists with id: {schema_id}\")\n",
    "\n",
    "schema_labels = get_schema_list(schema_id)\n",
    "\n",
    "# Retrieve existing labels for the schema to avoid duplicates.\n",
    "schema_label_id_value = []\n",
    "if schema_labels is not None:\n",
    "    labels = schema_labels['labels_out']\n",
    "    for label in labels:\n",
    "        value = {}\n",
    "        value['id'] = label['id']\n",
    "        value['name'] = label['label']['name']\n",
    "        schema_label_id_value.append(value)\n",
    "\n",
    "existing_label_names = set()\n",
    "try:\n",
    "    schema_label_id_value[0]['name']\n",
    "    for label in schema_label_id_value:\n",
    "            label_name = label.get(\"name\")\n",
    "            if label_name:\n",
    "                existing_label_names.add(label_name)\n",
    "    print(existing_label_names)      \n",
    "except:\n",
    "     print(\"There are no schema labels\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "badd12c5-4168-469c-8e03-f60109981beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_list = project.schema.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccee80f3-7e6b-4f87-8f1d-bf2ad78c6167",
   "metadata": {},
   "source": [
    "## NER Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71613790-4318-49f9-ba86-38ae9173f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create structured prompt for NER tagging\n",
    "# Include rules and formatting guidelines for entity recognition\n",
    "def create_ner_prompt(text: str, schema: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Creates a prompt for NER tagging that defaults to O tag for most text.\n",
    "    \"\"\"\n",
    "    schema_tags = sorted(set(s['name'] for s in schema))\n",
    "    \n",
    "    prompt = \"\"\"You are a Named Entity Recognition system for legislative text.\n",
    "Your task is to label each word using only the following allowed tags:\n",
    "{allowed_tags}\n",
    "\n",
    "Rules:\n",
    "1. Default to 'O' tag - most words should be tagged as Outside\n",
    "2. Only tag these specific elements:\n",
    "   - Section references (e.g., \"section 46\" -> B-SECTION_REF, I-SECTION_REF)\n",
    "   - Subsection references (e.g., \"(2)\", \"(a)\" -> B-SUBSECTION_REF)\n",
    "   - Essential metadata fields and their values\n",
    "3. Tag principles:\n",
    "   - All punctuation should be 'O'\n",
    "   - Use B- prefix only for start of key references\n",
    "   - Use I- prefix for continuing words of same reference\n",
    "   - When in doubt, use 'O' tag\n",
    "\n",
    "Each space should be annotated separately. For e.g. If you see child 's annotate it as child and 's as two different words as there is a sapce between them. All ' s must be treated as one word 's.\n",
    "If there are no spaces between words like Act-Motor or 2subsection or 1in or b. treat this as one word. If b. and c. are separatd by a space then b. is one word and c. is another word. If there is a space between say en . then en is one word and the . is another word which should have its own label.\n",
    "\n",
    "Output format:\n",
    "[\n",
    "    {{\"word\": \"section\", \"tag\": \"B-SECTION_REF\"}},\n",
    "    {{\"word\": \"46\", \"tag\": \"I-SECTION_REF\"}},\n",
    "    {{\"word\": \",\", \"tag\": \"O\"}},\n",
    "    {{\"word\": \"subsection\", \"tag\": \"O\"}},\n",
    "    {{\"word\": \"(\", \"tag\": \"O\"}},\n",
    "    {{\"word\": \"2\", \"tag\": \"B-SUBSECTION_REF\"}},\n",
    "    {{\"word\": \"'s\", \"tag\": \"O\"}},\n",
    "    {{\"word\": \")\", \"tag\": \"O\"}}\n",
    "]\n",
    "\n",
    "Text to label:\n",
    "{text}\n",
    "\n",
    "Provide only the JSON output with no additional explanation.\"\"\"\n",
    "\n",
    "    tags_str = \"\\n\".join(f\"- {tag}\" for tag in schema_tags)\n",
    "    return prompt.format(allowed_tags=tags_str, text=text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0355237-0369-4981-9243-dc435feb5d28",
   "metadata": {},
   "source": [
    "## Import all the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fda6544b-fe12-43f7-ac3d-8fe1a46f6552",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = project_local.job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3dea05c-3d92-4fb5-b11f-44e31b869b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_job = project_local.job.list(limit=10000, page_number=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9ff235b-9f89-4505-b9bc-781a481e9c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_with_data_index = []\n",
    "for job_key, job_list in enumerate(get_job):\n",
    "    try:\n",
    "        nickname = job_list['attached_directories_dict']['attached_directories_list'][0]['nickname']\n",
    "        if nickname:\n",
    "            job_value = {}\n",
    "            job_value['nickname'] = nickname\n",
    "            job_value['index'] = job_key\n",
    "            jobs_with_data_index.append(job_value)\n",
    "        #print(nickname)\n",
    "    except KeyError:\n",
    "        print(\"Key not found.\")\n",
    "    except IndexError:\n",
    "        print(\"List index out of range.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b0395c-52cf-4c63-9dba-fc649e0bf7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_with_data_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc8b2f1-610d-4fff-a99f-02f08fd59ff1",
   "metadata": {},
   "source": [
    "## Diffgram get utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f08e665b-39b7-464d-acee-bef5356de04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for processing Diffgram annotations\n",
    "# Extract and format word-level data from files\n",
    "def get_file_number(completed_annotations, files_index_in_job):\n",
    "    data = []\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    data_index = 0\n",
    "    for completed_annotation in completed_annotations:\n",
    "        try:\n",
    "            file_index = int(completed_annotation)\n",
    "            files_index_in_job.append(completed_annotation)\n",
    "            #print(completed_annotation)\n",
    "            continue\n",
    "        except:\n",
    "            #print(f\"{completed_annotation} is not a file\")\n",
    "            continue\n",
    "            \n",
    "        #print(f\"{completed_annotation} ----\")\n",
    "        if (completed_annotation != 'attribute_groups_reference')  \\\n",
    "            and (completed_annotation != 'export_info') \\\n",
    "            and (completed_annotation != 'label_map') \\\n",
    "            and (completed_annotation != 'readme') \\\n",
    "            and (completed_annotation != 'label_colour_map'):\n",
    "            sentence_local = []\n",
    "            labels_local = []\n",
    "\n",
    "            # First get the point where the annotation is started\n",
    "            for start in completed_annotations[completed_annotation]['instance_list']:\n",
    "                if 'start_token' in start:\n",
    "                    start_token =  start['start_token']\n",
    "                    break\n",
    "\n",
    "            #start_token = completed_annotations[completed_annotation]['instance_list'][0]['start_token']\n",
    "            for annotated_index in range(start_token, len(completed_annotations[completed_annotation]['text']['tokens']['words'])):\n",
    "                # check if this text is annotated\n",
    "                for data in completed_annotations[completed_annotation]['instance_list']:\n",
    "                    if 'start_token' in data:\n",
    "                        if annotated_index == data['start_token']:\n",
    "                            sentence_local.append(completed_annotations[completed_annotation]['text']['tokens']['words'][annotated_index]['value'])\n",
    "                            labels_local.append(completed_annotations['label_map'][str(data['label_file_id'])])\n",
    "                            #print(f\"{completed_annotations[completed_annotation]['text']['tokens']['words'][annotated_index]['value']} - {completed_annotations['label_map'][str(data['label_file_id'])]}\")\n",
    "                            break;\n",
    "            sentences.append(sentence_local)       \n",
    "            labels.append(labels_local)\n",
    "            data_index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e989797e-f9b4-497f-9b7f-5e10d5631cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_data(url):\n",
    "    # Original URL with localhost\n",
    "    # Replace localhost with ngrok URL (example: \"https://example.ngrok.io\")\n",
    "    file_url = url.replace(\"http://localhost:8085\", DIFFGRAM_CONFIG['host'])\n",
    "\n",
    "    # Make the GET request to fetch the file\n",
    "    response = requests.get(file_url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON content into a Python dictionary\n",
    "        data = response.json()  # Assuming the file is in JSON format\n",
    "        return data\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6171bf19-c663-42e9-8ea2-ca53561a7414",
   "metadata": {},
   "source": [
    "## Diffgram Send Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc626ff3-e422-4cf2-a727-fd61776ce602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to send pre-annotations back to Diffgram\n",
    "# Handle API communication and response processing\n",
    "def send_preannotation_to_diffgram(file):\n",
    "    # Step 1: Extract the session's auth credentials (client_id and client_secret)\n",
    "    auth = project_local.session.auth\n",
    "\n",
    "    # Step 2: Define the API URL for the custom annotation update endpoint\n",
    "    file_id = file.id  # Replace with your file ID\n",
    "    #project_string_id = \"your_project_string_id\"  # Replace with your project string ID\n",
    "    url = f\"{DIFFGRAM_CONFIG['host']}/api/project/{DIFFGRAM_CONFIG['project_string_id']}/file/{file_id}/annotation/update\"\n",
    "\n",
    "    # Step 3: Define the data (e.g., instance_list) for updating annotations\n",
    "    data = {\n",
    "        \"instance_list\": file.__dict__['instance_list']\n",
    "    }\n",
    "\n",
    "    # Step 4: Make the POST request using the SDK's session auth\n",
    "    response = requests.post(url, json=data, auth=auth)\n",
    "\n",
    "    # Step 5: Handle the response\n",
    "    if response.status_code == 200:\n",
    "        print(\"Annotation update successful!\")\n",
    "        # print(response.json())  # View the updated data\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)  # Print error details for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79163c4-2b82-49f4-baaa-1cdbe6137332",
   "metadata": {},
   "source": [
    "## Run annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8a1e48-95f0-4111-9cbb-eb8ba2351f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_index = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5279b87-bd2d-4d10-b529-8bef00f95853",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.refresh_from_dict(get_job[job_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7fd9b4-7b88-4a26-9599-eefdbd5b6243",
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_annotations = results.generate_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b49a73-4846-4464-8b4e-4813acf68e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(completed_annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e174ea-d79b-4f7a-8156-ba3df535c54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_index_in_job = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f506784-4673-435f-bf37-c438e18e240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_file_number(completed_annotations, files_index_in_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab40acd-8c69-4418-b830-d06faaef5431",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_index_in_job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f31ddf-e9aa-4146-af6d-daa0235984b2",
   "metadata": {},
   "source": [
    "## Automate the A.I annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aef4b31c-34fe-4b0a-b0e4-9c1bd9ab0ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_string(s):\n",
    "    # Handle double backticks first\n",
    "    s = s.replace('``', '\"')\n",
    "    # Handle other quote variations\n",
    "    s = s.replace('\"', '\"').replace('\"', '\"').replace('\\'\\'', '\"')\n",
    "    s = s.replace('\\\\', '')\n",
    "    s = s.replace('\\'s', 'is')\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f15abd1-d83a-4221-8804-0b4b37ae01c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_per_task_preannotation(file_index, job_data,  start_index=None):\n",
    "    if start_index is None:  # Better to check for None explicitly\n",
    "        start_index = file_index[0]\n",
    "    \n",
    "    # enumerate needs an iterable and returns both index and value\n",
    "    # Also, we want to slice the file_index from start_index\n",
    "    for i, file in enumerate(file_index[0:]):\n",
    "        # Your processing code here\n",
    "        print(file)\n",
    "        job_data['file'] = file\n",
    "        file = project_local.file.get_by_id(file,with_instances=True)\n",
    "        url = file.__dict__['text']['tokens_url_signed']\n",
    "        #print(url)\n",
    "        data = extract_word_data(url)\n",
    "        schema_list = project.schema.list()\n",
    "\n",
    "        words = \"\"\n",
    "        for word in data['nltk']['words']:\n",
    "            words += word['value'] + ' '\n",
    "        prompt = create_ner_prompt(words, schema_label_id_value)\n",
    "        #print(prompt)\n",
    "        try:\n",
    "            response = get_response_with_tracking(prompt, job_data)\n",
    "            ai_annotation = json.loads(response)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        len(ai_annotation)\n",
    "\n",
    "        annotate_index = 0\n",
    "        instance_list = []\n",
    "        for word_idx, words in enumerate(data['nltk']['words']):\n",
    "            #print(f\"Word idx {word_idx} and Word is {words['value']}\")\n",
    "            if (annotate_index >= len(ai_annotation)):\n",
    "                print(\"no more annotation from ai\")\n",
    "                #instance_list = default_annotation_O(instance_list, word_idx, schema_label_id_value)\n",
    "            elif (normalize_string(words['value']) == normalize_string(ai_annotation[annotate_index]['word'])):\n",
    "                #print(\"match\")\n",
    "                label_id = next((label['id'] for label in schema_label_id_value \n",
    "                        if label['name'] == ai_annotation[annotate_index]['tag']), None)\n",
    "                \n",
    "                if label_id:\n",
    "                    instance = {\n",
    "                        \"label_file_id\": label_id,\n",
    "                        \"start_token\": word_idx,\n",
    "                        \"end_token\": word_idx,\n",
    "                        \"type\": 'text_token'\n",
    "                    }\n",
    "                    instance_list.append(instance)\n",
    "                annotate_index +=1\n",
    "            else:\n",
    "                continue\n",
    "                #print(\"\")\n",
    "                #print(\"probably a new line. Move annotate pointer:\")\n",
    "        file.__dict__['instance_list'] = instance_list\n",
    "        sent = send_preannotation_to_diffgram(file)\n",
    "        #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de425fad-29b3-4137-a2af-0d8c695d0fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main processing loop for automated annotation\n",
    "# Handle text normalization and annotation matching\n",
    "# Process annotations batch-wise across multiple files\n",
    "for job_index in jobs_with_data_index[100:101]:\n",
    "    print(f\"The job nickname is {job_index['nickname']} and the index is {job_index['index']}\")\n",
    "    results.refresh_from_dict(get_job[job_index['index']])\n",
    "    completed_annotations = results.generate_export()\n",
    "    files_index_in_job = []\n",
    "    get_file_number(completed_annotations, files_index_in_job)\n",
    "    files_index_in_job\n",
    "    process_per_task_preannotation(files_index_in_job, job_index, 12705)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc431cb-4bea-4ecd-a3e9-54e0e9cd9fec",
   "metadata": {},
   "source": [
    "### Parallel Pre-annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3b76eb-6d17-448e-93fe-5143ad4fb47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor  # Changed from ProcessPoolExecutor\n",
    "\n",
    "def parallel_process_jobs(jobs_with_data_index, jobs, get_job, max_workers=4):\n",
    "    \"\"\"Parallelize at the job level using threads instead of processes\"\"\"\n",
    "    jobs_to_process = jobs_with_data_index\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:  # Changed to ThreadPoolExecutor\n",
    "        futures = [\n",
    "            executor.submit(process_single_job, job, jobs, get_job)\n",
    "            for job in jobs_to_process\n",
    "        ]\n",
    "        \n",
    "        # Track results\n",
    "        processed_jobs = []\n",
    "        for future in futures:\n",
    "            try:\n",
    "                result = future.result()\n",
    "                processed_jobs.append(result)\n",
    "                \n",
    "                if result['status'] == 'success':\n",
    "                    print(f\"Completed job {result['nickname']}, processed {result['files_processed']} files\")\n",
    "                else:\n",
    "                    print(f\"Failed job {result['nickname']}: {result.get('error')}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Exception in worker thread: {str(e)}\")  # Updated error message\n",
    "        \n",
    "        return processed_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e099de8e-c989-4d38-99b8-1528b5fb7d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_job(job_data: Dict, results_obj, get_job):\n",
    "    \"\"\"Process a single job with sequential file handling\"\"\"\n",
    "    try:\n",
    "        print(f\"Starting job: {job_data['nickname']} (index: {job_data['index']})\")\n",
    "        \n",
    "        # Get job data\n",
    "        results_obj.refresh_from_dict(get_job[job_data['index']])  # Use the shared object directly\n",
    "        \n",
    "        # Get files for this job\n",
    "        completed_annotations = results_obj.generate_export()\n",
    "        files_index_in_job = []\n",
    "        get_file_number(completed_annotations, files_index_in_job)\n",
    "        \n",
    "        # Process files sequentially\n",
    "        process_per_task_preannotation(files_index_in_job, job_data, 12705)\n",
    "        \n",
    "        return {\n",
    "            'status': 'success',\n",
    "            'job_index': job_data['index'],\n",
    "            'nickname': job_data['nickname'],\n",
    "            'files_processed': len(files_index_in_job)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in job {job_data['nickname']}: {str(e)}\")  # Added error logging\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'job_index': job_data['index'],\n",
    "            'nickname': job_data['nickname'],\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "# Usage remains the same|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5725d504-2f68-44b4-89e0-53d40840bf27",
   "metadata": {},
   "source": [
    "If you want to annotate specific indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "400f40ea-c5e5-41c9-9911-036ff2e062c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### jobs = project_local.job\n",
    "#results = parallel_process_jobs(jobs_with_data_index[100:101], jobs, get_job, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6433294-731e-4442-b244-c148da743f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#indices = [59, 97, 99, 104, 117, 120, 121, 130, 131, 144]\n",
    "#selected_jobs = [jobs_with_data_index[i] for i in indices]\n",
    "#results = parallel_process_jobs(selected_jobs, jobs, get_job, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228c776f-e29f-4cec-99d6-fa2faf8a1af9",
   "metadata": {},
   "source": [
    "### If the token for the output is a lot which can happen when each character may need annotation, it is best to break the input into two prompts and then append them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7839acd2-707b-45fc-bc38-65cd6ad6714c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def process_per_file_annotation(job_data):\n",
    "    try:\n",
    "        file_id = job_data['file']\n",
    "        print(f\"Processing file id {file_id} of task {job_data['nickname']} of index {job_data['index']}\")\n",
    "        file = project_local.file.get_by_id(file_id,with_instances=True)\n",
    "        url = file.__dict__['text']['tokens_url_signed']\n",
    "        #print(url)\n",
    "        data = extract_word_data(url)\n",
    "        schema_list = project.schema.list()\n",
    "    \n",
    "        words = \"\"\n",
    "        for word in data['nltk']['words']:\n",
    "            words += word['value'] + ' '\n",
    "        #prompt = create_ner_prompt(words, schema_label_id_value)\n",
    "        #print(prompt)\n",
    "    \n",
    "        prompt = create_ner_prompt(words[0: math.floor(len(words)/2)], schema_label_id_value)\n",
    "        response1 = get_response_with_tracking(prompt, job_data)\n",
    "        prompt = create_ner_prompt(words[math.floor(len(words)/2)+1:], schema_label_id_value)\n",
    "        response2 = get_response_with_tracking(prompt,  job_data)\n",
    "        \n",
    "        response1 = json.loads(response1)\n",
    "        response2 = json.loads(response2)\n",
    "        response = response1 + response2\n",
    "        ai_annotation = response\n",
    "\n",
    "            \n",
    "        len(ai_annotation)\n",
    "    \n",
    "        annotate_index = 0\n",
    "        instance_list = []\n",
    "        for word_idx, words in enumerate(data['nltk']['words']):\n",
    "            #print(f\"Word idx {word_idx} and Word is {words['value']}\")\n",
    "            if (annotate_index >= len(ai_annotation)):\n",
    "                print(\"no more annotation from ai\")\n",
    "                #instance_list = default_annotation_O(instance_list, word_idx, schema_label_id_value)\n",
    "            elif (normalize_string(words['value']) == normalize_string(ai_annotation[annotate_index]['word'])):\n",
    "                #print(\"match\")\n",
    "                label_id = next((label['id'] for label in schema_label_id_value \n",
    "                        if label['name'] == ai_annotation[annotate_index]['tag']), None)\n",
    "                \n",
    "                if label_id:\n",
    "                    instance = {\n",
    "                        \"label_file_id\": label_id,\n",
    "                        \"start_token\": word_idx,\n",
    "                        \"end_token\": word_idx,\n",
    "                        \"type\": 'text_token'\n",
    "                    }\n",
    "                    instance_list.append(instance)\n",
    "                annotate_index +=1\n",
    "            else:\n",
    "                continue\n",
    "                #print(\"\")\n",
    "                #print(\"probably a new line. Move annotate pointer:\")\n",
    "        file.__dict__['instance_list'] = instance_list\n",
    "        sent = send_preannotation_to_diffgram(file)\n",
    "        return {\n",
    "            'status': 'success',\n",
    "            'job_index': job_data['index'],\n",
    "            'nickname': job_data['nickname'],\n",
    "            'file_id': job_data['file']\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error in job {job_data['nickname']}: {str(e)}\")\n",
    "        print(f\"file id {file_id} of task {job_data['nickname']} of index {job_data['index']} Not able to split the data\") \n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'job_index': job_data['index'],\n",
    "            'nickname': job_data['nickname'],\n",
    "            'error': str(e)\n",
    "        }\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5fc5ff7b-f08d-4a66-a16b-31394ef0aa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process_failed_annotations(failed_files, max_workers=4):\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all files for processing\n",
    "        futures = [\n",
    "            executor.submit(process_per_file_annotation, file)\n",
    "            for file in failed_files\n",
    "        ]\n",
    "        \n",
    "        # Track results\n",
    "        processed_files = []\n",
    "        for future in futures:\n",
    "            try:\n",
    "                result = future.result()\n",
    "                processed_files.append(result)\n",
    "                \n",
    "                # Assuming process_per_file_annotation returns a dict with status\n",
    "                if result.get('status') == 'success':\n",
    "                    print(f\"Successfully processed file {result.get('file_id')}\")\n",
    "                else:\n",
    "                    print(f\"Failed to process file {result.get('file_id')}: {result.get('error')}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Exception in worker thread: {str(e)}\")\n",
    "                \n",
    "        return processed_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40efeab-a442-4a62-9b15-e30b1631b7b3",
   "metadata": {},
   "source": [
    "IF you want to annotate specific file then you can use this array to populate the individual files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "53f408a7-6ec3-4ec8-a39b-b4886bd492de",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_to_annotate = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "db4c7e68-9a5e-46a2-8c12-bfd4beef12ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(failed_to_annotate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d9e4aec4-100b-4b51-bba1-667ae81d48e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file id 19747 of task NER_train_batch_218 of index 31\n",
      "Annotation update successful!\n",
      "Successfully processed file 19747\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "failed_to_annotate = [{'nickname': 'NER_train_batch_218', 'index': 31, 'file': '19747'}]\n",
    "results = process_failed_annotations(failed_to_annotate, max_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cc2113-6659-4b7a-81ca-b3aadfd9a421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
