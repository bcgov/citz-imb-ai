{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e179e445-524d-48fa-b5cb-f1951beb5057",
   "metadata": {},
   "source": [
    "##### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99138db-0c3c-42f2-97d7-02ac84779b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27bac2f-d2fd-4acb-a01a-e0aa53ea8d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers diffgram neo4j anthropic pandas tqdm\n",
    "!pip install llama_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388d2206-2a36-4f68-a4c5-d1b446a21853",
   "metadata": {},
   "source": [
    "##### # Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cfde9e-089f-4703-8bc7-478f84efda71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from diffgram import Project\n",
    "from typing import List, Dict, Optional\n",
    "import anthropic\n",
    "import json\n",
    "from neo4j import GraphDatabase\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94357192-ac39-4d6e-b5c4-5efa0b820dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use os.getcwd() since __file__ is not available in interactive environments\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# If your structure is such that the package is in the parent directory, compute the parent directory:\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "\n",
    "# Add the parent directory to sys.path if it's not already there\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1334dee6-69f2-4231-9397-52093602e447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AgenticWorkflow.bedrock_session import get_boto_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ea77d3-8940-4536-996c-dfe8f5ec7bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = get_boto_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290e2fd9-fb37-46ed-b5f6-f158fa9acf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_runtime = session.client(\"bedrock-runtime\", region_name=\"us-east-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaa3b01-c6ab-40cd-ad92-f73ee43e2341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_claudia_kwargs(prompt):\n",
    "    kwargs = {\n",
    "      \"modelId\": \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "      \"contentType\": \"application/json\",\n",
    "      \"accept\": \"application/json\",\n",
    "      \"body\": json.dumps({\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 5000,\n",
    "        \"messages\": [\n",
    "          {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "              {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt\n",
    "              }\n",
    "            ]\n",
    "          }\n",
    "        ]\n",
    "      })\n",
    "    }\n",
    "    return kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2366a3e2-6685-4c68-86d9-0d1734bc3bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Does this work?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959e8c46-9786-480d-9072-02ea532b0d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = get_claudia_kwargs(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edaa88e-6799-4b4c-b0e7-5b7a079c627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(prompt):\n",
    "    kwargs = get_claudia_kwargs(prompt)\n",
    "    response = bedrock_runtime.invoke_model(**kwargs)\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    return response_body['content'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdd20cc-d3fa-439d-bc43-9d9bbb0d9d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438661ae-96e7-4217-971e-eca83c318de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9f3cf4-6c57-48ca-bb0a-8006be7d71ca",
   "metadata": {},
   "source": [
    "#### Setting up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421cff16-01ea-4d7f-94d8-8e7605d8b6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7b6cbb-f259-4812-a50c-f9cda804945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = Project(host=\"https://5293-2604-3d08-4f7f-e8c0-a011-c635-235f-690d.ngrok-free.app\",\n",
    "        project_string_id = \"translucenttracker\",\n",
    "        client_id = \"LIVE__u3v8q0m7tx1p851dp0ap\",\n",
    "        client_secret = \"1qgd8as7xfcbuem6mw9j1z0xvjfmmvlagbugqr8z1g1ntypugr2ul24cce5k\"\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf80158-c95a-4fb0-9277-b37efcea5901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DIFFGRAM_CONFIG = {\n",
    "    \"host\": \"https://5293-2604-3d08-4f7f-e8c0-a011-c635-235f-690d.ngrok-free.app\",\n",
    "    \"project_id\": \"translucenttracker\",\n",
    "    \"client_id\": \"LIVE__u3v8q0m7tx1p851dp0ap\",\n",
    "    \"client_secret\": \"1qgd8as7xfcbuem6mw9j1z0xvjfmmvlagbugqr8z1g1ntypugr2ul24cce5k\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c9e123-347d-4f55-8bc7-87e1ba7d08de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 256\n",
    "NUM_TRAIN_SAMPLES = 10000  # Number of samples to use for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e15a6a3-8add-4543-adfd-1286da292ba6",
   "metadata": {},
   "source": [
    "## Import all the files \n",
    "### make sure you have the diffgram_processing_v2 folder which has all the data arranged for NER task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1373677a-b382-4bbc-a43f-c15113301f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, StorageContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1fac11-9de7-412f-bd65-75490f705d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_metadata = lambda x: {\"filename\": x}\n",
    "diffgram_documents = SimpleDirectoryReader(\"diffgram_processing\",file_metadata=file_metadata).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ad4983-19fa-46fe-84c4-3222dedea0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(diffgram_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecfdddd-215a-4f3b-952d-5d6622649023",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diffgram_documents[500].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a5dcf4-7204-45c6-a962-8843abf258be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_chunks(diffgram_dcouments, sample_size: int = 10) -> List[Dict]:\n",
    "    \"\"\"Get sample chunks from diffgram processing folder\"\"\"\n",
    "    # Load all documents\n",
    "    documents = diffgram_documents\n",
    "    logger.info(f\"Loaded {len(documents)} documents\")\n",
    "    \n",
    "    # Sample documents ensuring diversity\n",
    "    samples = []\n",
    "    seen_types = set()\n",
    "    \n",
    "    # Randomly shuffle documents\n",
    "    shuffled_docs = list(documents)\n",
    "    random.shuffle(shuffled_docs)\n",
    "    \n",
    "    for doc in shuffled_docs:\n",
    "        if len(samples) >= sample_size:\n",
    "            break\n",
    "            \n",
    "        # Extract document type from metadata/text\n",
    "        doc_type = None\n",
    "        if 'Act' in doc.text:\n",
    "            doc_type = 'Act'\n",
    "        elif 'Regulation' in doc.text:\n",
    "            doc_type = 'Regulation'\n",
    "            \n",
    "        if doc_type not in seen_types or len(seen_types) >= sample_size/2:\n",
    "            samples.append({\n",
    "                'text': doc.text,\n",
    "                'type': doc_type,\n",
    "                'filename': doc.metadata['filename']\n",
    "            })\n",
    "            if doc_type:\n",
    "                seen_types.add(doc_type)\n",
    "                \n",
    "    logger.info(f\"Selected {len(samples)} sample documents\")\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b3c381-e450-4da8-b57f-276819cf04a7",
   "metadata": {},
   "source": [
    "## Schema Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "1f7cbc94-e748-494c-9e43-9d3552a1aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "def get_sample_chunks(diffgram_documents, sample_size: int = 10) -> List[Dict]:\n",
    "    \"\"\"Get sample chunks from diffgram processing folder using regex to detect Act and Regulation IDs.\"\"\"\n",
    "    logger.info(f\"Loaded {len(diffgram_documents)} documents\")\n",
    "    \n",
    "    samples = []\n",
    "    seen_types = set()\n",
    "    \n",
    "    # Regex patterns to extract IDs\n",
    "    act_pattern = re.compile(r\"ActId:\\s*([^\\n]+)\", re.IGNORECASE)\n",
    "    reg_pattern = re.compile(r\"RegId:\\s*([^\\n]+)\", re.IGNORECASE)\n",
    "    \n",
    "    # Shuffle the documents to ensure randomness\n",
    "    shuffled_docs = list(diffgram_documents)\n",
    "    random.shuffle(shuffled_docs)\n",
    "    \n",
    "    for doc in shuffled_docs:\n",
    "        if len(samples) >= sample_size:\n",
    "            break\n",
    "\n",
    "        doc_text = doc.text\n",
    "        \n",
    "        # Try to detect Regulation ID first; if found, mark as Regulation.\n",
    "        reg_match = reg_pattern.search(doc_text)\n",
    "        act_match = act_pattern.search(doc_text)\n",
    "        \n",
    "        if reg_match:\n",
    "            doc_type = 'Regulation'\n",
    "        elif act_match:\n",
    "            doc_type = 'Act'\n",
    "        else:\n",
    "            # If no ID is found, you may choose to label as None or skip.\n",
    "            doc_type = None\n",
    "\n",
    "        # Decide if we should add this document.\n",
    "        # Here we add the document if its type hasn't been added yet,\n",
    "        # or if we've already collected at least half of the desired samples (to ensure diversity).\n",
    "        if doc_type not in seen_types or len(seen_types) >= sample_size / 2:\n",
    "            samples.append({\n",
    "                'text': doc_text,\n",
    "                'type': doc_type,\n",
    "                'filename': doc.metadata.get('filename', 'unknown')\n",
    "            })\n",
    "            if doc_type:\n",
    "                seen_types.add(doc_type)\n",
    "                \n",
    "    logger.info(f\"Selected {len(samples)} sample documents\")\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "4c4f92de-efce-41f0-bfc8-42fd92d431e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "# Cell 4: Schema Generation\n",
    "\n",
    "def generate_schema_prompt(samples: List[Dict]) -> str:\n",
    "    \"\"\"\n",
    "    Create a prompt for generating a dynamic NER tagging schema for legal texts.\n",
    "    The output must be a JSON object with:\n",
    "      - a top-level \"schema\" key containing a list of BIO entity definitions,\n",
    "      - and an optional \"triplet_schema\" key containing relationship definitions.\n",
    "    \n",
    "    The tagging should follow the BIO convention:\n",
    "      - 'B-' indicates the beginning of an entity,\n",
    "      - 'I-' indicates a continuation of an entity,\n",
    "      - 'O' indicates tokens outside any entity.\n",
    "      \n",
    "    The entity definitions and relationship definitions should be dynamically derived from the provided sample texts.\n",
    "    The output JSON must be directly compatible with Diffgram.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"Analyze the following legal text samples and generate a comprehensive NER tagging schema tailored for legal documents. \"\n",
    "        \"Your output must be a JSON object with two keys: \\n\"\n",
    "        \"  1. \\\"schema\\\": an array of entity definitions using BIO tagging (include 'O' for outside tokens), and \\n\"\n",
    "        \"  2. \\\"triplet_schema\\\": an array of relationship definitions capturing subject–predicate–object triplets (if applicable).\\n\\n\"\n",
    "        \"For the BIO tagging schema, ensure you include definitions such as:\\n\"\n",
    "        \"  - O: tokens outside any named entity,\\n\"\n",
    "        \"  - B-<ENTITY_TYPE> and I-<ENTITY_TYPE> for the beginning and continuation of each entity type (e.g., internal section references, external act/regulation references, etc.).\\n\\n\"\n",
    "        \"For the triplet schema, define how relationships between entities are represented, including subject, relation, and object, \"\n",
    "        \"with examples drawn from the text.\\n\\n\"\n",
    "        \"Output the schema in the following format (do not include any extra text):\\n\\n\"\n",
    "        \"{\\n\"\n",
    "        '  \"schema\": [\\n'\n",
    "        \"    {\\n\"\n",
    "        '      \"name\": \"O\",\\n'\n",
    "        '      \"description\": \"Outside of any named entity\",\\n'\n",
    "        '      \"example\": \"The\"\\n'\n",
    "        \"    },\\n\"\n",
    "        \"    {\\n\"\n",
    "        '      \"name\": \"B-<ENTITY_TYPE>\",\\n'\n",
    "        '      \"description\": \"Description for the beginning of <ENTITY_TYPE>\",\\n'\n",
    "        '      \"example\": \"Dynamic example based on sample\"\\n'\n",
    "        \"    },\\n\"\n",
    "        \"    ...\\n\"\n",
    "        \"  ],\\n\"\n",
    "        '  \"triplet_schema\": [\\n'\n",
    "        \"    {\\n\"\n",
    "        '      \"subject\": \"<subject tag>\",\\n'\n",
    "        '      \"relation\": \"<relation type>\",\\n'\n",
    "        '      \"object\": \"<object tag>\",\\n'\n",
    "        '      \"description\": \"Description of the relationship\",\\n'\n",
    "        '      \"example\": \"Example triplet extracted from text\"\\n'\n",
    "        \"    }\\n\"\n",
    "        \"  ]\\n\"\n",
    "        \"}\\n\\n\"\n",
    "        \"Now, analyze the following samples:\\n\"\n",
    "    )\n",
    "    \n",
    "    for i, sample in enumerate(samples, 1):\n",
    "        prompt += f\"\\nSample {i}:\\n{sample['text'][:500]}...\\n\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "def validate_schema(schema: Dict) -> Dict:\n",
    "    \"\"\"Validate and enhance the generated schema by ensuring all required entities are present.\"\"\"\n",
    "    required_entities = {\n",
    "        'references': [\n",
    "            'INTERNAL_SECTION_REF',\n",
    "            'EXTERNAL_ACT_REF',\n",
    "            'EXTERNAL_REGULATION_REF'\n",
    "        ],\n",
    "        'structure': [\n",
    "            'SECTION',\n",
    "            'SUBSECTION',\n",
    "            'PARAGRAPH',\n",
    "            'SUBPARAGRAPH'\n",
    "        ],\n",
    "        'concepts': [\n",
    "            'DEFINITION',\n",
    "            'LEGAL_TERM',\n",
    "            'TIME_PERIOD'\n",
    "        ],\n",
    "        'relationships': [\n",
    "            'CROSS_REF',\n",
    "            'AMENDMENT_REF',\n",
    "            'PARENT_CHILD_REF'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Safely retrieve the existing list of entity types; initialize if missing.\n",
    "    existing_entities = {entity['name'] for entity in schema.get('entity_types', [])}\n",
    "    \n",
    "    # Iterate over each category and its required entity names.\n",
    "    for category, entities in required_entities.items():\n",
    "        for entity in entities:\n",
    "            # For each entity, ensure both BIO variants are present.\n",
    "            for prefix in ['B-', 'I-']:\n",
    "                full_entity_name = f\"{prefix}{entity}\"\n",
    "                if full_entity_name not in existing_entities:\n",
    "                    schema.setdefault('entity_types', []).append({\n",
    "                        'name': full_entity_name,\n",
    "                        'description': f\"{'Beginning' if prefix == 'B-' else 'Inside'} token for {entity}\",\n",
    "                        'example': f\"Example usage of {entity} from text\",\n",
    "                        'category': category\n",
    "                    })\n",
    "    \n",
    "    return schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "010a62b8-fe40-4ca3-b1bb-a7736f2dbe5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loaded 81611 documents\n",
      "INFO:__main__:Selected 10 sample documents\n"
     ]
    }
   ],
   "source": [
    "# Get sample documents\n",
    "samples = get_sample_chunks(diffgram_documents, sample_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "8329d2c1-df04-4ed2-b3de-0e51e188a20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_prompt = generate_schema_prompt(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "a5e65e62-5f3b-4949-87b7-e1022434c7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate schema\n",
    "schema_response = get_response(schema_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "27e170a2-256a-41b5-b073-598db77af8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_response = json.loads(schema_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "54f0d7b8-a13b-4a27-ade8-091ceb637dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'schema': [{'name': 'O', 'description': 'Outside of any named entity', 'example': 'The'}, {'name': 'B-ACT', 'description': 'Beginning of an Act reference', 'example': 'Pension Benefits Standards Act'}, {'name': 'I-ACT', 'description': 'Continuation of an Act reference', 'example': 'Benefits Standards Act'}, {'name': 'B-REGULATION', 'description': 'Beginning of a Regulation reference', 'example': 'Pipeline Regulation'}, {'name': 'I-REGULATION', 'description': 'Continuation of a Regulation reference', 'example': 'Consumer Credit Regulation'}, {'name': 'B-SECTION', 'description': 'Beginning of a Section reference', 'example': 'section 41'}, {'name': 'I-SECTION', 'description': 'Continuation of a Section reference', 'example': 'subsection ( 2 )'}, {'name': 'B-LEGAL_TERM', 'description': 'Beginning of a legal term', 'example': 'collective agreement'}, {'name': 'I-LEGAL_TERM', 'description': 'Continuation of a legal term', 'example': 'solemn declaration'}, {'name': 'B-AUTHORITY', 'description': 'Beginning of an authority reference', 'example': 'lieutenant governor'}, {'name': 'I-AUTHORITY', 'description': 'Continuation of an authority reference', 'example': 'in council'}, {'name': 'B-POSITION', 'description': 'Beginning of a position or role', 'example': 'commissioner'}, {'name': 'I-POSITION', 'description': 'Continuation of a position or role', 'example': 'for taking affidavits'}, {'name': 'B-DOCUMENT', 'description': 'Beginning of a document reference', 'example': 'actuarial valuation report'}, {'name': 'I-DOCUMENT', 'description': 'Continuation of a document reference', 'example': 'cost certificate'}, {'name': 'B-ACTION', 'description': 'Beginning of a legal action', 'example': 'design'}, {'name': 'I-ACTION', 'description': 'Continuation of a legal action', 'example': 'construct'}, {'name': 'B-STANDARD', 'description': 'Beginning of a standard reference', 'example': 'csa z662'}, {'name': 'I-STANDARD', 'description': 'Continuation of a standard reference', 'example': 'annex a of csa z662'}], 'triplet_schema': [{'subject': 'B-AUTHORITY', 'relation': 'appoints', 'object': 'B-POSITION', 'description': 'Authority appointing a position', 'example': 'lieutenant governor in council may appoint, under the public service act, a commissioner'}, {'subject': 'B-ACT', 'relation': 'includes', 'object': 'B-SECTION', 'description': 'Act containing a section', 'example': 'Pension Benefits Standards Act section 1'}, {'subject': 'B-POSITION', 'relation': 'performs', 'object': 'B-ACTION', 'description': 'Position performing a legal action', 'example': 'pipeline permit holder must not design, construct, operate'}, {'subject': 'B-DOCUMENT', 'relation': 'requires', 'object': 'B-LEGAL_TERM', 'description': 'Document requiring a legal term', 'example': 'actuarial valuation report and a cost certificate must include the following'}, {'subject': 'B-ACT', 'relation': 'regulates', 'object': 'B-LEGAL_TERM', 'description': 'Act regulating a legal term', 'example': 'Liquor Control and Licensing Act regulates sale and purchase of liquor'}]}\n"
     ]
    }
   ],
   "source": [
    "print(schema_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "b79e4634-561f-48db-b3b0-47782480dbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Helper function to deduplicate an existing schema dictionary.\n",
    "# It ensures that within both the NER (\"schema\") and triplet (\"triplet_schema\")\n",
    "# sections, no duplicate entries exist.\n",
    "# ----------------------------------------------------------------------------\n",
    "def dedupe_schema(schema):\n",
    "    # Deduplicate the NER labels using the \"name\" field (case-insensitive).\n",
    "    if \"schema\" in schema:\n",
    "        unique_labels = {}\n",
    "        for label in schema[\"schema\"]:\n",
    "            # Use the uppercase, stripped version of the label name as the key.\n",
    "            key = label[\"name\"].strip().upper()\n",
    "            unique_labels[key] = label\n",
    "        schema[\"schema\"] = list(unique_labels.values())\n",
    "    \n",
    "    # Deduplicate the triplet schema using the tuple (subject, relation, object) (case-insensitive).\n",
    "    if \"triplet_schema\" in schema:\n",
    "        unique_triplets = {}\n",
    "        for triplet in schema[\"triplet_schema\"]:\n",
    "            # Use the uppercase, stripped versions for subject, relation, and object.\n",
    "            subject_key = triplet[\"subject\"].strip().upper()\n",
    "            relation_key = triplet[\"relation\"].strip().upper()\n",
    "            object_key = triplet[\"object\"].strip().upper()\n",
    "            key = (subject_key, relation_key, object_key)\n",
    "            unique_triplets[key] = triplet\n",
    "        schema[\"triplet_schema\"] = list(unique_triplets.values())\n",
    "    \n",
    "    return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "120aaeaf-9721-46f0-9901-ba6a09bffbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded saved schema from file.\n",
      "Merged and updated schema saved to file.\n"
     ]
    }
   ],
   "source": [
    "# File where the full AI-generated schema (NER + triplet) will be stored.\n",
    "schema_file = \"ner_schema_response.json\"\n",
    "# ----------------------------------------------------------------------------\n",
    "# Function to merge the new AI-generated schema with the saved schema.\n",
    "# It appends only new labels and new triplet definitions (based on uniqueness).\n",
    "# ----------------------------------------------------------------------------\n",
    "def merge_schemas(new_schema, saved_schema):\n",
    "    # First, deduplicate the saved schema in case there are any duplicates.\n",
    "    saved_schema = dedupe_schema(saved_schema)\n",
    "    \n",
    "    # Create a copy of the saved schema to merge into.\n",
    "    merged_schema = saved_schema.copy()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Merge the NER labels (\"schema\" key)\n",
    "    # -----------------------------\n",
    "    if \"schema\" not in merged_schema:\n",
    "        merged_schema[\"schema\"] = []\n",
    "    # Build a set of existing label names (normalized to uppercase).\n",
    "    existing_labels = { label[\"name\"].strip().upper() for label in merged_schema.get(\"schema\", []) }\n",
    "    # Add any new label that is not already present.\n",
    "    for new_label in new_schema.get(\"schema\", []):\n",
    "        new_label_name = new_label[\"name\"].strip().upper()\n",
    "        if new_label_name not in existing_labels:\n",
    "            merged_schema[\"schema\"].append(new_label)\n",
    "            existing_labels.add(new_label_name)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Merge the Triplet schema (\"triplet_schema\" key)\n",
    "    # -----------------------------\n",
    "    if \"triplet_schema\" in new_schema:\n",
    "        if \"triplet_schema\" not in merged_schema:\n",
    "            merged_schema[\"triplet_schema\"] = []\n",
    "        # Build a set of existing triplets using normalized (subject, relation, object) as a key.\n",
    "        existing_triplets = {\n",
    "            (triplet[\"subject\"].strip().upper(), triplet[\"relation\"].strip().upper(), triplet[\"object\"].strip().upper())\n",
    "            for triplet in merged_schema.get(\"triplet_schema\", [])\n",
    "        }\n",
    "        # Add any new triplet that is not already present.\n",
    "        for new_triplet in new_schema.get(\"triplet_schema\", []):\n",
    "            triplet_key = (\n",
    "                new_triplet[\"subject\"].strip().upper(),\n",
    "                new_triplet[\"relation\"].strip().upper(),\n",
    "                new_triplet[\"object\"].strip().upper()\n",
    "            )\n",
    "            if triplet_key not in existing_triplets:\n",
    "                merged_schema[\"triplet_schema\"].append(new_triplet)\n",
    "                existing_triplets.add(triplet_key)\n",
    "\n",
    "    # Optionally dedupe again after merging.\n",
    "    merged_schema = dedupe_schema(merged_schema)\n",
    "    return merged_schema\n",
    "\n",
    "# File where the full AI-generated schema (NER + triplet) is stored.\n",
    "schema_file = \"ner_schema_response.json\"\n",
    "\n",
    "# Assume schema_response contains the new AI-generated schema.\n",
    "# (schema_response must be defined prior to this code.)\n",
    "\n",
    "if os.path.exists(schema_file):\n",
    "    with open(schema_file, \"r\") as infile:\n",
    "        saved_schema = json.load(infile)\n",
    "    print(\"Loaded saved schema from file.\")\n",
    "\n",
    "    # Merge the new schema with the saved schema.\n",
    "    merged_schema = merge_schemas(schema_response, saved_schema)\n",
    "\n",
    "    # Save the merged (and deduplicated) schema back to the file.\n",
    "    with open(schema_file, \"w\") as outfile:\n",
    "        json.dump(merged_schema, outfile, indent=2)\n",
    "    print(\"Merged and updated schema saved to file.\")\n",
    "\n",
    "    # Update schema_response to reflect the merged result.\n",
    "    schema_response = merged_schema\n",
    "else:\n",
    "    # If the file doesn't exist, save the new schema response directly.\n",
    "    with open(schema_file, \"w\") as outfile:\n",
    "        json.dump(schema_response, outfile, indent=2)\n",
    "    print(\"Saved new schema response to file.\")\n",
    "\n",
    "# (Optional) Print the current schema_response to verify.\n",
    "#print(\"Current schema_response:\", json.dumps(schema_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "f0363943-cb40-4f32-bf83-55b4b615ecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schema_list(id):\n",
    "    auth = project.session.auth\n",
    "    url = f\"{DIFFGRAM_CONFIG['host']}/api/project/{DIFFGRAM_CONFIG['project_id']}/labels?schema_id={id}\"\n",
    "    # Step 4: Make the POST request using the SDK's session auth\n",
    "    response = requests.get(url, auth=auth)\n",
    "    # Step 5: Handle the response\n",
    "    if response.status_code == 200:\n",
    "        #print(\"Annotation update successful!\")\n",
    "        #pprint.pprint(response.json())  # View the updated data\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)  # Print error details for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "70f944a7-2a00-49e0-bb06-5a62ea290994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_schema_label(label_name, existing_label_names):\n",
    "        if label_name.upper() not in existing_label_names:\n",
    "            print(\"Adding label:\", label_name.upper())\n",
    "            label = { 'name': label_name.upper()}\n",
    "            project.label_new(label, schema_id=schema_id)\n",
    "            existing_label_names.add(label_name.upper())\n",
    "        else:\n",
    "            print(f\"Label '{label_name.upper()}' already exists in the schema, skipping.\")\n",
    "        return existing_label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5203e3-fb5f-4940-a03d-f62580a80f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# STEP 1: Create the label mappings.\n",
    "# -------------------------------\n",
    "\n",
    "label2id = {}  # mapping from label string to numeric id\n",
    "id2label = {}  # mapping from numeric id to label string\n",
    "\n",
    "# Iterate through the schema list and create the mappings.\n",
    "for idx, label_info in enumerate(schema_response['schema']):\n",
    "    label = label_info['name']\n",
    "    label2id[label] = idx\n",
    "    id2label[idx] = label\n",
    "\n",
    "# Print the mappings to verify\n",
    "print(\"Label to ID Mapping:\", label2id)\n",
    "print(\"ID to Label Mapping:\", id2label)\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 2: Save the schema to Diffgram.\n",
    "# -------------------------------\n",
    "\n",
    "# Define the name of your NER schema in Diffgram.\n",
    "NER_schema_name = 'ENTITY_TRAINING_SCHEMA'\n",
    "schema_id = None\n",
    "\n",
    "# List the existing schemas in your Diffgram project.\n",
    "schemas = project.schema.list()\n",
    "print(\"Existing Schemas in Diffgram:\")\n",
    "print(json.dumps(schemas, indent=2))\n",
    "\n",
    "# Check if a schema with the name NER_schema_name already exists.\n",
    "for schema in schemas:\n",
    "    if schema.get('name') == NER_schema_name:\n",
    "        schema_id = schema.get('id')\n",
    "        break\n",
    "\n",
    "# If the schema does not exist, create a new one.\n",
    "if schema_id is None:\n",
    "    print(f\"Schema '{NER_schema_name}' not found. Creating a new one...\")\n",
    "    json_response = project.new_schema(name=NER_schema_name)\n",
    "    schema_id = json_response.get(\"id\")\n",
    "    print(f\"Created new schema with id: {schema_id}\")\n",
    "else:\n",
    "    print(f\"Schema '{NER_schema_name}' already exists with id: {schema_id}\")\n",
    "\n",
    "schema_labels = get_schema_list(schema_id)\n",
    "\n",
    "# Retrieve existing labels for the schema to avoid duplicates.\n",
    "schema_label_id_value = []\n",
    "if schema_labels is not None:\n",
    "    labels = schema_labels['labels_out']\n",
    "    for label in labels:\n",
    "        value = {}\n",
    "        value['id'] = label['id']\n",
    "        value['name'] = label['label']['name']\n",
    "        schema_label_id_value.append(value)\n",
    "\n",
    "existing_label_names = set()\n",
    "try:\n",
    "    schema_label_id_value[0]['name']\n",
    "    for label in schema_label_id_value:\n",
    "            label_name = label.get(\"name\")\n",
    "            if label_name:\n",
    "                existing_label_names.add(label_name)\n",
    "    print(existing_label_names)      \n",
    "except:\n",
    "     print(\"There are no schema labels\")   \n",
    "\n",
    "# Now add each label from your NER_schema to the schema if it doesn't already exist.\n",
    "for label_def in schema_response['schema']:\n",
    "    label_name = label_def['name'].upper()\n",
    "    if label_name not in existing_label_names:\n",
    "        print(\"Adding label:\", label_name)\n",
    "        project.label_new(label_def, schema_id=schema_id)\n",
    "        existing_label_names.add(label_name)\n",
    "    else:\n",
    "        print(f\"Label '{label_name}' already exists in the schema, skipping.\")   \n",
    "\n",
    "# Adding triplet data\n",
    "try:\n",
    "    for triplet_def in schema_response['triplet_schema']:\n",
    "        triplet_subject = triplet_def['subject']\n",
    "        triplet_relation = triplet_def['relation']\n",
    "        triplet_object = triplet_def['object']\n",
    "        existing_label_names = add_schema_label(triplet_subject, existing_label_names)\n",
    "        existing_label_names = add_schema_label(triplet_relation, existing_label_names)\n",
    "        existing_label_names = add_schema_label(triplet_subject, existing_label_names)\n",
    "except:\n",
    "     print(\"There are no schema labels\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1df332-22f9-43b0-9281-d4ef72683972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862e9288-f5a6-4840-9b43-ee700a1b3d50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
