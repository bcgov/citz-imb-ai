{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b38761d",
   "metadata": {},
   "source": [
    "# Annotation by Model\n",
    "\n",
    "This notebook takes a pre-trained local model and uses it for NER tasks.\n",
    "The model's goal is to identify and label text that references sections within the act.\n",
    "\n",
    "Prior to this notebook, you must have run `Training a Model` to generate the trained model.\n",
    "\n",
    "Ensure to define the labels you wish the model to use in the `label_list` variable.\n",
    "\n",
    "Input file should be a JSONL file with objects matching this format:\n",
    "```json\n",
    "{\"meta\": {\"identity\": 73955, \"sectionId\": \"12\", \"sectionName\": \"Repealed\", \"ActId\": \"Civil Forfeiture Act\"}, \"text\": \"repealed 12 [ repealed 2023 - 13 - 11. ]\", \"label\": []}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997fff91-dc1c-4ba6-9747-a6316ba78b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68490c9c-4fad-4205-9819-a2f4b079979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intializing tokenizer with help of bert model\n",
    "# must use \"fast\" version to get start and end indices\n",
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ec5bb5-5686-40b9-af91-fe30c012b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Adjust the trained model's config\n",
    "config = json.load(open(\"exported_models/fine_tuned_ner_model/config.json\"))\n",
    "label_list = [\"O\", \"B_ACT\", \"I_ACT\", \"B_REF_IN\", \"I_REF_IN\", \"B_REF_EX\", \"I_REF_EX\"]\n",
    "id2label = {\n",
    "    str(i): label for i,label in enumerate(label_list)\n",
    "}\n",
    "label2id = {\n",
    "    label: str(i) for i,label in enumerate(label_list)\n",
    "}\n",
    "config[\"id2label\"] = id2label\n",
    "config[\"label2id\"] = label2id\n",
    "json.dump(config, open(\"exported_models/fine_tuned_ner_model/config.json\",\"w\"), indent=2)\n",
    "\n",
    "# Load our trained model\n",
    "from transformers import AutoModelForTokenClassification     # This class is responsible for load model into memory\n",
    "model_fine_tuned = AutoModelForTokenClassification.from_pretrained(\"exported_models/fine_tuned_ner_model\")\n",
    "\n",
    "# Pipeline handles the NER process for a single chunk of text\n",
    "from transformers import pipeline\n",
    "nlp = pipeline(\n",
    "    \"ner\",\n",
    "    model=model_fine_tuned,\n",
    "    tokenizer=tokenizer,\n",
    "    ignore_labels=[\"O\"] # ignore_labels is [\"O\"] by default\n",
    ") \n",
    "\n",
    "# Enable offsets so we have a start and end for labels\n",
    "tokenizer_kwargs = {\"return_offsets_mapping\": True}\n",
    "nlp.tokenizer_kwargs = tokenizer_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f64c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to use the pipeline\n",
    "example = \"This can be found in section 14.07 in this act.\"\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0bd544-e3c6-4a6c-995f-2a97222d9827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the label output from the model back to doccano's label format\n",
    "def convert_ner_result(ner_result):\n",
    "    start = ner_result[\"start\"]\n",
    "    end = ner_result[\"end\"]\n",
    "    tag = ner_result[\"entity\"][2:]\n",
    "    return [start, end, tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bec4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_overlapping_tags(tags):\n",
    "    # This works because we assume they are already ordered\n",
    "    # If not, need to sort first\n",
    "    merged_tags = []\n",
    "    for tag in tags:\n",
    "        # If merged_tags is empty or there's no overlap or labels differ\n",
    "        if not merged_tags or tag[0] > merged_tags[-1][1] + 1 or tag[2] != merged_tags[-1][2]:\n",
    "            merged_tags.append(tag)\n",
    "        else:  # Overlap and labels are the same, merge the tags\n",
    "            merged_tags[-1][1] = max(merged_tags[-1][1], tag[1])  # Extend end\n",
    "\n",
    "    return merged_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac42024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From input we would have imported in doccano,\n",
    "# Replace the empty labels with model-generated labels and write to file\n",
    "with open(\"./doccano_import.jsonl\", \"r\") as input:\n",
    "  with open(\"./model_annotated_output.jsonl\", \"w\") as output:\n",
    "    for index, line in enumerate(input):\n",
    "      object = json.loads(line)\n",
    "      ner_results = nlp(object[\"text\"])\n",
    "      label_list = list(map(convert_ner_result, ner_results))\n",
    "      object[\"label\"] = merge_overlapping_tags(label_list)\n",
    "      json.dump(object, output, ensure_ascii=False)\n",
    "      output.write(\"\\n\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
