{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf36bba8",
   "metadata": {},
   "source": [
    "# Training a Model\n",
    "\n",
    "This workbook takes annotated examples and uses them to train a small model for NER tasks.\n",
    "\n",
    "Prior to running this workbook:\n",
    "- define all expected labels under the `entity_types` variable\n",
    "- have an input file with annotated training data\n",
    "\n",
    "The training data should be a JSONL file with lines in this format:\n",
    "```json\n",
    "{\"id\":26,\"text\":\"repealed 12 [ repealed 2023 - 13 - 11. ]\",\"meta\":{\"ActId\":\"Civil Forfeiture Act\",\"identity\":73955,\"sectionId\":\"12\",\"sectionName\":\"Repealed\"},\"label\":[]}\n",
    "```\n",
    "\n",
    "Variables to edit for alterning training strategy:\n",
    "- running_locally\n",
    "- batch_size\n",
    "- num_epochs\n",
    "- learning_rate\n",
    "\n",
    "See the README file in this folder for more information on what these parameters change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386b505b-ab2c-4f6b-84b4-06a691261030",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install tqdm\n",
    "%pip install intel_extension_for_pytorch\n",
    "%pip install transformers==4.45.0 # intel extension requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f1b22d7-acbf-4f31-b59d-3fad2d7cd93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import BertTokenizer, BertForTokenClassification, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader \n",
    "from tqdm import tqdm \n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5809d946-61fe-4bac-a81c-c416f17f9298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming a predefined set of entity types\n",
    "entity_types = [\"O\", \"B_ACT\", \"I_ACT\", \"B_REF_IN\", \"I_REF_IN\", \"B_REF_EX\", \"I_REF_EX\"]\n",
    "# Set num_labels\n",
    "num_labels = len(entity_types)\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') \n",
    "running_locally = True\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_labels, attn_implementation=\"eager\")\n",
    "i_model = model if running_locally else ipex.fast_bert(model, dtype=torch.bfloat16)\n",
    "# Define batch_size\n",
    "batch_size = 32  # Adjust as needed\n",
    "# Define learning rate\n",
    "learning_rate = 5e-5  # Adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abf677eb-5785-403a-9228-2322039ad8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_format_data(dataset, tokenizer):\n",
    "    tokenized_data = []\n",
    "    for sample in dataset:\n",
    "        text = sample[\"text\"]\n",
    "        entities = sample[\"label\"]\n",
    "        # Tokenize the input text using the BERT tokenizer\n",
    "\n",
    "        tokens =  tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text)))\n",
    "        # Initialize labels for each token as 'O' (Outside)\n",
    "        labels = ['O'] * len(tokens)\n",
    "        # Update labels for entity spans\n",
    "        for start, end, entity_type in entities:\n",
    "            # Tokenize the prefix to get the correct offset\n",
    "            prefix_tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text[:start])))\n",
    "            start_token = len(prefix_tokens) - 1\n",
    "            # Tokenize the entity to get its length\n",
    "            entity_tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text[start:end])))\n",
    "            end_token = start_token + len(entity_tokens) - 2\n",
    "            labels[start_token] = f\"B_{entity_type}\"\n",
    "            for i in range(start_token + 1, end_token):\n",
    "                labels[i] = f\"I_{entity_type}\"\n",
    "            \n",
    "                # Convert tokens and labels to input IDs and label IDs\n",
    "                input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "                label_ids = [entity_types.index(label) for label in labels]\n",
    "                # Pad input_ids and label_ids to the maximum sequence length\n",
    "                padding_length = tokenizer.model_max_length - len(input_ids)\n",
    "                input_ids += [tokenizer.pad_token_id] * padding_length\n",
    "                label_ids += [entity_types.index('O')] * padding_length\n",
    "                tokenized_data.append({'input_ids': input_ids,\n",
    "                    'labels': label_ids\n",
    "                })\n",
    "    # Convert tokenized data to PyTorch dataset\n",
    "    dataset = TensorDataset(\n",
    "        torch.tensor([item['input_ids'] for item in tokenized_data]), torch.tensor([item['labels'] for item in tokenized_data])\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e06dca-b253-406f-824f-2e39484ecffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train_dataset = []\n",
    "with open(\"doccano_export.jsonl\", \"r\") as input:\n",
    "  for line in input:\n",
    "    train_dataset.append(json.loads(line))\n",
    "\n",
    "# Prepare data for fine-tuning\n",
    "train_data = tokenize_and_format_data(train_dataset, tokenizer) \n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8d072e-56d3-4618-86c6-95f2d18667da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "optimizer = torch.optim.AdamW(i_model.parameters(), lr=learning_rate) \n",
    "num_epochs = 15  # Adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    i_model.train()\n",
    "    for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "        inputs, labels = batch\n",
    "        # Unpack the tuple\n",
    "        outputs = i_model(inputs, labels=labels)\n",
    "        loss =  outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c58cea-131b-4334-8018-1a0309f5e6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model for later \n",
    "i_model.save_pretrained('exported_models/fine_tuned_ner_model')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
