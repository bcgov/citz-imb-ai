# Highâ€‘Performance C Preâ€‘Tokenizer

*A productionâ€‘ready, AVXâ€‘512â€‘accelerated WordPiece frontâ€‘end for CPUâ€‘only Transformer pipelines*

[![MIT License](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![AVX-512](https://img.shields.io/badge/SIMD-AVX--512-green.svg)](https://en.wikipedia.org/wiki/AVX-512)
[![Thread Safe](https://img.shields.io/badge/Thread-Safe-brightgreen.svg)](https://en.wikipedia.org/wiki/Thread_safety)
[![C17](https://img.shields.io/badge/C-C17-orange.svg)](https://en.wikipedia.org/wiki/C17_(C_standard_revision))
[![Intel Optimized](https://img.shields.io/badge/Intel-Optimized-blue.svg)](https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html)

---

## ğŸš€ Quick Facts

|                | Detail                                                    |
| -------------- | --------------------------------------------------------- |
| **Version**    | v1.0.0                                                    |
| **Language**   | ISO C17 + IntelÂ® intrinsics                               |
| **CPU target** | x86â€‘64 with AVXâ€‘512 F + BW (Byte and Word instructions)   |
| **Hardware**   | Optimized for Intel Xeon Cascade Lake+ (2nd Gen Scalable) |
| **Input Support** | ASCII text + UTF-8 non-breaking spaces (auto-converted) |
| **Throughput** | â‰ˆ 30 Âµs for a 2 kB paragraph (Xeon Gold 6244, DDR4)      |
| **Tokeniser**  | Greedy WordPiece with "##" continuation                   |
| **Chunking**   | 255 tokens *(CLS + â‰¤ 253 + SEP)*, 50â€‘token overlap        |
| **Licence**    | MIT                                                       |

---

## ğŸ“‹ Table of Contents

1. [Architecture Pipeline](#1-architecture-pipeline)
2. [Files & Modules](#2-files--modules)
3. [Core Components](#3-core-components)
4. [Performance Deep-Dive](#4-performance-deep-dive)
5. [Implementation Details](#5-implementation-details)
6. [Build & Usage](#6-build--usage)
7. [Thread Safety & Concurrency](#7-thread-safety--concurrency)
8. [Limitations & Future Work](#8-limitations--future-work)
9. [API Reference](#9-api-reference)
10. [Common Pitfalls & Debug Tips](#10-common-pitfalls--debug-tips)
11. [Glossary](#11-glossary)

---

## 1. Architecture Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Raw UTFâ€‘8    â”‚  â†’  â”‚ Preâ€‘process â”‚  â†’  â”‚Word Splitter â”‚  â†’  â”‚ Subâ€‘word   â”‚
â”‚ text         â”‚     â”‚(lower+punct)|     â”‚(SIMD spaces) â”‚     â”‚ Tokenizer  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                                                    â–¼
                                                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                             â”‚Chunk Maker â”‚
                                                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pipeline Stages

| Stage | Function | SIMD Acceleration | Performance Impact |
|-------|----------|-------------------|-------------------|
| **Preâ€‘processing** | Caseâ€‘folding, punctuation isolation | âœ… AVXâ€‘512 | ~15% of total time |
| **Word Segmentation** | Space detection, inâ€‘place splitting | âœ… AVXâ€‘512 | ~25% of total time |
| **Subâ€‘word Tokenisation** | Hash table lookup, greedy matching | âŒ Scalar | ~50% of total time |
| **Chunking** | Sliding window with overlap | âŒ Scalar | ~10% of total time |

---

## 2. Files & Modules

### Source Directory Structure

```
src/
â”œâ”€â”€ main.c                     # MPI orchestrator
â”œâ”€â”€ utility/                   # Core processing modules
â”‚   â”œâ”€â”€ token_text_splitter.c  # Main tokenizer implementation
â”‚   â”œâ”€â”€ memory_pool.c          # Bump allocator for strings
â”‚   â”œâ”€â”€ text_splitter.c        # Text preprocessing utilities
â”‚   â”œâ”€â”€ load_tokens.c          # Vocabulary loading
â”‚   â”œâ”€â”€ xml_parser.c           # Legal document XML parsing
â”‚   â”œâ”€â”€ json_format.c          # Output formatting
â”‚   â”œâ”€â”€ data_processing.c      # General data utilities
â”‚   â”œâ”€â”€ file_dram.c            # File I/O and memory mapping
â”‚   â”œâ”€â”€ memory.c               # Memory management
â”‚   â”œâ”€â”€ thread_buffer.c        # Thread-safe buffering
â”‚   â”œâ”€â”€ process_config.c       # Configuration processing
â”‚   â””â”€â”€ mpi_def.c              # MPI utilities
â”œâ”€â”€ data_structures/           # Core data structures
â”‚   â”œâ”€â”€ hash_table.c           # Vocabulary lookup (MurMur3)
â”‚   â””â”€â”€ linked_list.c          # Supporting containers
â”œâ”€â”€ process_acts_reg/          # Legal document processing
â”‚   â””â”€â”€ acts_reg.c             # Acts and regulations parser
â””â”€â”€ integrations/              # External integrations
    â””â”€â”€ kafta_producer.h       # Kafka interface (shared object, unused)
```

### Header Directory Structure

```
include/
â”œâ”€â”€ token_text_splitter.h      # Main tokenizer interface
â”œâ”€â”€ memory_pool.h              # Memory pool API
â”œâ”€â”€ text_splitter.h            # Text preprocessing
â”œâ”€â”€ load_tokens.h              # Vocabulary loading
â”œâ”€â”€ xml_parser.h               # XML parsing interface
â”œâ”€â”€ json_format.h              # JSON output formatting
â”œâ”€â”€ file_dram.h                # File I/O interface
â”œâ”€â”€ memory.h                   # Memory management
â”œâ”€â”€ thread_buffer.h            # Thread-safe buffering
â”œâ”€â”€ process_config.h           # Configuration interface
â”œâ”€â”€ mpi_def.h                  # MPI utilities
â”œâ”€â”€ acts_reg.h                 # Legal document processing
â””â”€â”€ data_structures/
    â””â”€â”€ hash_table.h           # Hash table interface
```

### Core Tokenizer Components

| File | Purpose | Key Functions | Dependencies |
|------|---------|---------------|--------------|
| **`utility/token_text_splitter.c`** | Main SIMD tokenizer | `token_text_splitter()`, SIMD routines | `memory_pool.h`, `hash_table.h` |
| **`utility/memory_pool.c`** | Bump allocator | `init_pool()`, `pool_strdup()` | Standard C library |
| **`data_structures/hash_table.c`** | Vocabulary lookup | `search()`, `create_hash_table()` | MurMur3 hash implementation |
| **`utility/load_tokens.c`** | Vocabulary loader | Token loading from disk | File I/O utilities |
| **`utility/text_splitter.c`** | Text preprocessing | Text cleaning, normalization | UTF-8 handling |

### Data Structure Definitions

From `token_text_splitter.h`:

```c
typedef struct tokens {
    int *token_values;
    char *word;
    int token_count;
} tokens_t;

typedef struct {
    int* values;
    int count;
} TokenArray;

typedef struct {
    TokenArray* arrays;
    int num_arrays;
    int total_tokens;
    int capacity;
} TokenCollection;

typedef struct {
    char **words;           // Original words extracted
    int **token_values;     // 2D array for token values per word
    int *token_counts;      // Number of tokens per word
    int word_count;         // Total number of words
    int *flattened_tokens;  // All tokens in sequence
    int flattened_count;    // Total token count
    int **token_chunks;     // Array of 255-token chunks
    char **chunk_texts;     // Human-readable chunk text
    int chunk_count;        // Number of chunks
} TokenizedData;
```

### Module Dependencies

```
main.c (MPI orchestrator)
â”œâ”€â”€ token_text_splitter.h      (core tokenizer)
â”œâ”€â”€ acts_reg.h                 (legal document processing)
â”œâ”€â”€ xml_parser.h               (document parsing)
â”œâ”€â”€ json_format.h              (output formatting)
â”œâ”€â”€ mpi_def.h                  (MPI utilities)
â””â”€â”€ process_config.h           (configuration)

utility/token_text_splitter.c
â”œâ”€â”€ data_structures/hash_table.h  (vocabulary lookup)
â”œâ”€â”€ memory_pool.h                 (string allocation)
â”œâ”€â”€ <immintrin.h>                 (AVX-512 intrinsics)
â””â”€â”€ <stdbool.h>                   (boolean types)

process_acts_reg/acts_reg.c
â”œâ”€â”€ xml_parser.h               (XML document parsing)
â”œâ”€â”€ text_splitter.h            (text preprocessing)
â””â”€â”€ token_text_splitter.h      (tokenization)
```

---

## 3. Core Components

### 3.1 Data Structures

#### Core Types (from `token_text_splitter.h`)

```c
typedef struct tokens {
    int *token_values;      // Array of token IDs for a single word
    char *word;             // Original word string
    int token_count;        // Number of tokens in this word
} tokens_t;

typedef struct {
    int* values;            // Token ID array
    int count;              // Number of tokens
} TokenArray;

typedef struct {
    TokenArray* arrays;     // Collection of token arrays
    int num_arrays;         // Number of arrays in collection
    int total_tokens;       // Total tokens across all arrays
    int capacity;           // Allocated capacity
} TokenCollection;

typedef struct {
    char **words;           // Original words extracted
    int **token_values;     // 2D array: token IDs for each word
    int *token_counts;      // Number of tokens per word
    int word_count;         // Total number of words
    int *flattened_tokens;  // All tokens in single sequence
    int flattened_count;    // Total token count
    int **token_chunks;     // Array of 255-token chunks
    char **chunk_texts;     // Human-readable chunk representations
    int chunk_count;        // Number of chunks created
} TokenizedData;
```

#### API Functions

| Function | Purpose | Returns | Notes |
|----------|---------|---------|-------|
| `token_text_splitter()` | Main tokenization pipeline | `TokenizedData` | Core entry point |
| `create_token_collection()` | Initialize token collection | `TokenCollection*` | Dynamic capacity management |
| `add_token_array()` | Add tokens to collection | `void` | Handles capacity expansion |
| `free_token_collection()` | Cleanup token collection | `void` | Memory management |

#### Memory Layout Explanation

```c
// Example TokenizedData for input: "Hello, world!"
TokenizedData result = {
    .words = {"hello", ",", "world", "!"},     // 4 words after preprocessing
    .word_count = 4,
    
    .token_values = {
        {7592},           // "hello" â†’ [7592]
        {1010},           // "," â†’ [1010] 
        {2088},           // "world" â†’ [2088]
        {999}             // "!" â†’ [999]
    },
    .token_counts = {1, 1, 1, 1},              // 1 token per word
    
    .flattened_tokens = {7592, 1010, 2088, 999}, // All tokens in sequence
    .flattened_count = 4,
    
    .token_chunks = {
        {101, 7592, 1010, 2088, 999, 102, 0, 0, ...} // [CLS] + tokens + [SEP] + padding
    },
    .chunk_count = 1,
    .chunk_texts = {"hello , world !"}         // Human-readable representation
};
```

### 3.2 Core Algorithm Flow

```
token_text_splitter() Pipeline Flow:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input Text  â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ split_punctuations_     â”‚ â†’  â”‚ Handle UTF-8 NBSP       â”‚
â”‚ and_to_lowercase()      â”‚    â”‚ (0xC2 0xA0 â†’ space)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â–¼                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ to_lowercase_avx512()   â”‚ â†’  â”‚ mark_punctuation_       â”‚
â”‚ (case normalization)    â”‚    â”‚ avx512() + space insert â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ split_text_to_words()   â”‚ â†’  â”‚ SIMD space detection    â”‚
â”‚ (word segmentation)     â”‚    â”‚ + in-place null terms   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ For each word:          â”‚ â†’  â”‚ get_token()             â”‚
â”‚ greedy tokenization     â”‚    â”‚ (WordPiece algorithm)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Flatten all tokens      â”‚ â†’  â”‚ Create sliding window   â”‚
â”‚ into single array       â”‚    â”‚ chunks with overlap     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Return TokenizedData    â”‚
â”‚ with chunks + metadata  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Performance Deep-Dive

### 3.1 SIMD Optimizations

#### Case Conversion (AVX-512)

**Why it matters:** Case-folding represents ~15% of pipeline cost on English text.

```c
char *to_lowercase_avx512(char *str, size_t len) {
    __m512i mask_uppercase = _mm512_set1_epi8(0x20);  // Add 32 to convert
    __m512i lower_limit = _mm512_set1_epi8('A');
    __m512i upper_limit = _mm512_set1_epi8('Z');
    
    for (size_t i = 0; i + 64 <= len; i += 64) {
        __m512i chunk = _mm512_loadu_si512((__m512i *)(str + i));
        
        // Create mask for uppercase letters
        __mmask64 mask = _mm512_cmple_epu8_mask(chunk, upper_limit);
        mask &= _mm512_cmpge_epu8_mask(chunk, lower_limit);
        
        // Conditionally add 32 to convert to lowercase
        __m512i result = _mm512_mask_add_epi8(chunk, mask, chunk, mask_uppercase);
        _mm512_storeu_si512((__m512i *)(str + i), result);
    }
    // Handle remaining bytes with scalar code...
}
```

**Performance:** 6-8Ã— faster than scalar implementation.

#### Space Detection (AVX-512)

**Why it matters:** Word splitting dominates for long texts.

```c
// Inside split_text_to_words()
for (i = 0; i + 64 <= len; i += 64) {
    __m512i chunk = _mm512_loadu_si512((const __m512i *)(text_copy + i));
    __mmask64 space_mask = _mm512_cmpeq_epi8_mask(chunk, _mm512_set1_epi8(' '));
    
    while (space_mask) {
        int space_idx = __builtin_ctzll(space_mask);  // Find first set bit
        space_mask &= space_mask - 1;                 // Clear lowest set bit
        
        // Null-terminate and store word
        text_copy[i + space_idx] = '\0';
        result[count] = pool_strdup(pool, token_start);
        count++;
        token_start = text_copy + i + space_idx + 1;
    }
}
```

**Performance:** Processes entire cache lines, ~4Ã— faster than scalar.

### 3.2 Memory Optimization

| Technique | Impact | Implementation |
|-----------|---------|----------------|
| **Memory Pool** | Eliminates malloc overhead | Single 1MB arena, bump allocation |
| **In-place Processing** | Reduces memory copies | Null-terminate strings directly |
| **Batch Processing** | Improves cache locality | Process 64-byte chunks |

### 3.3 Performance Characteristics

| Input Size | Processing Time | Memory Usage | Dominant Stage |
|------------|----------------|--------------|----------------|
| < 1 KB | < 10 Âµs | < 4 KB | Hash lookups |
| 1-10 KB | 10-100 Âµs | 16-64 KB | SIMD processing |
| > 10 KB | > 100 Âµs | 64+ KB | Word tokenization |

---

## 4. Implementation Details

### 4.1 WordPiece Tokenization Algorithm

The `get_token()` function implements greedy longest-match WordPiece:

```c
tokens_t get_token(HashTable *table, const char *text) {
    bool prefix = false;
    for (size_t i = 0; i < len; ) {
        int found = 0;
        
        // Try progressively shorter substrings
        for (size_t j = len - i; j > 0; j--) {
            strncpy(buffer, text + i, j);
            buffer[j] = '\0';
            
            // Add ## prefix for continuation tokens
            if (!prefix) {
                snprintf(prefix_buffer, j + 1, "%s", buffer);
            } else {
                snprintf(prefix_buffer, j + 3, "##%s", buffer);
            }
            
            char *key_found = check_substring(table, prefix_buffer);
            if (key_found) {
                token_result.token_values[token_result.token_count++] = atoi(key_found);
                prefix = true;
                i += j;
                found = true;
                break;
            }
        }
        
        if (!found) {
            i++; // Skip OOV character
        }
    }
    return token_result;
}
```

**Key Features:**
- **Greedy Matching:** Always tries longest possible match first
- **Prefix Handling:** Adds "##" for subword continuations
- **OOV Handling:** Gracefully skips unknown characters
- **Dynamic Allocation:** Allocates token arrays based on actual needs

### 4.2 Chunking Strategy

**Configuration:**
- `CHUNK_SIZE = 255` (including special tokens)
- `OVERLAP = 50` tokens between chunks  
- `STRIDE = 203` tokens (253 - 50)
- `CLS_TOKEN = 101`, `SEP_TOKEN = 102`

**Algorithm:**
```c
result.chunk_count = (total_tokens <= effective_chunk_size) 
                   ? 1 
                   : ((total_tokens - overlap_size) / stride + 1);

for (int i = 0; i < result.chunk_count; i++) {
    int start = i * stride;
    int copy_size = min(remaining_tokens, effective_chunk_size);
    
    // Build chunk: [CLS] + tokens + [SEP] + padding
    result.token_chunks[i][0] = CLS_TOKEN;
    memcpy(&result.token_chunks[i][1], &flattened_tokens[start], copy_size * sizeof(int));
    result.token_chunks[i][copy_size + 1] = SEP_TOKEN;
    
    // Zero-pad remaining positions
    if (copy_size < effective_chunk_size) {
        memset(&result.token_chunks[i][copy_size + 2], 0, 
               (chunk_size - copy_size - 2) * sizeof(int));
    }
}
```

---

## 5. Build & Usage

### 5.1 Build Instructions

#### CMake Build (Recommended)

```bash
# Clone repository and setup build directory
mkdir build && cd build

# Configure with CMake (auto-detects Intel ICX or falls back to GCC)
cmake ..

# Build the project
make -j$(nproc)
```

#### Manual Build (Standalone Tokenizer)

```bash
# GCC 13+ recommended; adjust -march for your CPU
CFLAGS="-O3 -march=native -ffast-math -pipe"
gcc $CFLAGS -Iinclude pre_tokenizer.c memory_pool.c hash_table.c -o pre_tokenizer

# Force scalar fallback (for non-AVX-512 CPUs)
gcc $CFLAGS -DNO_AVX512 -Iinclude *.c -o pre_tokenizer
```

### 5.2 Dependencies

#### Core Dependencies
- **C Compiler:** Intel ICX (preferred) or GCC 9+
- **Standard Library:** `<immintrin.h>` for SIMD intrinsics

#### HPC Pipeline Dependencies (Full Project)
| Library | Purpose | Required |
|---------|---------|----------|
| **MPI** | Distributed processing | âœ… Yes |
| **OpenVINO** | Neural network inference | âš ï¸ Optional |
| **libxml2** | XML processing | âœ… Yes |
| **NUMA** | Memory locality optimization | âœ… Yes |
| **OpenMP** | Thread parallelism | âœ… Yes |
| **librdkafka** | Message streaming | âš ï¸ Linked but unused |
| **json-c** | JSON processing | âœ… Yes |

#### Compiler Detection Logic
```cmake
find_program(ICX_EXECUTABLE NAMES icx)
if(ICX_EXECUTABLE)
    set(CMAKE_C_COMPILER ${ICX_EXECUTABLE})
    message(STATUS "Using Intel ICX compiler")
else()
    set(CMAKE_C_COMPILER gcc)
    message(WARNING "ICX not found, falling back to GCC")
endif()
```

### 5.3 Usage Examples

#### Standalone Tokenizer Usage

```c
#include "token_text_splitter.h"
#include "memory_pool.h"

int main() {
    // Initialize components
    HashTable *vocab = load_vocab("vocab.txt");        // Load vocabulary
    MemoryPool pool; 
    init_pool(&pool, 1 << 20);                         // 1 MB memory pool
    
    // Process text
    const char *text = "Hello, WORLD! Dogs & Cats?\u00A0Yes.";
    TokenizedData td = token_text_splitter(vocab, text, &pool);
    
    // Output results
    printf("Processed %d words into %d chunks:\n", td.word_count, td.chunk_count);
    
    for (int i = 0; i < td.chunk_count; i++) {
        printf("Chunk %d: %s\n", i, td.chunk_texts[i]);
        printf("Tokens (%d): ", 255);
        
        for (int j = 0; j < 255; j++) {
            if (td.token_chunks[i][j] != 0) {
                printf("%d ", td.token_chunks[i][j]);
            }
        }
        printf("\n\n");
    }
    
    // Cleanup
    free_tokenized_data(&td);
    free_pool(&pool);
    destroy_vocab(vocab);
    
    return 0;
}
```

#### HPC Pipeline Usage (MPI)

```bash
# Build the full HPC pipeline
mkdir build && cd build
cmake .. && make -j$(nproc)

# Run with MPI across multiple nodes/sockets
mpirun -genv I_MPI_DEBUG=5 --bind-to socket:2 -np 2 \
    ./HPCChain \
    ../vocab_file \
    ../BCLaws_Output/act_flat/Consol_43___October_15_2024 \
    ../BCLaws_Output/act_flat/Consol_42___March_11_2024 \
    1 : python ../mpi_receiver.py

# Alternative with escaped paths
mpirun -genv I_MPI_DEBUG=5 --bind-to socket:2 -np 2 \
    ./HPCChain \
    ../vocab_file \
    ../../../data/bclaws/Consolidations/Acts/Consol\ 14\ -\ February\ 13\,\ 2006/ \
    ../../../data/bclaws/Consolidations/Acts/Consol\ 15\ -\ July\ 11\,\ 2006/ \
    1 : python ../mpi_receiver.py
```

#### MPI Command Breakdown

| Parameter | Purpose | Value | Your Hardware |
|-----------|---------|-------|---------------|
| `-genv I_MPI_DEBUG=5` | Enable detailed MPI debugging | `5` = verbose logging | Useful for 2-socket system |
| `--bind-to socket:2` | Bind processes to CPU sockets | `2` processes per socket | Perfect for your 2Ã—8-core setup |
| `-np 2` | Number of MPI processes | `2` parallel workers | Matches your 2 NUMA nodes |
| `./HPCChain` | Main executable | Built from CMake | Targets Cascade Lake optimizations |
| `vocab_file` | Tokenizer vocabulary | Hash table input | Shared across both sockets |
| `input_paths` | Document directories | Legal document corpus | Each socket processes different files |
| `1` | Processing mode | Configuration flag | Single-file-per-rank mode |
| `: python ../mpi_receiver.py` | Result processor | Python backend | Aggregates results from both nodes |

### 5.4 Compiler Optimization Flags

#### Intel ICX (Primary/Preferred)
```bash
-qopenmp -O3 -mavx512f -mavx -msse3 -mavx512bw -mtune=generic -march=x86-64 -Wall -Wextra
```

#### GCC (Fallback Only)
```bash
-fopenmp -O3 -mavx512f -mavx -msse3 -mavx512bw -mtune=generic -march=x86-64 -Wall -Wextra
```

#### Debug Build (AddressSanitizer - Both Compilers)
```bash
-fsanitize=address -g
```

**Performance Notes:**
- **Intel ICX:** Primary compiler for HPC environment; ~10-15% better performance on Intel CPUs
- **GCC Fallback:** Used only when ICX unavailable; compatibility mode
- **AVX-512:** Required for SIMD optimizations; falls back to scalar on older CPUs
- **AddressSanitizer:** Enables runtime memory error detection (debug builds only)
- **Your Environment:** ICX expected and preferred for optimal performance

### 5.5 Error Handling

The implementation provides comprehensive error handling:

- **Memory Allocation:** All `malloc()` calls checked with `perror()` + `exit(EXIT_FAILURE)`
- **Buffer Overflows:** Bounds checking for chunk indices and string operations
- **Invalid Input:** Graceful handling of malformed UTF-8 sequences
- **Resource Cleanup:** Proper cleanup on all error paths
- **MPI Errors:** Comprehensive MPI error handling in distributed mode

---

## 9. API Reference

### Core Functions

#### `TokenizedData token_text_splitter(HashTable *table, const char *text, MemoryPool *pool)`
**Purpose:** Main entry point for tokenization pipeline  
**Returns:** Complete tokenization result with chunks  
**Complexity:** O(n + wÃ—lÃ—log(v)) where n=text length, w=words, l=avg word length, v=vocab size

#### `tokens_t get_token(HashTable *table, const char *text)`
**Purpose:** Tokenize a single word using WordPiece algorithm  
**Returns:** Token IDs and metadata for the word  
**Complexity:** O(lÃ—log(v)) where l=word length, v=vocab size

#### `char *split_punctuations_and_to_lowercase(const char *str)`
**Purpose:** Preprocess text by normalizing case and isolating punctuation  
**Returns:** Processed string (caller must free)  
**Features:** UTF-8 NBSP handling, SIMD acceleration

#### `void split_text_to_words(const char *text, char ***words, int *word_count, MemoryPool *pool)`
**Purpose:** Split text into words using space delimiters  
**Output:** Array of word strings allocated from memory pool  
**Features:** SIMD acceleration for â‰¥64 byte inputs

### Utility Functions

| Function | Purpose | SIMD | Notes |
|----------|---------|------|-------|
| `to_lowercase_avx512()` | Convert string to lowercase | âœ… | 64-byte chunks |
| `mark_punctuation_avx512()` | Identify punctuation positions | âœ… | Uses lookup table |
| `check_substring()` | Hash table lookup with preprocessing | âŒ | Removes whitespace |
| `removeWhitespace()` | In-place whitespace removal | âŒ | O(n) single pass |

---

## 10. Common Pitfalls & Debug Tips

### ğŸš¨ SIMD Compatibility Issues

**Problem:** Segmentation fault or illegal instruction on older CPUs
```bash
# Error: SIGILL (Illegal instruction)
# Cause: AVX-512 instructions on non-supporting CPU
```

**Check Your CPU Support:**
```bash
# Verify AVX-512 capabilities
lscpu | grep avx512
# Required flags: avx512f avx512bw avx512dq avx512vl

# Your Xeon Gold 6244 supports:
# âœ… avx512f   (Foundation - basic 512-bit ops)
# âœ… avx512bw  (Byte/Word - string processing) 
# âœ… avx512dq  (Doubleword/Quadword)
# âœ… avx512vl  (Vector Length extensions)
```

**Solutions for Compatibility Issues:**
```bash
# 1. Force scalar fallback (Intel ICX)
icx -DNO_AVX512 -O3 *.c -o tokenizer

# 2. Force scalar fallback (GCC fallback)
gcc -DNO_AVX512 -O3 *.c -o tokenizer

# 3. Use conservative march flags (ICX)
icx -march=x86-64-v2 instead of -march=x86-64

# 4. Target specific CPU generation (ICX preferred)
icx -march=cascadelake     # For your Xeon Gold 6244
icx -march=sapphirerapids  # For newer systems

# 5. GCC fallback (if ICX unavailable)
gcc -march=skylake-avx512  # For Cascade Lake with GCC
```

**Compiler Priority:**
- **Primary:** Intel ICX (preferred for your HPC environment)
- **Fallback:** GCC (compatibility only)
- **Your System:** âœ… ICX expected, Xeon Gold 6244 (Cascade Lake) - Full compatibility

### ğŸ”§ Memory Pool Debugging

**Problem:** Segmentation fault in `pool_strdup()`
```c
// Common mistake: Using pool after it's destroyed
MemoryPool pool;
init_pool(&pool, 1024);
char *str = pool_strdup(&pool, "test");
free_pool(&pool);  // Pool destroyed
printf("%s", str); // âŒ SEGFAULT - string memory freed
```

**Solution:**
```c
// Correct: Keep pool alive until all strings are no longer needed
MemoryPool pool;
init_pool(&pool, 1024);
TokenizedData result = token_text_splitter(vocab, text, &pool);
// ... use result ...
free_tokenized_data(&result);  // Free first
free_pool(&pool);              // Then free pool
```

### ğŸ› Hash Table Issues

**Problem:** Tokens not found despite being in vocabulary
```c
// Debug: Check key preprocessing
char *debug_key = check_substring(table, "Hello,");
// This might fail because punctuation affects lookup
```

**Debug Steps:**
```c
// 1. Print preprocessed keys
char *processed = split_punctuations_and_to_lowercase("Hello,");
printf("Original: 'Hello,' â†’ Processed: '%s'\n", processed);
// Output: "hello , " (note spaces around comma)

// 2. Check individual components
char **words; int count;
split_text_to_words(processed, &words, &count, &pool);
for (int i = 0; i < count; i++) {
    printf("Word[%d]: '%s'\n", i, words[i]);
}
```

### âš™ï¸ MPI Debug Configuration

**Problem:** MPI processes hanging or not communicating
```bash
# Enable detailed MPI debugging
export I_MPI_DEBUG=5
export I_MPI_HYDRA_DEBUG=1

# Check process binding
mpirun --report-bindings -np 2 ./HPCChain ...

# Verify socket binding
mpirun --bind-to socket:2 --report-bindings -np 2 ./HPCChain ...
```

### ğŸ“ Build Debug Tips

**Intel ICX not found:**
```bash
# Check if ICX is in PATH
which icx

# Load Intel OneAPI environment (common setup)
source /opt/intel/oneapi/setvars.sh

# Verify ICX is working
icx --version

# Your HPC system expects ICX - ensure it's properly loaded
module load intel/oneapi  # If using environment modules
```

**Compiler Detection Logic:**
```bash
# CMake automatically detects ICX first, falls back to GCC
# Your system priority:
# 1. Intel ICX (preferred for HPC performance)
# 2. GCC (compatibility fallback only)
```

**OpenVINO missing:**
```bash
# Check CMake OpenVINO detection
cmake .. -DOPENVINO_DIR=/path/to/openvino
# Or disable OpenVINO support
cmake .. -DUSE_OPENVINO=OFF
```

**Dependency Issues:**
```bash
# Install missing libraries (Ubuntu/Debian)
sudo apt-get install libxml2-dev libnuma-dev librdkafka-dev libjson-c-dev

# Note: librdkafka is linked but not actively used
# Check library locations
ldconfig -p | grep xml2
pkg-config --libs libxml-2.0

# Optional: Remove unused Kafka dependency
# Edit CMakeLists.txt to comment out Kafka linking if not needed
```

---

## 7. Thread Safety & Concurrency

## 8. Limitations & Future Work

| Limitation | Impact | Workaround |
|------------|---------|-----------|
| **ASCII-centric** | Limited UTF-8 support | Use for English/Western languages |
| **No normalization** | Inconsistent Unicode handling | Preprocess with ICU library |
| **Fixed chunk size** | Memory overhead for short texts | Adjust constants for use case |

### Future Enhancements

- **Full UTF-8 Support:** NFC/NFKC normalization, proper grapheme handling, multi-byte character tokenization

---

## 8. Thread Safety & Concurrency

### Thread Safety Analysis

The pre-tokenizer is **thread-safe by design** for the intended usage pattern:

**âœ… Safe Operations (per-thread):**
- **Read-only hash table access:** Multiple threads can safely perform `check_substring()` lookups simultaneously
- **Independent memory pools:** Each thread uses its own `MemoryPool` instance  
- **Local stack variables:** All processing buffers are allocated on local stack or thread-local heap
- **No shared mutable state:** Each `token_text_splitter()` call operates on independent data

**âœ… MPI Deployment Pattern:**
```c
// Each MPI rank processes different files - naturally thread-safe
// Rank 0: processes file_A.txt using its own MemoryPool
// Rank 1: processes file_B.txt using its own MemoryPool  
// No shared memory between processes
```

**âš ï¸ Shared Resources (must be managed carefully):**
- **Hash table initialization:** Must be completed before spawning worker threads
- **Vocabulary loading:** Should be done once in main thread, then shared read-only

**Recommended Usage Pattern:**
```c
// Main thread: Initialize shared read-only resources
HashTable *shared_vocab = load_vocab("vocab.txt");  // Once, shared

#pragma omp parallel
{
    // Each thread: Independent processing resources
    MemoryPool local_pool;
    init_pool(&local_pool, 1 << 20);  // Thread-local memory pool
    
    // Process different files per thread - no contention
    char filename[256];
    sprintf(filename, "input_%d.txt", omp_get_thread_num());
    
    TokenizedData result = token_text_splitter(shared_vocab, text, &local_pool);
    // ... process result ...
    
    free_pool(&local_pool);  // Thread-local cleanup
}
```

---

*Â© 2025 â€” Thread-safe, SIMD-accelerated tokenizer for distributed NLP pipelines*